{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from _preprocessing import change_dtypes\n",
    "from _preprocessing import aggregate\n",
    "from _stats import f_ratio\n",
    "from _stats import corrwith\n",
    "from _stats import mode\n",
    "\n",
    "INP_DIR = \"data/download\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.read_csv(os.path.join(INP_DIR, \"application_train.csv\"))\n",
    "application_train = change_dtypes(application_train)\n",
    "\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "application_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals = application_train.isnull().mean()\n",
    "missing_vals = missing_vals.sort_values(ascending=False)\n",
    "print(\"Total number of features:\", application_train.shape[1])\n",
    "print(\"Number of features having missing values:\", (missing_vals > 0).sum())\n",
    "\n",
    "print(\"head:\")\n",
    "display(missing_vals.head(20))\n",
    "print(\"tail\")\n",
    "display(missing_vals[missing_vals > 0].tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the `TARGET` by a categorical feature. `TARGET` in each group is aggregated by `mean`, `var` and `count`. The resulting `group mean`, `group variance` and `group count` are used to calculate the F-ratio which is define as the `between-group variance` divided by `within-group variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ratio_df(df):\n",
    "    return f_ratio(df[\"default_rate\"], df[\"var_of_default_rate\"], df[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of categorical type\n",
    "cat_cols = application_train.select_dtypes([\"category\"]).columns\n",
    "agg_stats = [(\"default_rate\", \"mean\"), (\"var_of_default_rate\", np.var), \"count\"]\n",
    "\n",
    "results = []\n",
    "for col in cat_cols:\n",
    "    df = application_train.groupby(\n",
    "        application_train[col].astype(str))[\"TARGET\"].agg(agg_stats)\n",
    "    df = df.sort_values(\"default_rate\", ascending=False)\n",
    "    results.append((col, df))\n",
    "\n",
    "# sort by the F-ratio\n",
    "results = sorted(results, key=lambda x: f_ratio_df(x[1]), reverse=True)\n",
    "for col, df in results:\n",
    "    print(\"%s, f-ratio=%0.5f\" % (col, f_ratio_df(df)))\n",
    "    display(df)\n",
    "    print(\"------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = application_train.drop([\"TARGET\"], axis=1).select_dtypes([\"bool\"]).columns\n",
    "\n",
    "agg_stats = [(\"default_rate\", \"mean\"), (\"var_of_default_rate\", np.var), \"count\"]\n",
    "\n",
    "results = []\n",
    "for col in bool_cols:\n",
    "    df = application_train.groupby(\n",
    "        application_train[col].astype(str))[\"TARGET\"].agg(agg_stats)\n",
    "    df = df.sort_values(\"default_rate\", ascending=False)\n",
    "    results.append((col, df))\n",
    "\n",
    "# sort by the F-ratio of default_rate\n",
    "results = sorted(results, key=lambda x: f_ratio_df(x[1]), reverse=True)\n",
    "for col, df in results:\n",
    "    print(\"%s, f-ratio=%0.5f\" % (col, f_ratio_df(df)))\n",
    "    display(df)\n",
    "    print(\"------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of numerical features with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = application_train.drop([\"TARGET\", \"SK_ID_CURR\"] + list(cat_cols) + list(bool_cols), axis=1).columns\n",
    "\n",
    "corrs = application_train.loc[:, num_cols].corrwith(application_train[\"TARGET\"])\n",
    "corrs = corrwith(application_train.loc[:, num_cols], application_train[\"TARGET\"])\n",
    "\n",
    "corrs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=25, ncols=4, figsize=(16, 120))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(corrs.index):\n",
    "    application_train[col].plot(kind=\"hist\", ax=axes[i])\n",
    "    axes[i].set_title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features are very screwed and range over many orders of magnitude such as `AMT_GOODS_PRICE`, `AMT_CREDIT`, `AMT_ANNUITY`, `AMT_INCOME_TOTAL`. May consider log transforming them.\n",
    "\n",
    "`DAYS_EMPLOYED` are supposed to be negative but there is a very large positive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_emp_max = application_train[\"DAYS_EMPLOYED\"].max()\n",
    "print(\"days_emp_max:\", days_emp_max)\n",
    "\n",
    "print(\"Count of the maximun value:\", sum(application_train[\"DAYS_EMPLOYED\"] == days_emp_max))\n",
    "print(\"Count of positive values:\", sum(application_train[\"DAYS_EMPLOYED\"] > 0))\n",
    "\n",
    "default_rate_pos = application_train.loc[application_train[\"DAYS_EMPLOYED\"] == days_emp_max, \"TARGET\"].mean()\n",
    "print(\"Default rate for positive DAYS_EMPLOYED: %0.5f\" % default_rate_pos)\n",
    "\n",
    "default_rate_neg = application_train.loc[application_train[\"DAYS_EMPLOYED\"] < days_emp_max, \"TARGET\"].mean()\n",
    "print(\"Default rate for negative DAYS_EMPLOYED: %0.5f\" % default_rate_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is quite significant. So we may consider adding a binary column which indicates whether `DAYS_EMPLOYED` is positive. Also we may change the maximum to 1 or `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"AMT_GOODS_PRICE\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_INCOME_TOTAL\"]\n",
    "log_transformed_features = application_train[cols]\n",
    "for col in cols:\n",
    "    print(col, (log_transformed_features[col] < 0).any())\n",
    "\n",
    "for col in cols:\n",
    "    log_transformed_features[col + \"_log\"] = np.log(log_transformed_features[col] + 1)\n",
    "\n",
    "corrs = corrwith(log_transformed_features, application_train[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like only AMT_INCOME_TOTAL_log improves the correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_to_income = application_train[\"AMT_CREDIT\"] / application_train[\"AMT_INCOME_TOTAL\"]\n",
    "credit_to_income.corr(application_train[\"TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may be a useful feature\n",
    "credit_to_goods = application_train[\"AMT_CREDIT\"] / application_train[\"AMT_GOODS_PRICE\"]\n",
    "credit_to_goods.corr(application_train[\"TARGET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = pd.read_csv(os.path.join(INP_DIR, \"bureau.csv\"))\n",
    "bureau = change_dtypes(bureau)\n",
    "\n",
    "print(\"bureau.shape:\", bureau.shape)\n",
    "bureau.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals = bureau.isnull().mean()\n",
    "missing_vals = missing_vals.sort_values(ascending=False)\n",
    "print(\"Total number of features:\", bureau.shape[1])\n",
    "print(\"Number of features having missing values:\", (missing_vals > 0).sum())\n",
    "\n",
    "print(\"head:\")\n",
    "display(missing_vals.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of aggregated features with `TARGET`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate numerical columns with count\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"num\", num_stats=(\"count\",))\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "bureau_agg = bureau_agg.fillna(0)\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate numerical columns with mean\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"num\", num_stats=(\"mean\",))\n",
    "\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate numerical columns with median\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"num\", num_stats=(\"median\",))\n",
    "\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate numerical columns with variance\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"num\", num_stats=(np.var,))\n",
    "\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate numerical columns with min\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"num\", num_stats=(\"min\",))\n",
    "\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate numerical columns with max\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"num\", num_stats=(\"max\",))\n",
    "\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = bureau.select_dtypes([\"category\", \"object\", \"bool\"]).columns\n",
    "for col in cat_cols:\n",
    "    print(col, bureau[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate categorical columns with sum\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"sum\",))\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "#bureau_agg = bureau_agg.fillna(0)\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate categorical columns with mean\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"mean\",))\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate categorical columns with number of unique values\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"nunique\",), onehot_encode=False)\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "\n",
    "bureau_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "bureau_agg = bureau_agg.fillna(0)\n",
    "\n",
    "corrs = corrwith(bureau_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate categorical columns with mode\n",
    "\n",
    "bureau_agg = aggregate(bureau.drop([\"SK_ID_BUREAU\"], axis=1), \n",
    "                       by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(mode,), onehot_encode=False)\n",
    "print(\"bureau_agg shape:\", bureau_agg.shape)\n",
    "bureau_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count and percent of number of times DPD are over 1, 3, and 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by mean\n",
    "\n",
    "credit_day_overdue = bureau[[\"SK_ID_CURR\"]]\n",
    "credit_day_overdue[\"CREDIT_DAY_OVERDUE_OVER_0M\"] = bureau[\"CREDIT_DAY_OVERDUE\"] == 0\n",
    "credit_day_overdue[\"CREDIT_DAY_OVERDUE_OVER_1M\"] = bureau[\"CREDIT_DAY_OVERDUE\"] > 30\n",
    "credit_day_overdue[\"CREDIT_DAY_OVERDUE_OVER_3M\"] = bureau[\"CREDIT_DAY_OVERDUE\"] > 90\n",
    "credit_day_overdue[\"CREDIT_DAY_OVERDUE_OVER_6M\"] = bureau[\"CREDIT_DAY_OVERDUE\"] > 120\n",
    "\n",
    "credit_day_overdue_agg = aggregate(credit_day_overdue, by=[\"SK_ID_CURR\"], cat_stats=(\"mean\",))\n",
    "print(\"credit_day_overdue_agg shape:\", credit_day_overdue_agg.shape)\n",
    "\n",
    "credit_day_overdue_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(credit_day_overdue_agg, \n",
    "                                                                           how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(credit_day_overdue_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), credit_day_overdue_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by sum\n",
    "\n",
    "credit_day_overdue_agg = aggregate(credit_day_overdue, by=[\"SK_ID_CURR\"], cat_stats=(\"sum\",))\n",
    "print(\"credit_day_overdue_agg shape:\", credit_day_overdue_agg.shape)\n",
    "\n",
    "credit_day_overdue_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(credit_day_overdue_agg, \n",
    "                                                                           how=\"left\", on=\"SK_ID_CURR\")\n",
    "credit_day_overdue_agg = credit_day_overdue_agg.fillna(0)\n",
    "corrs = corrwith(credit_day_overdue_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), credit_day_overdue_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whether `DAYS_CREDIT_ENDDATE` is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by mean\n",
    "\n",
    "days_credit_enddate_pos = bureau[[\"SK_ID_CURR\"]]\n",
    "days_credit_enddate_pos[\"DAYS_CREDIT_ENDDATE_POS\"] = bureau[\"DAYS_CREDIT_ENDDATE\"] > 0\n",
    "\n",
    "days_credit_enddate_pos_agg = aggregate(days_credit_enddate_pos, by=[\"SK_ID_CURR\"], cat_stats=(\"mean\",))\n",
    "\n",
    "print(\"days_credit_enddate_pos_agg shape:\", days_credit_enddate_pos_agg.shape)\n",
    "\n",
    "days_credit_enddate_pos_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(days_credit_enddate_pos_agg, \n",
    "                                                                           how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(days_credit_enddate_pos_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), \n",
    "                 days_credit_enddate_pos_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by sum\n",
    "days_credit_enddate_pos_agg = aggregate(days_credit_enddate_pos, by=[\"SK_ID_CURR\"], cat_stats=(\"sum\",))\n",
    "\n",
    "print(\"days_credit_enddate_pos_agg shape:\", days_credit_enddate_pos_agg.shape)\n",
    "\n",
    "days_credit_enddate_pos_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(days_credit_enddate_pos_agg, \n",
    "                                                                           how=\"left\", on=\"SK_ID_CURR\")\n",
    "days_credit_enddate_pos_agg = days_credit_enddate_pos_agg.fillna(0)\n",
    "\n",
    "corrs = corrwith(days_credit_enddate_pos_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), \n",
    "                 days_credit_enddate_pos_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whether `DAYS_CREDIT_UPDATE` is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very few cases \n",
    "(bureau[\"DAYS_CREDIT_UPDATE\"] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debt to Credit ratio and Total overdue to debt ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_agg = aggregate(bureau[[\"SK_ID_CURR\", \"AMT_CREDIT_SUM\", \"AMT_CREDIT_SUM_DEBT\", \"AMT_CREDIT_SUM_OVERDUE\"]],\n",
    "                   by=[\"SK_ID_CURR\"], num_stats=(\"sum\",))\n",
    "\n",
    "amt_agg[\"DEBT_TO_CREDIT\"] = amt_agg[\"AMT_CREDIT_SUM_DEBT_sum\"] / amt_agg[\"AMT_CREDIT_SUM_sum\"]\n",
    "amt_agg[\"OVERDUE_TO_DEBT\"] = amt_agg[\"AMT_CREDIT_SUM_OVERDUE_sum\"] / amt_agg[\"AMT_CREDIT_SUM_DEBT_sum\"]\n",
    "\n",
    "amt_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(amt_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "amt_agg[\"DEBT_TO_CREDIT\"] = amt_agg[\"DEBT_TO_CREDIT\"].replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "amt_agg[\"OVERDUE_TO_DEBT\"] = amt_agg[\"OVERDUE_TO_DEBT\"].replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "corrs = corrwith(amt_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), amt_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time between successive loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bet_loans = aggregate(bureau[[\"SK_ID_CURR\", \"DAYS_CREDIT\"]], by=[\"SK_ID_CURR\"], \n",
    "                           num_stats=(mean_diff, var_diff, range_diff,))\n",
    "time_bet_loans = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(time_bet_loans, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in time_bet_loans.columns:\n",
    "    time_bet_loans[col] = time_bet_loans[col].replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    \n",
    "corrs = corrwith(time_bet_loans.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), time_bet_loans[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. `bureau_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = pd.read_csv(os.path.join(INP_DIR, \"bureau_balance.csv\"))\n",
    "bureau_balance = change_dtypes(bureau_balance)\n",
    "\n",
    "print(\"bureau_balance.shape:\", bureau_balance.shape)\n",
    "bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MONTHS_BALANCE min:\", bureau_balance[\"MONTHS_BALANCE\"].min())\n",
    "print(\"MONTHS_BALANCE max:\", bureau_balance[\"MONTHS_BALANCE\"].max())\n",
    "\n",
    "print(\"STATUS unique values:\", bureau_balance[\"STATUS\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals = bureau_balance.isnull().mean()\n",
    "missing_vals = missing_vals.sort_values(ascending=False)\n",
    "print(\"Total number of features:\", bureau_balance.shape[1])\n",
    "print(\"Number of features having missing values:\", (missing_vals > 0).sum())\n",
    "\n",
    "print(\"head:\")\n",
    "display(missing_vals.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bureau_balance_merge = application_train[[\"SK_ID_CURR\"]].merge(bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]], \n",
    "                                                                      how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "bureau_balance_merge = bureau_balance_merge.merge(bureau_balance, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance_merge = bureau_balance_merge.drop([\"SK_ID_BUREAU\"], axis=1)\n",
    "\n",
    "bureau_balance_agg = aggregate(bureau_balance_merge, by=[\"SK_ID_CURR\"], \n",
    "                               num_stats=(\"count\", \"mean\", \"min\", \"max\", np.var),\n",
    "                              cat_stats=(\"sum\", \"mean\"))\n",
    "\n",
    "bureau_balance_agg_uniq = aggregate(bureau_balance_merge, by=[\"SK_ID_CURR\"], dtype=\"cat\", \n",
    "                                    cat_stats=(\"nunique\",))\n",
    "\n",
    "bureau_balance_agg = bureau_balance_agg.merge(bureau_balance_agg_uniq, how=\"outer\", on=\"SK_ID_CURR\")\n",
    "\n",
    "print(\"bureau_balance_agg shape:\", bureau_balance_agg.shape)\n",
    "\n",
    "bureau_balance_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_balance_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_balance_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_balance_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_agg = aggregate(bureau_balance, by=[\"SK_ID_BUREAU\"], \n",
    "                               num_stats=(\"count\", \"mean\", \"min\", \"max\", np.var),\n",
    "                               cat_stats=(\"sum\", \"mean\"))\n",
    "\n",
    "bureau_balance_agg_uniq = aggregate(bureau_balance, by=[\"SK_ID_BUREAU\"], dtype=\"cat\",\n",
    "                                    cat_stats=(\"nunique\",), onehot_encode=False)\n",
    "\n",
    "bureau_balance_agg = bureau_balance_agg.merge(bureau_balance_agg_uniq, how=\"outer\", on=\"SK_ID_BUREAU\")\n",
    "\n",
    "bureau_balance_agg = bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]].merge(bureau_balance_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "\n",
    "bureau_balance_agg = bureau_balance_agg.drop([\"SK_ID_BUREAU\"], axis=1)\n",
    "\n",
    "bureau_balance_agg = aggregate(bureau_balance_agg, by=[\"SK_ID_CURR\"], \n",
    "                              num_stats=(\"count\", \"mean\", \"min\", \"max\", np.var),\n",
    "                              cat_stats=(\"sum\", \"mean\"))\n",
    "\n",
    "print(\"bureau_balance_agg shape:\", bureau_balance_agg.shape)\n",
    "\n",
    "bureau_balance_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(bureau_balance_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(bureau_balance_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), bureau_balance_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. `previous_application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application = pd.read_csv(os.path.join(INP_DIR, \"previous_application.csv\"))\n",
    "previous_application = change_dtypes(previous_application)\n",
    "\n",
    "print(\"previous_application shape:\", previous_application.shape)\n",
    "previous_application.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "`RATE_INTEREST_PRIVILEGED` and `RATE_INTEREST_PRIMARY` have more than 99% missing values. We can drop these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals = previous_application.isnull().mean()\n",
    "missing_vals = missing_vals.sort_values(ascending=False)\n",
    "print(\"Total number of features:\", previous_application.shape[1])\n",
    "print(\"Number of features having missing values:\", (missing_vals > 0).sum())\n",
    "\n",
    "print(\"head:\")\n",
    "display(missing_vals.head(20))\n",
    "\n",
    "previous_application = previous_application.drop([\"RATE_INTEREST_PRIVILEGED\", \"RATE_INTEREST_PRIMARY\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RATE_INTEREST_PRIVILEGED` and `RATE_INTEREST_PRIMARY` have more than 99% missing values. We can drop these two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by count\n",
    "previous_application_agg = aggregate(previous_application.drop([\"SK_ID_PREV\"], axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"num\",\n",
    "                                     num_stats=(\"count\",))\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by mean\n",
    "previous_application_agg = aggregate(previous_application.drop([\"SK_ID_PREV\"], axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"num\",\n",
    "                                     num_stats=(\"mean\",))\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by min\n",
    "previous_application_agg = aggregate(previous_application.drop([\"SK_ID_PREV\"], axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"num\",\n",
    "                                     num_stats=(\"min\",))\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by max\n",
    "previous_application_agg = aggregate(previous_application.drop([\"SK_ID_PREV\"], axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"num\",\n",
    "                                     num_stats=(\"max\",))\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg by var\n",
    "previous_application_agg = aggregate(previous_application.drop([\"SK_ID_PREV\"], axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"num\",\n",
    "                                     num_stats=(np.var,))\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = previous_application.select_dtypes([\"category\", \"object\", \"bool\"]).columns\n",
    "for col in cat_cols:\n",
    "    print(col, previous_application[col].nunique())\n",
    "\n",
    "cat_cols_1 = [col for col in cat_cols if previous_application[col].nunique() <= 5]\n",
    "cat_cols_2 = [col for col in cat_cols if previous_application[col].nunique() > 5]\n",
    "\n",
    "print(\"cat_cols_1\", cat_cols_1)\n",
    "print(\"cat_cols_2\", cat_cols_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only categorical columns having nunique less than or equal to 5\n",
    "# agg by sum and mean\n",
    "previous_application_agg = aggregate(previous_application.drop(cat_cols_2, axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"sum\", \"mean\"))\n",
    "\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only categorical columns having nunique less than or equal to 5\n",
    "# agg by nunique\n",
    "previous_application_agg = aggregate(previous_application.drop(cat_cols_2, axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"nunique\",), onehot_encode=False)\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only categorical columns having nunique less than or equal to 5\n",
    "# agg by sum and mean\n",
    "previous_application_agg = aggregate(previous_application.drop(cat_cols_1, axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"sum\", \"mean\"))\n",
    "\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only categorical columns having nunique less than or equal to 5\n",
    "# agg by nunique\n",
    "previous_application_agg = aggregate(previous_application.drop(cat_cols_1, axis=1), \n",
    "                                     by=[\"SK_ID_CURR\"], dtype=\"cat\", cat_stats=(\"nunique\",), onehot_encode=False)\n",
    "\n",
    "print(\"previous_application_agg shape:\", previous_application_agg.shape)\n",
    "\n",
    "previous_application_agg = application_train[[\"SK_ID_CURR\", \"TARGET\"]].merge(previous_application_agg, \n",
    "                                                                       how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "corrs = corrwith(previous_application_agg.drop([\"SK_ID_CURR\", \"TARGET\"], axis=1), previous_application_agg[\"TARGET\"])\n",
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will similarly aggregate `POS_CASH_balance`, `credit_card_balance` and `installments_payments` tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
