{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == \"object\") and (df[col].nunique() < df.shape[0]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        elif df[col].dtype == int:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtypes(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type(val):\n",
    "    if type(val) == str:\n",
    "        return \"string\"\n",
    "    \n",
    "    if np.issubsctype(type(val), np.number):\n",
    "        return \"number\"\n",
    "    \n",
    "    if callable(val):\n",
    "        return \"function\"\n",
    "    \n",
    "    return str(type(val))\n",
    "\n",
    "\n",
    "class NumColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"median\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (float)}, impute values for some specific columns\n",
    "        :param default: str, function or float, value or function used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if (self._specified_values is not None) and (len(self._specified_values) > 0):\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) == \"number\", \"Impute value for \" + col + \" is not number.\"\n",
    "        \n",
    "        self._default = default\n",
    "        self._default_type = check_type(self._default)\n",
    "        if self._default_type not in [\"number\", \"string\", \"function\"]:\n",
    "            raise ValueError(\"Unsupported stat type \" + self._default_type)\n",
    "    \n",
    "    def _cal_imput_vals(self, df):\n",
    "        cat_cols = df.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        if len(cat_cols) > 0:\n",
    "            raise ValueError(\"There are non-number columns: \" + \", \".join(cat_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        if self._default_type == \"number\":\n",
    "            impute_values = {col: self._default for col in all_cols}\n",
    "            \n",
    "        elif self._default_type == \"string\":\n",
    "            impute_values = getattr(df, self._default)()\n",
    "        \n",
    "        elif self._default_type == \"function\":\n",
    "            impute_values = df.apply(self._default)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown default imputer:\", self._default)\n",
    "        \n",
    "        # if it is a pandas series, turn it into dict\n",
    "        impute_values = dict(impute_values)\n",
    "        if (self._specified_values is None) or (len(self._specified_values) == 0):\n",
    "            return impute_values\n",
    "        \n",
    "        for col in self._specified_values:\n",
    "            impute_values[col] = self._specified_values[col]\n",
    "            \n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        for k, v in self._impute_values.items():\n",
    "            if np.isnan(v):\n",
    "                raise ValueError(\"One of the impute_values is NaN: \" + k)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.fillna(self._impute_values)\n",
    "\n",
    "\n",
    "class CatColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"missing_value\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (str, float, function)}, \n",
    "                                 impute values for some specific columns\n",
    "        :param default: str, used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if (self._specified_values is not None) and (len(self._specified_values) > 0):\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) in [\"string\", \n",
    "                                           \"function\"], \"Impute value for \" + col + \" is \" + check_type(val)\n",
    "        \n",
    "        self._default = default\n",
    "        assert check_type(self._default) == \"string\", \"default must be string\"\n",
    "        \n",
    "        \n",
    "    def _cal_imput_vals(self, df):\n",
    "        num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "        if len(num_cols) > 0:\n",
    "            raise ValueError(\"There are number columns: \" + \", \".join(num_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        impute_values = {col: self._default for col in all_cols}\n",
    "        if (self._specified_values is None) or (len(self._specified_values) == 0):\n",
    "            return impute_values\n",
    "        \n",
    "        for col, val in self._specified_values.items():\n",
    "            dtype = check_type(val)\n",
    "            if dtype == \"string\":\n",
    "                impute_values[col] = val\n",
    "            \n",
    "            elif dtype == \"function\":\n",
    "                impute_values[col] = val(df[col])\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Unknown imputer for \" + col + \": \", str(val))\n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            df_new[col] = df_new[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "            \n",
    "        return df_new\n",
    "\n",
    "\n",
    "def get_colnames_from_regex(df, regex_strings):\n",
    "    cols = []\n",
    "    for regex_str in regex_strings:\n",
    "        cols.extend(df.filter(regex=regex_str).columns.to_list())\n",
    "    return cols\n",
    "\n",
    "\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "        \n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        if self._regex_strings is not None:\n",
    "            cols_imput_with_regex = get_colnames_from_regex(df_train, self._regex_strings)\n",
    "            self._spec_impt_vals_num.update({col: self._spec_impt_regex_val_num \n",
    "                                             for col in cols_imput_with_regex})\n",
    "        \n",
    "        df_num = df_train.select_dtypes([\"number\"])\n",
    "        self._imputer_num = NumColsImputer(specified_values=self._spec_impt_vals_num, \n",
    "                                           default=self._default_imput_vals_num)\n",
    "        self._imputer_num.fit(df_num)\n",
    "        \n",
    "        df_cat = df_train.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        self._imputer_cat = CatColsImputer(specified_values=self._spec_impt_vals_cat, \n",
    "                                           default=self._default_imput_vals_cat)\n",
    "        self._imputer_cat.fit(df_cat)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        dfs = []\n",
    "        num_df = df.select_dtypes([\"number\"])\n",
    "        \n",
    "        if len(self._spec_impt_vals_num) > 0:\n",
    "            some_col = list(self._spec_impt_vals_num.keys())[0]\n",
    "            isnull_df = num_df[[some_col]]\n",
    "            for col in self._spec_impt_vals_num:\n",
    "                isnull_df[col + \"_ISNULL\"] = num_df[col].isnull()\n",
    "                \n",
    "            isnull_df = isnull_df.drop([some_col], axis=\"columns\")\n",
    "            dfs.append(isnull_df)\n",
    "        \n",
    "        num_df = self._imputer_num.transform(num_df)\n",
    "        dfs.append(num_df)\n",
    "        \n",
    "        # cat\n",
    "        cat_df = df.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        cat_df = self._imputer_cat.transform(cat_df)\n",
    "        dfs.append(cat_df)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")\n",
    "    \n",
    "\n",
    "class DefaultImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {}\n",
    "        self._default_imput_vals_num = 0.\n",
    "        \n",
    "        self._spec_impt_vals_cat = {}\n",
    "        self._default_imput_vals_cat = \"missing_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollinearColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold, col_regex=None):\n",
    "        \"\"\"\n",
    "        :param threshold: float in [0, 1], if two columns have correlation greater than threshold\n",
    "                          one of them will be removed\n",
    "        :param col_regex: str, regular expression to select columns\n",
    "        \"\"\"\n",
    "        self._threshold = threshold\n",
    "        self._col_regex = col_regex\n",
    "    \n",
    "    def _collinear_columns(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "            df_sel = df_sel.select_dtypes([\"number\", \"bool\"])\n",
    "        \n",
    "        df_sel = df_sel.astype(\"float32\")\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        corr_mat = df_sel.corr().abs()\n",
    "        self._corr_mat = corr_mat\n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = corr_mat.loc[col_i, col_j]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        self._collin_cols = self._collinear_columns(df, self._threshold)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        all_cols = df.columns.to_list()\n",
    "        nonexist_cols = [col for col in self._collin_cols if col not in all_cols]\n",
    "        if len(nonexist_cols) > 0:\n",
    "            print(\"WARNING: These collinear cols to be droped do not exist in df:\", nonexist_cols)\n",
    "            \n",
    "        droped_col = [col for col in self._collin_cols if col in all_cols]\n",
    "        print(\"Number of columns droped due to collinearity:\", len(droped_col))\n",
    "        return df.drop(droped_col, axis=\"columns\")\n",
    "\n",
    "\n",
    "class ConstantColumnsRemover(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df_train):\n",
    "        all_cols = df_train.columns\n",
    "        self._cols_to_remove = []\n",
    "        \n",
    "        for col in all_cols:\n",
    "            if df_train[col].nunique() == 1:\n",
    "                self._cols_to_remove.append(col)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        print(\"Number of constant columns droped:\", len(self._cols_to_remove))\n",
    "        return df.drop(self._cols_to_remove, axis=\"columns\")\n",
    "                \n",
    "    \n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, train_df):\n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df\n",
    "        \n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        cols_to_drop = [col for col in df_cat.columns if col not in self._cat_cols_ohe]\n",
    "        df_cat = df_cat.drop(cols_to_drop, axis=\"columns\")\n",
    "        \n",
    "        # change to float32\n",
    "        for col in df_cat.columns:\n",
    "            df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        return pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex_cols(columns):\n",
    "    fat_cols = [\"_\".join([str(c) for c in flat_col]) for flat_col in columns.to_flat_index()]\n",
    "    return fat_cols\n",
    "\n",
    "\n",
    "def agg_num_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    cat_cols = df.select_dtypes([\"object\", \"category\"]).columns.to_list()\n",
    "    if len(cat_cols) > 0:\n",
    "        raise ValueError(\"There are non-number cols: \" + \", \".join(cat_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def agg_cat_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "    if len(num_cols) > 0:\n",
    "        raise ValueError(\"There are number cols: \" + \", \".join(num_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "class Aggregator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, by_list_cols, \n",
    "                 num_stats, bool_stats, cat_stats, \n",
    "                 ohe_cat_stats=None,\n",
    "                 ohe_cat_max_class=None,\n",
    "                 iqr=False, minmax_range=False, mean_median_diff=False):\n",
    "        \"\"\"\n",
    "        :param by_list_cols: list of str, cols by which the dataframe is grouped\n",
    "        :param num_stats: list, aggregating functions for numerical columns\n",
    "        :param bool_stats: list, aggregating functions for bool columns\n",
    "        :param cat_stats: list, aggregating functions for category columns\n",
    "        :param ohe_cat_stats: list, aggregating functions for category columns after one-hot encoded\n",
    "        :param ohe_cat_max_class: int, category columns with at most ohe_cat_max_class classes \n",
    "                                 will be one-hot encoded before aggregating, \n",
    "                                 If None, no one-hot encoding will be done.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._by_list_cols = by_list_cols\n",
    "        \n",
    "        self._num_stats = num_stats\n",
    "        self._bool_stats = bool_stats\n",
    "        self._cat_stats = cat_stats\n",
    "        self._ohe_cat_stats = ohe_cat_stats\n",
    "        \n",
    "        if ohe_cat_max_class is None:\n",
    "            self._ohe_cat_max_class = [\"mean\", \"sum\"]\n",
    "        else:\n",
    "            self._ohe_cat_max_class = ohe_cat_max_class\n",
    "        \n",
    "        self._iqr = iqr\n",
    "        self._minmax_range = minmax_range\n",
    "        self._mean_median_diff = mean_median_diff\n",
    "    \n",
    "    def _num_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._num_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _bool_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._bool_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _cat_agg(self, df, by_sers):\n",
    "        agg_df =  agg_cat_cols(df, by_sers, stats=self._cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _ohe_cat_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._ohe_cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    \n",
    "    def _iqr_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        iqr_df = grouped.quantile(0.75) - grouped.quantile(0.25)\n",
    "        iqr_df.columns = [col + \"_iqr\" for col in iqr_df.columns]\n",
    "        return iqr_df\n",
    "    \n",
    "    def _range_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        range_df = grouped.max() - grouped.min()\n",
    "        range_df.columns = [col + \"_range\" for col in range_df.columns]\n",
    "        return range_df\n",
    "    \n",
    "    def _mm_diff_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        diff_df = grouped.mean() - grouped.median()\n",
    "        diff_df.columns = [col + \"_mm_diff\" for col in diff_df.columns]\n",
    "        return diff_df\n",
    "    \n",
    "    def _cat_cols_to_ohe(self, df_train):\n",
    "        cat_cols = []\n",
    "        if self._ohe_cat_max_class is None:\n",
    "            return cat_cols\n",
    "        \n",
    "        for col in self._cat_cols:\n",
    "            if df_train[col].nunique() <=  self._ohe_cat_max_class:\n",
    "                for cl in df_train[col].unique():\n",
    "                    cat_cols.append(col + \"_\" + str(cl))\n",
    "        return cat_cols\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        df_train = df_train.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        self._ohe = OneHotEncoder()\n",
    "        self._ohe.fit(df_train)\n",
    "        \n",
    "        self._bool_cols = df_train.select_dtypes([\"bool\"]).columns.to_list()\n",
    "        self._cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        self._num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        \n",
    "        # cat column names after being one-hot encoded\n",
    "        all_cat_ohe_cols = self._ohe.transform(df_train).columns.to_list()\n",
    "        self._cat_cols_ohe = []\n",
    "        if self._ohe_cat_max_class is not None:\n",
    "            for col in self._cat_cols:\n",
    "                if df_train[col].nunique() <=  self._ohe_cat_max_class:\n",
    "                    for cl in df_train[col].unique():\n",
    "                        ohe_col = col + \"_\" + str(cl)\n",
    "                        \n",
    "                        if ohe_col in all_cat_ohe_cols:\n",
    "                            self._cat_cols_ohe.append(ohe_col)\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        by_sers = [df[col] for col in self._by_list_cols]\n",
    "        df = df.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        for col in self._bool_cols + self._cat_cols + self._num_cols:\n",
    "            if col not in all_cols:\n",
    "                raise ValueError(col + \" exists in train but not in test\")\n",
    "        \n",
    "        dfs = []\n",
    "        \n",
    "        # bool cols\n",
    "        if len(self._bool_cols) > 0:\n",
    "            df_bool = df[self._bool_cols]\n",
    "            print(\"Aggregating bool df with shape:\", df_bool.shape)\n",
    "            df_bool = self._bool_agg(df_bool, by_sers)\n",
    "            \n",
    "            for col in df_bool.columns:\n",
    "                if col.endswith(\"_mean\") or col.endswith(\"_sum\") or col.endswith(\"_entropy\"):\n",
    "                    df_bool[col] = df_bool[col].astype(\"float32\")\n",
    "                \n",
    "                if col.endswith(\"_mode\"):\n",
    "                    df_bool[col] = df_bool[col].astype(\"category\")\n",
    "            dfs.append(df_bool)\n",
    "        \n",
    "        # categorical cols\n",
    "        if len(self._cat_cols) > 0:\n",
    "            df_cat = df[self._cat_cols]\n",
    "            print(\"Aggregating cat df with shape:\", df_cat.shape)\n",
    "            df_cat = self._cat_agg(df_cat, by_sers)\n",
    "            \n",
    "            for col in df_cat.columns:\n",
    "                if col.endswith(\"_mean\") or col.endswith(\"_entropy\"):\n",
    "                    df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "                \n",
    "                if col.endswith(\"_mode\"):\n",
    "                    df_cat[col] = df_cat[col].astype(\"category\")\n",
    "            \n",
    "            dfs.append(df_cat)\n",
    "        \n",
    "        # number cols\n",
    "        df_num = df[self._num_cols]\n",
    "        if df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with shape:\", df_num.shape)\n",
    "            df_num = self._num_agg(df_num, by_sers)\n",
    "            dfs.append(df_num)\n",
    "        \n",
    "        # ohe cat cols\n",
    "        if len(self._cat_cols_ohe) > 0:\n",
    "            df_ohe = self._ohe.transform(df)\n",
    "            for col in self._cat_cols_ohe:\n",
    "                if col not in df_ohe.columns:\n",
    "                    raise ValueError(col + \" is not in cols of df_ohe\")\n",
    "                    \n",
    "            df_ohe = df_ohe[self._cat_cols_ohe]\n",
    "            print(\"Aggregating ohe cat df with shape:\", df_ohe.shape)\n",
    "            \n",
    "            df_ohe = self._ohe_cat_agg(df_ohe, by_sers)\n",
    "            dfs.append(df_ohe)\n",
    "        \n",
    "        # aggregate num cols with iqr, range and mean-median difference\n",
    "        df_num = df.select_dtypes([\"number\"])\n",
    "        print(\"df_num.shape for iqr, mimmax_range, mean_median_diff\", df_num.shape)\n",
    "            \n",
    "        if self._iqr and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with iqr\")\n",
    "            df_iqr = self._iqr_agg(df_num, by_sers)\n",
    "            dfs.append(df_iqr)\n",
    "        \n",
    "        if self._minmax_range and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with range\")\n",
    "            df_range = self._range_agg(df_num, by_sers)\n",
    "            print(\"df_range.shape\", df_range.shape)\n",
    "            dfs.append(df_range)\n",
    "        \n",
    "        if self._mean_median_diff and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with mean-median difference\")\n",
    "            df_diff = self._mm_diff_agg(df_num, by_sers)\n",
    "            dfs.append(df_diff)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(df, matching_key, train_id_ser):\n",
    "    is_train = df[matching_key].isin(train_id_ser.values)\n",
    "    \n",
    "    train = df.loc[is_train, :]\n",
    "    test = df.loc[~is_train, :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(ser):\n",
    "    return ser.mode().values[0]\n",
    "\n",
    "\n",
    "def entropy(ser):\n",
    "    pk = ser.value_counts(normalize=True)\n",
    "    return stats.entropy(pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `application_[train|test]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = [\"^APARTMENTS_\", \"^BASEMENTAREA_\", \"^YEARS_B\", \"^COMMONAREA_\", \n",
    "                               \"^ELEVATORS_\", \"^ENTRANCES_\", \"^FLOORS\", \"^LANDAREA_\", \"^LIVING\", \n",
    "                               \"^NONLIVING\", \"AMT_REQ_CREDIT_BUREAU_\"]\n",
    "        self._spec_impt_regex_val_num = -1.\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"OWN_CAR_AGE\": -1.,\n",
    "                                    \"EXT_SOURCE_1\": 0.,\n",
    "                                    \"EXT_SOURCE_3\": 0.,\n",
    "                                    \"TOTALAREA_MODE\": -1.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "        \n",
    "\n",
    "class ApplNewColsAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df_train):\n",
    "        credit_to_income = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_INCOME_TOTAL\"]\n",
    "        self._cti_min = credit_to_income.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._cti_max = credit_to_income.replace(np.inf, np.nan).max() * 10.\n",
    "        \n",
    "        credit_to_goods = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_GOODS_PRICE\"]\n",
    "        self._ctg_min = credit_to_goods.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._ctg_max = credit_to_goods.replace(np.inf, np.nan).max() * 10.\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        df_new[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df_new[\"AMT_INCOME_TOTAL\"])\n",
    "        df_new[\"DAYS_EMPLOYED_POSITIVE\"] = df_new[\"DAYS_EMPLOYED\"] > 0\n",
    "        days_emp_max = df_new[\"DAYS_EMPLOYED\"].max()\n",
    "        if days_emp_max > 0:\n",
    "            df_new[\"DAYS_EMPLOYED\"] = df_new[\"DAYS_EMPLOYED\"].replace({days_emp_max: 100.})\n",
    "        \n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_INCOME_TOTAL\"]\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(-np.inf, self._cti_min)\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(np.inf, self._cti_max)\n",
    "        \n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_GOODS_PRICE\"]\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(-np.inf, self._ctg_min)\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(np.inf, self._ctg_max)\n",
    "        \n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = load_csv(\"data/download/application_train.csv\")\n",
    "application_test = load_csv(\"data/download/application_test.csv\")\n",
    "\n",
    "appl_train_key = application_train[\"SK_ID_CURR\"]\n",
    "appl_test_key = application_test[\"SK_ID_CURR\"]\n",
    "print(application_train.shape, application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "appl_train = application_train.copy()\n",
    "appl_test = application_test.copy()\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "imputer = ApplImputer()\n",
    "imputer.fit(appl_train)\n",
    "appl_train = imputer.transform(appl_train)\n",
    "appl_test = imputer.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "remover = CollinearColumnRemover(0.95, col_regex=\"_ISNULL$\")\n",
    "remover.fit(appl_train)\n",
    "appl_train = remover.transform(appl_train)\n",
    "appl_test = remover.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "adder = ApplNewColsAdder()\n",
    "adder.fit(appl_train)\n",
    "appl_train = adder.transform(appl_train)\n",
    "appl_test = adder.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    appl_train.to_csv(\"data/data_/application_train.csv\", index=False)\n",
    "    appl_test.to_csv(\"data/data_/application_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `bureau` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BureauImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"DAYS_ENDDATE_FACT\": 100.,\n",
    "                                    \"AMT_CREDIT_MAX_OVERDUE\": -1000.,\n",
    "                                    \"AMT_CREDIT_SUM_DEBT\": 0.,\n",
    "                                    \"AMT_CREDIT_SUM_LIMIT\": 0.,\n",
    "                                    \"AMT_ANNUITY\": -1000.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "\n",
    "def drop_cols_bureau(df):\n",
    "    # mostly zero\n",
    "    cols_to_drop = [\"CREDIT_DAY_OVERDUE\", \"CNT_CREDIT_PROLONG\", \"AMT_CREDIT_SUM_OVERDUE\"]\n",
    "    return df.drop(cols_to_drop, axis=\"columns\")\n",
    "\n",
    "\n",
    "def add_new_cols_bureau(df):\n",
    "    df[\"DAYS_CREDIT_ENDDATE_ISPOSITIVE\"] = df[\"DAYS_CREDIT_ENDDATE\"] > 0\n",
    "\n",
    "    df[\"AMT_CREDIT_MAX_OVERDUE_ISPOSITIVE\"] = df[\"AMT_CREDIT_MAX_OVERDUE\"] > 0\n",
    "    df[\"AMT_CREDIT_SUM_DEBT_ISPOSITIVE\"] = df[\"AMT_CREDIT_SUM_DEBT\"] > 0\n",
    "    df[\"AMT_CREDIT_SUM_LIMIT_ISPOSITIVE\"] = df[\"AMT_CREDIT_SUM_LIMIT\"] > 0\n",
    "    df[\"AMT_ANNUITY_ISPOSITIVE\"] =  df[\"AMT_ANNUITY\"] > 0\n",
    "    \n",
    "    amt_credt_sum = df[\"AMT_CREDIT_SUM\"] + 1\n",
    "    df[\"AMT_CREDIT_MAX_OVERDUE_TO_SUM\"] = df[\"AMT_CREDIT_MAX_OVERDUE\"] / amt_credt_sum\n",
    "    df[\"AMT_CREDIT_SUM_DEBT_TO_SUM\"] = df[\"AMT_CREDIT_SUM_DEBT\"] / amt_credt_sum\n",
    "    df[\"AMT_CREDIT_SUM_LIMIT_TO_SUM\"] = df[\"AMT_CREDIT_SUM_LIMIT\"] / amt_credt_sum\n",
    "    return df\n",
    "\n",
    "\n",
    "def bu_nearest_status(df):\n",
    "    return df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[0]\n",
    "\n",
    "def bu_mode_status_three_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[: 3]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def bu_mode_status_six_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[: 6]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "\n",
    "def add_new_agg_cols_bu(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_CREDIT_ACTIVE\"] = df.groupby(\"SK_ID_CURR\").apply(bu_nearest_status)\n",
    "    results[\"MODE_CREDIT_ACTIVE_THREE\"] = df.groupby(\"SK_ID_CURR\").apply(bu_mode_status_three_nearest)\n",
    "    results[\"MODE_CREDIT_ACTIVE_SIX\"] = df.groupby(\"SK_ID_CURR\").apply(bu_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = load_csv(\"data/download/bureau.csv\")\n",
    "print(\"bureau.shape\", bureau.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_partition(bureau, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"bureau_train.shape\", bureau_train.shape)\n",
    "print(\"bureau_test.shape\", bureau_test.shape)\n",
    "\n",
    "bureau_train_keys = bureau_train[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "bureau_test_keys = bureau_test[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "\n",
    "bureau_train = bureau_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_test = bureau_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "\n",
    "bureau_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "bureau_agg_train = bureau_train.copy()\n",
    "bureau_agg_test = bureau_test.copy()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# drop some cols\n",
    "bureau_agg_train = drop_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = drop_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = BureauImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# add some cols\n",
    "bureau_agg_train = add_new_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = add_new_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_bu(bureau_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bu(bureau_agg_test)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_CURR\"\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "aggregator.fit(bureau_agg_train)\n",
    "bureau_agg_train = aggregator.transform(bureau_agg_train)\n",
    "bureau_agg_test = aggregator.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "# in case \n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear cols\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "bureau_agg_train = bureau_agg_train.reset_index()\n",
    "bureau_agg_test = bureau_agg_test.reset_index()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    bureau_agg_train.to_csv(\"data/data_/bureau_agg_train.csv\", index=False)\n",
    "    bureau_agg_test.to_csv(\"data/data_/bureau_agg_test.csv\", index=False)\n",
    "\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_nearest_status(df):\n",
    "    return df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[0]\n",
    "\n",
    "def bb_mode_status_three_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 3]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def bb_mode_status_six_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 6]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "\n",
    "def add_new_agg_cols_bb(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_STATUS\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_nearest_status)\n",
    "    results[\"MODE_STATUS_THREE_NEAREST\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_mode_status_three_nearest)\n",
    "    results[\"MODE_STATUS_SIX_NEAREST\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = load_csv(\"data/download/bureau_balance.csv\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance = bureau_balance.merge(bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]], how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance = bureau_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "bureau_balance[\"SK_ID_CURR\"] = bureau_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_partition(bureau_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "bureau_balance_train = bureau_balance_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "bureau_balance_test = bureau_balance_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"bureau_balance_train.shape:\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape:\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_bb(bureau_balance_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bb(bureau_balance_agg_test)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_BUREAU\"\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_BUREAU\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new aggs cols\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_balance_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_balance_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = load_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\")\n",
    "bureau_balance_agg_test = load_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.merge(bureau_train_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.merge(bureau_test_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `previous application` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `previous_application.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"RATE_DOWN_PAYMENT\": -1.,\n",
    "                                   \"CNT_PAYMENT\": -10.,\n",
    "                                   \"DAYS_FIRST_DRAWING\": 0., \n",
    "                                   \"DAYS_FIRST_DUE\": 0.,\n",
    "                                   \"DAYS_LAST_DUE_1ST_VERSION\": 0.,\n",
    "                                   \"DAYS_LAST_DUE\": 0.,\n",
    "                                   \"DAYS_TERMINATION\": 0.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {\"NAME_TYPE_SUITE\": \"missing_value\",\n",
    "                                    \"NFLAG_INSURED_ON_APPROVAL\": \"missing_value\"}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "\n",
    "def hour_period_bin(hours):\n",
    "    hours = hours.values\n",
    "    hour_bin = np.array([\"evening\"] * len(hours), dtype=\"object\")\n",
    "    morning_mask = (hours > 5) & (hours < 12)\n",
    "    afternoon_mask = (hours >= 12) & (hours < 18)\n",
    "    \n",
    "    hour_bin[morning_mask] = \"morning\"\n",
    "    hour_bin[afternoon_mask] = \"afternoon\"\n",
    "    return hour_bin\n",
    "\n",
    "\n",
    "def add_new_cols_prev_appl(df):\n",
    "    # add to bool cols to identify if values are non-negative\n",
    "    cols_is_nonneg = [\"DAYS_FIRST_DRAWING\", \"DAYS_FIRST_DUE\", \n",
    "                      \"DAYS_LAST_DUE_1ST_VERSION\", \n",
    "                      \"DAYS_LAST_DUE\", \"DAYS_TERMINATION\"]\n",
    "    for col in cols_is_nonneg:\n",
    "        df[col + \"_IS_NONNEG\"] = df[col] >= 0\n",
    "        \n",
    "        df[\"PERIOD_APPR_PROCESS_START\"] = hour_period_bin(df[\"HOUR_APPR_PROCESS_START\"])\n",
    "        df[\"PERIOD_APPR_PROCESS_START\"] = df[\"PERIOD_APPR_PROCESS_START\"].astype(\"category\")\n",
    "    \n",
    "    df[\"AMT_APPLICATION_TO_CREDIT\"] = df[\"AMT_APPLICATION\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    df[\"AMT_DOWN_PAY_TO_CREDIT\"] = df[\"AMT_DOWN_PAYMENT\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    df[\"AMT_GOODS_PRICE_TO_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def mean_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.mean()\n",
    "\n",
    "def min_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.min()\n",
    "\n",
    "def max_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.max()\n",
    "\n",
    "\n",
    "def add_new_agg_cols_prev_appl(df):\n",
    "    sort_by = [\"DAYS_DECISION\"]\n",
    "    orig_cols = [\"AMT_APPLICATION_TO_CREDIT\", \"AMT_DOWN_PAY_TO_CREDIT\", \"AMT_GOODS_PRICE_TO_CREDIT\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application = load_csv(\"data/download/previous_application.csv\")\n",
    "print(\"previous_application.shape\", previous_application.shape)\n",
    "\n",
    "prev_appl_train, prev_appl_test = train_test_partition(previous_application, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_train_keys = prev_appl_train[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "prev_appl_test_keys = prev_appl_test[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "\n",
    "prev_appl_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "prev_appl_agg_train = prev_appl_train.copy()\n",
    "prev_appl_agg_test = prev_appl_test.copy()\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# drop cols with high percentage of null\n",
    "cols_to_drop = [\"RATE_INTEREST_PRIMARY\", \"RATE_INTEREST_PRIVILEGED\"]\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop(cols_to_drop, axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop(cols_to_drop, axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = PrevApplImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "# add new cols\n",
    "prev_appl_agg_train = add_new_cols_prev_appl(prev_appl_agg_train)\n",
    "prev_appl_agg_test = add_new_cols_prev_appl(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_prev_appl(prev_appl_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_prev_appl(prev_appl_agg_test)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_CURR\"\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = aggregator.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = aggregator.transform(prev_appl_agg_test)\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    prev_appl_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    prev_appl_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = remover.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = remover.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = remover.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = remover.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.reset_index()\n",
    "prev_appl_agg_test = prev_appl_agg_test.reset_index()\n",
    "\n",
    "if True:\n",
    "    prev_appl_agg_train.to_csv(\"data/data_/previous_application_agg_train.csv\", index=False)\n",
    "    prev_appl_agg_test.to_csv(\"data/data_/previous_application_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `POS_CASH_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCBalImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"CNT_INSTALMENT\": -1.,\n",
    "                                   \"CNT_INSTALMENT_FUTURE\": -1.,\n",
    "                                   }\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "\n",
    "def add_new_agg_cols_pc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\", \"SK_DPD\", \"SK_DPD_DEF\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance = load_csv(\"data/download/POS_CASH_balance.csv\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "POS_CASH_balance = POS_CASH_balance.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "POS_CASH_balance = POS_CASH_balance.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]], \n",
    "                                          how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "# drop rows that does not have \"SK_ID_PREV\" in previous_application\n",
    "POS_CASH_balance = POS_CASH_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "POS_CASH_balance[\"SK_ID_CURR\"] = POS_CASH_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "PC_bal_train, PC_bal_test = train_test_partition(POS_CASH_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "PC_bal_train = PC_bal_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "PC_bal_test = PC_bal_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"PC_bal_train.shape:\", PC_bal_train.shape)\n",
    "print(\"PC_bal_test.shape:\", PC_bal_test.shape)\n",
    "\n",
    "PC_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "PC_bal_agg_train = PC_bal_train.copy()\n",
    "PC_bal_agg_test = PC_bal_test.copy()\n",
    "\n",
    "# impute missing values\n",
    "imputer = PCBalImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_pc_bal(PC_bal_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_pc_bal(PC_bal_agg_test)\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_PREV\"\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    PC_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    PC_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_train_tmp.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over SK_ID_CURR\n",
    "time_start = time.time()\n",
    "\n",
    "# this turns [0, 1] into bool\n",
    "PC_bal_agg_train = load_csv(\"data/data_/POS_CASH_balance_agg_train_tmp.csv\")\n",
    "PC_bal_agg_test = load_csv(\"data/data_/POS_CASH_balance_agg_test_tmp.csv\")\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_train.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `installments_payments.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstalPayImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._days_entry_payment_impute = df_train[\"DAYS_ENTRY_PAYMENT\"].min() - 10\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df[\"DAYS_ENTRY_PAYMENT\"] = df[\"DAYS_ENTRY_PAYMENT\"].fillna(self._days_entry_payment_impute)\n",
    "        df[\"AMT_PAYMENT\"] = df[\"AMT_PAYMENT\"].fillna(0.)\n",
    "        return df\n",
    "\n",
    "\n",
    "def add_new_cols_inst_pay(df):\n",
    "    df[\"DAYS_INSTAL_PAY_DIFF\"] = df[\"DAYS_ENTRY_PAYMENT\"] - df[\"DAYS_INSTALMENT\"]\n",
    "    df[\"DAYS_INSTAL_PAY_DIFF_ISPOSITIVE\"] = df[\"DAYS_INSTAL_PAY_DIFF\"] > 0\n",
    "    \n",
    "    df[\"AMT_INSTAL_PAY_DIFF\"] = df[\"AMT_PAYMENT\"] - df[\"AMT_INSTALMENT\"]\n",
    "    df[\"AMT_INSTAL_PAY_DIFF_ISPOSITIVE\"] = df[\"AMT_INSTAL_PAY_DIFF\"] > 0\n",
    "    \n",
    "    df[\"AMT_PAY_INSTAL_RATIO\"] = np.clip((df[\"AMT_PAYMENT\"] + 1) / (df[\"AMT_INSTALMENT\"] + 1), 0., 10.)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_agg_cols_inst_pay(df):\n",
    "    sort_by = [\"DAYS_INSTALMENT\"]\n",
    "    orig_cols = [\"DAYS_INSTAL_PAY_DIFF\", \"AMT_INSTAL_PAY_DIFF\", \"AMT_PAY_INSTAL_RATIO\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments = load_csv(\"data/download/installments_payments.csv\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "installments_payments = installments_payments.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "installments_payments = installments_payments.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]],\n",
    "                                                   how=\"left\", on=\"SK_ID_PREV\")\n",
    "\n",
    "# drop rows that does not have \"SK_ID_PREV\" in previous_application\n",
    "installments_payments = installments_payments.dropna(subset=[\"SK_ID_CURR\"])\n",
    "installments_payments[\"SK_ID_CURR\"] = installments_payments[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "inst_pay_train, inst_pay_test = train_test_partition(installments_payments, \"SK_ID_CURR\", appl_train_key)\n",
    "inst_pay_train = inst_pay_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "inst_pay_test = inst_pay_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"inst_pay_train.shape:\", inst_pay_train.shape)\n",
    "print(\"inst_pay_test.shape:\", inst_pay_test.shape)\n",
    "\n",
    "inst_pay_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "inst_pay_agg_train = inst_pay_train.copy()\n",
    "inst_pay_agg_test = inst_pay_test.copy()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "imputer = InstalPayImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# add some more cols\n",
    "inst_pay_agg_train = add_new_cols_inst_pay(inst_pay_agg_train)\n",
    "inst_pay_agg_test = add_new_cols_inst_pay(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_inst_pay(inst_pay_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_inst_pay(inst_pay_agg_test)\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_PREV\"\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    inst_pay_agg_train[col] = new_agg_cols_train[col]\n",
    "    \n",
    "for col in new_agg_cols_test:\n",
    "    inst_pay_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# just in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_train_tmp.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "inst_pay_agg_train = load_csv(\"data/data_/installments_payments_agg_train_tmp.csv\")\n",
    "inst_pay_agg_test = load_csv(\"data/data_/installments_payments_agg_test_tmp.csv\")\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# just in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_train.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `credit_card_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCBalImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"AMT_DRAWINGS_ATM_CURRENT\": 0.,\n",
    "                                   \"AMT_DRAWINGS_OTHER_CURRENT\": 0., \n",
    "                                    \"AMT_DRAWINGS_POS_CURRENT\": 0.,\n",
    "                                    \"AMT_INST_MIN_REGULARITY\": 0.,\n",
    "                                    \"AMT_PAYMENT_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_ATM_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_OTHER_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_POS_CURRENT\": 0.,\n",
    "                                    \"CNT_INSTALMENT_MATURE_CUM\": 0.,\n",
    "                                    \n",
    "                                   }\n",
    "        self._default_imput_vals_num = 0.\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "        \n",
    "\n",
    "def add_new_cols_cc_bal(df):\n",
    "    df[\"AMT_PAYMENT_TO_BALANCE\"] = df[\"AMT_PAYMENT_TOTAL_CURRENT\"] / df[\"AMT_BALANCE\"].replace(0., 1.)\n",
    "    df[\"AMT_BALANCE_TO_CREDIT_LIMIT\"] = (df[\"AMT_BALANCE\"] + 1) / (df[\"AMT_CREDIT_LIMIT_ACTUAL\"] + 1)\n",
    "    df[\"AMT_DRAWING_TOTAL\"] = (df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_CURRENT\"] + \n",
    "                              df[\"AMT_DRAWINGS_OTHER_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"])\n",
    "    df[\"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\"] = df[\"AMT_DRAWING_TOTAL\"] / (df[\"AMT_CREDIT_LIMIT_ACTUAL\"] + 1)\n",
    "    df[\"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\"] = (df[\"AMT_PAYMENT_TOTAL_CURRENT\"] / \n",
    "                                                df[\"AMT_DRAWING_TOTAL\"].replace(0., 1.))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_agg_cols_cc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"AMT_PAYMENT_TO_BALANCE\", \"AMT_BALANCE_TO_CREDIT_LIMIT\", \n",
    "                 \"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\", \"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\", \n",
    "                 \"SK_DPD\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance = load_csv(\"data/download/credit_card_balance.csv\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "credit_card_balance = credit_card_balance.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "credit_card_balance = credit_card_balance.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]],\n",
    "                                               how=\"left\", on=\"SK_ID_PREV\")\n",
    "\n",
    "credit_card_balance = credit_card_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "credit_card_balance[\"SK_ID_CURR\"] = credit_card_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "cc_bal_train, cc_bal_test = train_test_partition(credit_card_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "cc_bal_train = cc_bal_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "cc_bal_test = cc_bal_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"cc_bal_train.shape:\", cc_bal_train.shape)\n",
    "print(\"cc_bal_test.shape:\", cc_bal_test.shape)\n",
    "\n",
    "cc_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "cc_bal_agg_train = cc_bal_train.copy()\n",
    "cc_bal_agg_test = cc_bal_test.copy()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = CCBalImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new cols\n",
    "cc_bal_agg_train = add_new_cols_cc_bal(cc_bal_agg_train)\n",
    "cc_bal_agg_test = add_new_cols_cc_bal(cc_bal_agg_test)\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_cc_bal(cc_bal_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_cc_bal(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_PREV\"\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    cc_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    cc_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# just in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_train_tmp.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "cc_bal_agg_train = load_csv(\"data/data_/credit_card_balance_agg_train_tmp.csv\")\n",
    "cc_bal_agg_test = load_csv(\"data/data_/credit_card_balance_agg_test_tmp.csv\")\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "cc_bal_agg_train = cc_bal_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "cc_bal_agg_test = cc_bal_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "cc_bal_agg_train = cc_bal_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "cc_bal_agg_test = cc_bal_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"min\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# remove constant columns\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_train.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_col_align(df_train, df_test, exclude_cols=None):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    cols_train = df_train.columns.to_list()\n",
    "    \n",
    "    for col in exclude_cols:\n",
    "        assert col in cols_train, col + \" is not in df_train\"\n",
    "        \n",
    "    test_cols = [col for col in cols_train if col not in exclude_cols]\n",
    "    return df_train[test_cols + exclude_cols], df_test[test_cols]\n",
    "\n",
    "\n",
    "def add_prefix_to_cols(df, prefix, exclude_cols=None):\n",
    "    df = df.copy()\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "        \n",
    "    cols = df.columns.to_list()\n",
    "    for i, col in enumerate(cols):\n",
    "        if col not in exclude_cols:\n",
    "            cols[i] = prefix + col\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframes(df_left, dfs_right, \n",
    "                     left_prefix, right_prefixes, \n",
    "                     on_col=\"SK_ID_CURR\"):\n",
    "    assert isinstance(dfs_right, list), \"dfs_right must be a list\"\n",
    "    assert isinstance(right_prefixes, list), \"right_prefixes must be a list\"\n",
    "    assert len(dfs_right) == len(right_prefixes), \"dfs_right and right_prefixes must have the same len\"\n",
    "    \n",
    "    result = df_left\n",
    "    result = add_prefix_to_cols(result, left_prefix, exclude_cols=[on_col])\n",
    "    \n",
    "    for df, prefix in zip(dfs_right, right_prefixes):\n",
    "        print(\"Merging with \" + prefix)\n",
    "        df = add_prefix_to_cols(df, prefix, exclude_cols=[on_col])\n",
    "        result = result.merge(df, how=\"left\", on=on_col)\n",
    "        \n",
    "        df_cols = [col for col in result.columns if col.startswith(prefix)]\n",
    "        # add mask column to tell which rows in df are missing\n",
    "        result[prefix + \"MISSING_ROW\"] = result[df_cols[0]].isnull()\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "class MergeMissingImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cols = df_train.columns.to_list()\n",
    "        cols_num = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        cols_cat = [col for col in cols if col not in cols_num]\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in cols_num:\n",
    "            if df_train[col].isnull().sum() > 0:\n",
    "                self._impute_values[col] = df_train[col].median()\n",
    "        \n",
    "        for col in cols_cat:\n",
    "            if df_train[col].isnull().sum() > 0:\n",
    "                self._impute_values[col] = mode(df_train[col])\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            try:\n",
    "                df[col] = df[col].fillna(val)\n",
    "            except AttributeError:\n",
    "                print(\"problem with \" + col)\n",
    "                raise AttributeError()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = load_csv(\"data/data_/application_train.csv\")\n",
    "application_test = load_csv(\"data/data_/application_test.csv\")\n",
    "\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)\n",
    "\n",
    "application_train, application_test = train_test_col_align(application_train, application_test, \n",
    "                                                           exclude_cols=[\"TARGET\"])\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_train = load_csv(\"data/data_/bureau_agg_train.csv\")\n",
    "bureau_test = load_csv(\"data/data_/bureau_agg_test.csv\")\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_col_align(bureau_train, bureau_test)\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_train = load_csv(\"data/data_/bureau_balance_agg_train.csv\")\n",
    "bureau_balance_test = load_csv(\"data/data_/bureau_balance_agg_test.csv\")\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_col_align(bureau_balance_train, bureau_balance_test)\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `previous_application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application_train = load_csv(\"data/data_/previous_application_agg_train.csv\")\n",
    "previous_application_test = load_csv(\"data/data_/previous_application_agg_test.csv\")\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)\n",
    "\n",
    "previous_application_train, previous_application_test = train_test_col_align(previous_application_train, \n",
    "                                                                             previous_application_test)\n",
    "\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `POS_CASH_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance_train = load_csv(\"data/data_/POS_CASH_balance_agg_train.csv\")\n",
    "POS_CASH_balance_test = load_csv(\"data/data_/POS_CASH_balance_agg_test.csv\")\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)\n",
    "\n",
    "POS_CASH_balance_train, POS_CASH_balance_test = train_test_col_align(POS_CASH_balance_train, POS_CASH_balance_test)\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `installments_payments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments_train = load_csv(\"data/data_/installments_payments_agg_train.csv\")\n",
    "installments_payments_test = load_csv(\"data/data_/installments_payments_agg_test.csv\")\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)\n",
    "\n",
    "installments_payments_train, installments_payments_test = train_test_col_align(installments_payments_train, \n",
    "                                                                               installments_payments_test)\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `credit_card_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance_train = load_csv(\"data/data_/credit_card_balance_agg_train.csv\")\n",
    "credit_card_balance_test = load_csv(\"data/data_/credit_card_balance_agg_test.csv\")\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)\n",
    "\n",
    "credit_card_balance_train, credit_card_balance_test = train_test_col_align(credit_card_balance_train, \n",
    "                                                                           credit_card_balance_test)\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train = merge_dataframes(application_train, \n",
    "                               [bureau_train, bureau_balance_train, \n",
    "                                previous_application_train, POS_CASH_balance_train,\n",
    "                                installments_payments_train, credit_card_balance_train], \n",
    "                               \"APPL_\", [\"BURE_\", \"BUBA_\", \"PRAP_\", \"POBA_\", \"INPA_\", \"CCBA_\"])\n",
    "\n",
    "merge_test = merge_dataframes(application_test, \n",
    "                              [bureau_test, bureau_balance_test, \n",
    "                               previous_application_test, POS_CASH_balance_test,\n",
    "                               installments_payments_test, credit_card_balance_test], \n",
    "                              \"APPL_\", [\"BURE_\", \"BUBA_\", \"PRAP_\", \"POBA_\", \"INPA_\", \"CCBA_\"])\n",
    "\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "print(\"merge_train.isnull().sum().sum()\", merge_train.isnull().sum().sum())\n",
    "print(\"merge_test.isnull().sum().sum()\", merge_test.isnull().sum().sum())\n",
    "\n",
    "imputer = MergeMissingImputer()\n",
    "imputer.fit(merge_train)\n",
    "merge_train = imputer.transform(merge_train)\n",
    "merge_test = imputer.transform(merge_test)\n",
    "\n",
    "print(\"merge_train.isnull().sum().sum()\", merge_train.isnull().sum().sum())\n",
    "print(\"merge_test.isnull().sum().sum()\", merge_test.isnull().sum().sum())\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(merge_train)\n",
    "merge_train = remover.transform(merge_train)\n",
    "merge_test = remover.transform(merge_test)\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "merge_train, merge_test = train_test_col_align(merge_train, merge_test, exclude_cols=[\"APPL_TARGET\"])\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train.to_csv(\"data/data_/X_y_train.csv\", index=False)\n",
    "merge_test.to_csv(\"data/data_/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del application_train, application_test\n",
    "del bureau_train, bureau_test\n",
    "del bureau_balance_train, bureau_balance_test\n",
    "del previous_application_train, previous_application_test\n",
    "del POS_CASH_balance_train, POS_CASH_balance_test\n",
    "del installments_payments_train, installments_payments_test\n",
    "del credit_card_balance_train, credit_card_balance_test\n",
    "del merge_train, merge_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportantFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator, threshold):\n",
    "        self._fea_impt = estimator.feature_importances_\n",
    "        self._total_features = len(self._fea_impt)\n",
    "        \n",
    "        if isinstance(threshold, int):\n",
    "            self._n_sel_features = threshold\n",
    "        elif isinstance(threshold, float) and (0 <= threshold <= 1):\n",
    "            self._n_sel_features = int(np.ceil(self._total_features * threshold))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown value of threshold\" + str(threshold))\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        features = df_train.columns.to_list()\n",
    "        assert len(features) == self._total_features\n",
    "        \n",
    "        feature_imp = pd.DataFrame({\"feature\": features, \"importance\": self._fea_impt})\n",
    "        feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "        \n",
    "        self._sel_cols = feature_imp[\"feature\"][: self._n_sel_features].values\n",
    "        self._sel_cols = list(self._sel_cols)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.loc[:, self._sel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def feature_importance_df(estimator, features):\n",
    "    \"\"\"\n",
    "    :param estimator: an estimator object that has feature_importances_ attribute\n",
    "    :param features: list of str, list of feature names\n",
    "    :return: feature_imp, dataframe\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\"feature\": features, \"importance\": estimator.feature_importances_})\n",
    "    feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4435.54 MB\n",
      "Memory usage after changing types 2178.73 MB\n",
      "Memory usage before changing types 702.69 MB\n",
      "Memory usage after changing types 345.17 MB\n",
      "X_train.shape (307511, 1817)\n",
      "X_test.shape (48744, 1816)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 1815)\n",
      "X_test.shape (48744, 1815)\n",
      "X_train.shape (307511, 1815)\n",
      "X_test.shape (48744, 1815)\n",
      "Elapsed Time 1081.1118476390839\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(\"data/data_/X_y_train.csv\")\n",
    "X_test = load_csv(\"data/data_/X_test.csv\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"SK_ID_CURR\", \"APPL_TARGET\"], axis=\"columns\")\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (307511, 2084)\n",
      "X_test.shape (48744, 2084)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "# make sure that columns in train and test are aligned\n",
    "X_train, X_test = train_test_col_align(X_train, X_test)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use feature importance from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, min_samples_leaf=40, n_jobs=16, random_state=21083)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "auc_rf_train = roc_auc(rf, X_train, y_train)\n",
    "print(\"AUC of Random Forest model on the train set: %0.5f\" % auc_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance_df(rf, features)\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(feature_importance.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[\"importance_cumsum\"] = feature_importance[\"importance\"].cumsum()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "ax.plot(np.arange(feature_importance.shape[0]) + 1, feature_importance[\"importance_cumsum\"].values, lw=\"2.\")\n",
    "ax.set_xlabel(\"# of features\")\n",
    "ax.set_ylabel(\"Cumulative feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ImportantFeatureSelector(rf, threshold=1300)\n",
    "selector.fit(X_train)\n",
    "X_sel_train = selector.transform(X_train)\n",
    "X_sel_test = selector.transform(X_test)\n",
    "\n",
    "print(\"X_sel_train.shape\", X_sel_train.shape)\n",
    "print(\"X_sel_test.shape\", X_sel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_train[\"APPL_TARGET\"] = y_train\n",
    "\n",
    "X_sel_train.to_csv(\"data/data_/X_y_sel_rf_train.csv\", index=False)\n",
    "X_sel_test.to_csv(\"data/data_/X_sel_rf_test.csv\", index=False)\n",
    "\n",
    "del X_sel_train, X_sel_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use feature importance from XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBoost model on the train set: 0.88772\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_jobs=16)\n",
    "#xgb = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>APPL_EXT_SOURCE_3</td>\n",
       "      <td>0.008175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>APPL_EXT_SOURCE_2</td>\n",
       "      <td>0.007345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APPL_EXT_SOURCE_3_ISNULL</td>\n",
       "      <td>0.006334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>APPL_NAME_INCOME_TYPE_Working</td>\n",
       "      <td>0.005884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>APPL_FLAG_OWN_CAR_Y</td>\n",
       "      <td>0.004039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>APPL_NAME_EDUCATION_TYPE_Higher education</td>\n",
       "      <td>0.003860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>INPA_AMT_INSTAL_PAY_DIFF_ISPOSITIVE_sum_range</td>\n",
       "      <td>0.003659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>APPL_FLAG_DOCUMENT_3</td>\n",
       "      <td>0.003397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>PRAP_NAME_CONTRACT_STATUS_Refused_mean</td>\n",
       "      <td>0.003350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>BURE_AMT_CREDIT_SUM_DEBT_TO_SUM_mean</td>\n",
       "      <td>0.003285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>APPL_AMT_REQ_CREDIT_BUREAU_HOUR_ISNULL</td>\n",
       "      <td>0.003271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>PRAP_AMT_DOWN_PAYMENT_var</td>\n",
       "      <td>0.003246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>APPL_CODE_GENDER_M</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>APPL_CREDIT_TO_GOODS</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>INPA_DAYS_INSTAL_PAY_DIFF_ISPOSITIVE_entropy_min</td>\n",
       "      <td>0.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>PRAP_DAYS_LAST_DUE_1ST_VERSION_IS_NONNEG_mean</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>PRAP_CODE_REJECT_REASON_XAP_mean</td>\n",
       "      <td>0.002850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>APPL_REG_CITY_NOT_LIVE_CITY</td>\n",
       "      <td>0.002782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>CCBA_MAX_AMT_BALANCE_TO_CREDIT_LIMIT_3_NEAREST...</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>APPL_EXT_SOURCE_1</td>\n",
       "      <td>0.002663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature  importance\n",
       "40                                    APPL_EXT_SOURCE_3    0.008175\n",
       "39                                    APPL_EXT_SOURCE_2    0.007345\n",
       "2                              APPL_EXT_SOURCE_3_ISNULL    0.006334\n",
       "1788                      APPL_NAME_INCOME_TYPE_Working    0.005884\n",
       "1773                                APPL_FLAG_OWN_CAR_Y    0.004039\n",
       "1789          APPL_NAME_EDUCATION_TYPE_Higher education    0.003860\n",
       "1281      INPA_AMT_INSTAL_PAY_DIFF_ISPOSITIVE_sum_range    0.003659\n",
       "47                                 APPL_FLAG_DOCUMENT_3    0.003397\n",
       "507              PRAP_NAME_CONTRACT_STATUS_Refused_mean    0.003350\n",
       "150                BURE_AMT_CREDIT_SUM_DEBT_TO_SUM_mean    0.003285\n",
       "12               APPL_AMT_REQ_CREDIT_BUREAU_HOUR_ISNULL    0.003271\n",
       "438                           PRAP_AMT_DOWN_PAYMENT_var    0.003246\n",
       "1771                                 APPL_CODE_GENDER_M    0.003241\n",
       "72                                 APPL_CREDIT_TO_GOODS    0.003188\n",
       "967    INPA_DAYS_INSTAL_PAY_DIFF_ISPOSITIVE_entropy_min    0.003070\n",
       "400       PRAP_DAYS_LAST_DUE_1ST_VERSION_IS_NONNEG_mean    0.002853\n",
       "519                    PRAP_CODE_REJECT_REASON_XAP_mean    0.002850\n",
       "35                          APPL_REG_CITY_NOT_LIVE_CITY    0.002782\n",
       "1651  CCBA_MAX_AMT_BALANCE_TO_CREDIT_LIMIT_3_NEAREST...    0.002774\n",
       "38                                    APPL_EXT_SOURCE_1    0.002663"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = feature_importance_df(xgb, features)\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative feature importance')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance[\"importance_cumsum\"] = feature_importance[\"importance\"].cumsum()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "ax.plot(np.arange(feature_importance.shape[0]) + 1, feature_importance[\"importance_cumsum\"].values, lw=\"2.\")\n",
    "ax.set_xlabel(\"# of features\")\n",
    "ax.set_ylabel(\"Cumulative feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sel_train.shape (307511, 1100)\n",
      "X_sel_test.shape (48744, 1100)\n"
     ]
    }
   ],
   "source": [
    "selector = ImportantFeatureSelector(xgb, threshold=1100)\n",
    "selector.fit(X_train)\n",
    "X_sel_train = selector.transform(X_train)\n",
    "X_sel_test = selector.transform(X_test)\n",
    "\n",
    "print(\"X_sel_train.shape\", X_sel_train.shape)\n",
    "print(\"X_sel_test.shape\", X_sel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_train[\"APPL_TARGET\"] = y_train\n",
    "\n",
    "X_sel_train.to_csv(\"data/data_/X_y_sel_xgb_train.csv\", index=False)\n",
    "X_sel_test.to_csv(\"data/data_/X_sel_xgb_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
