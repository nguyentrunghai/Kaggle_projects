{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser, make_01_cat=True):\n",
    "    \n",
    "    if make_01_cat:\n",
    "        if set(ser.unique()) == set([0, 1]):\n",
    "            return ser.astype(\"category\")\n",
    "        \n",
    "        if set(ser.unique()) == set([0., 1.]):\n",
    "            return ser.astype(\"category\")\n",
    "    \n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == \"object\":\n",
    "        if ser.nunique() < ser.shape[0]:\n",
    "            return ser.astype(\"category\")\n",
    "        else:\n",
    "            raise TypeError(ser.name + \": type is object but are all distinct\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "def change_dtype_df(df, make_01_cat=True):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col], make_01_cat=make_01_cat)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename, make_01_cat=True):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df, make_01_cat=make_01_cat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type(val):\n",
    "    if type(val) == str:\n",
    "        return \"string\"\n",
    "    \n",
    "    if np.issubsctype(type(val), np.number):\n",
    "        return \"number\"\n",
    "    \n",
    "    if callable(val):\n",
    "        return \"function\"\n",
    "    \n",
    "    return str(type(val))\n",
    "\n",
    "\n",
    "class NumColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"median\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (float)}, impute values for some specific columns\n",
    "        :param default: str, function or float, value or function used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if (self._specified_values is not None) and (len(self._specified_values) > 0):\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) == \"number\", \"Impute value for \" + col + \" is not number.\"\n",
    "        \n",
    "        self._default = default\n",
    "        self._default_type = check_type(self._default)\n",
    "        if self._default_type not in [\"number\", \"string\", \"function\"]:\n",
    "            raise ValueError(\"Unsupported stat type \" + self._default_type)\n",
    "    \n",
    "    def _cal_imput_vals(self, df):\n",
    "        cat_cols = df.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        if len(cat_cols) > 0:\n",
    "            raise ValueError(\"There are non-number columns: \" + \", \".join(cat_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        if self._default_type == \"number\":\n",
    "            impute_values = {col: self._default for col in all_cols}\n",
    "            \n",
    "        elif self._default_type == \"string\":\n",
    "            impute_values = getattr(df, self._default)()\n",
    "        \n",
    "        elif self._default_type == \"function\":\n",
    "            impute_values = df.apply(self._default)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown default imputer:\", self._default)\n",
    "        \n",
    "        # if it is a pandas series, turn it into dict\n",
    "        impute_values = dict(impute_values)\n",
    "        if (self._specified_values is None) or (len(self._specified_values) == 0):\n",
    "            return impute_values\n",
    "        \n",
    "        for col in self._specified_values:\n",
    "            impute_values[col] = self._specified_values[col]\n",
    "            \n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        for k, v in self._impute_values.items():\n",
    "            if np.isnan(v):\n",
    "                raise ValueError(\"One of the impute_values is NaN: \" + k)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.fillna(self._impute_values)\n",
    "\n",
    "\n",
    "class CatColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"missing_value\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (str, float, function)}, \n",
    "                                 impute values for some specific columns\n",
    "        :param default: str, used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if (self._specified_values is not None) and (len(self._specified_values) > 0):\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) in [\"string\", \n",
    "                                           \"function\"], \"Impute value for \" + col + \" is \" + check_type(val)\n",
    "        \n",
    "        self._default = default\n",
    "        assert check_type(self._default) == \"string\", \"default must be string\"\n",
    "        \n",
    "        \n",
    "    def _cal_imput_vals(self, df):\n",
    "        num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "        if len(num_cols) > 0:\n",
    "            raise ValueError(\"There are number columns: \" + \", \".join(num_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        impute_values = {col: self._default for col in all_cols}\n",
    "        if (self._specified_values is None) or (len(self._specified_values) == 0):\n",
    "            return impute_values\n",
    "        \n",
    "        for col, val in self._specified_values.items():\n",
    "            dtype = check_type(val)\n",
    "            if dtype == \"string\":\n",
    "                impute_values[col] = val\n",
    "            \n",
    "            elif dtype == \"function\":\n",
    "                impute_values[col] = val(df[col])\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Unknown imputer for \" + col + \": \", str(val))\n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            df_new[col] = df_new[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "            \n",
    "        return df_new\n",
    "\n",
    "\n",
    "def get_colnames_from_regex(df, regex_strings):\n",
    "    cols = []\n",
    "    for regex_str in regex_strings:\n",
    "        cols.extend(df.filter(regex=regex_str).columns.to_list())\n",
    "    return cols\n",
    "\n",
    "\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "        \n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        df_num = df_train.select_dtypes([\"number\"])\n",
    "        cols_num = df_num.columns.to_list()\n",
    "                \n",
    "        if self._regex_strings is not None:\n",
    "            cols_imput_with_regex = get_colnames_from_regex(df_train, self._regex_strings)\n",
    "            # make sure they are in cols_num\n",
    "            cols_imput_with_regex = [col for col in cols_imput_with_regex if col in cols_num]\n",
    "            \n",
    "            self._spec_impt_vals_num.update({col: self._spec_impt_regex_val_num \n",
    "                                             for col in cols_imput_with_regex})\n",
    "        \n",
    "        self._imputer_num = NumColsImputer(specified_values=self._spec_impt_vals_num, \n",
    "                                           default=self._default_imput_vals_num)\n",
    "        self._imputer_num.fit(df_num)\n",
    "        \n",
    "        df_cat = df_train.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        self._imputer_cat = CatColsImputer(specified_values=self._spec_impt_vals_cat, \n",
    "                                           default=self._default_imput_vals_cat)\n",
    "        self._imputer_cat.fit(df_cat)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        dfs = []\n",
    "        num_df = df.select_dtypes([\"number\"])\n",
    "        \n",
    "        if len(self._spec_impt_vals_num) > 0:\n",
    "            some_col = list(self._spec_impt_vals_num.keys())[0]\n",
    "            isnull_df = num_df[[some_col]]\n",
    "            for col in self._spec_impt_vals_num:\n",
    "                isnull_df[col + \"_ISNULL\"] = num_df[col].isnull()\n",
    "                \n",
    "            isnull_df = isnull_df.drop([some_col], axis=\"columns\")\n",
    "            dfs.append(isnull_df)\n",
    "        \n",
    "        num_df = self._imputer_num.transform(num_df)\n",
    "        dfs.append(num_df)\n",
    "        \n",
    "        # cat\n",
    "        cat_df = df.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        cat_df = self._imputer_cat.transform(cat_df)\n",
    "        dfs.append(cat_df)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")\n",
    "    \n",
    "\n",
    "class DefaultImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {}\n",
    "        self._default_imput_vals_num = 0.\n",
    "        \n",
    "        self._spec_impt_vals_cat = {}\n",
    "        self._default_imput_vals_cat = \"missing_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollinearColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold, col_regex=None, exclude_cols=None):\n",
    "        \"\"\"\n",
    "        :param threshold: float in [0, 1], if two columns have correlation greater than threshold\n",
    "                          one of them will be removed\n",
    "        :param col_regex: str, regular expression to select columns\n",
    "        \"\"\"\n",
    "        self._threshold = threshold\n",
    "        self._col_regex = col_regex\n",
    "        if exclude_cols is None:\n",
    "            self._exclude_cols = []\n",
    "        else:\n",
    "            self._exclude_cols = exclude_cols\n",
    "    \n",
    "    def _collinear_columns(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "            df_sel = df_sel.select_dtypes([\"number\", \"bool\"])\n",
    "        \n",
    "        df_sel = df_sel.astype(\"float32\")\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        all_cols = [col for col in all_cols if col not in self._exclude_cols]\n",
    "        df_sel = df_sel[all_cols]\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        corr_mat = df_sel.corr().abs()\n",
    "        self._corr_mat = corr_mat\n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = corr_mat.loc[col_i, col_j]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        self._collin_cols = self._collinear_columns(df, self._threshold)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        all_cols = df.columns.to_list()\n",
    "        nonexist_cols = [col for col in self._collin_cols if col not in all_cols]\n",
    "        if len(nonexist_cols) > 0:\n",
    "            print(\"WARNING: These collinear cols to be droped do not exist in df:\", nonexist_cols)\n",
    "            \n",
    "        droped_col = [col for col in self._collin_cols if col in all_cols]\n",
    "        print(\"Number of columns droped due to collinearity:\", len(droped_col))\n",
    "        return df.drop(droped_col, axis=\"columns\")\n",
    "\n",
    "\n",
    "class ConstantColumnsRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, exclude_cols=None):\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + exclude_cols\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        all_cols = [col for col in df_train.columns if col not in self._exclude_cols]\n",
    "        self._cols_to_remove = []\n",
    "        \n",
    "        for col in all_cols:\n",
    "            if df_train[col].nunique() < 2:\n",
    "                self._cols_to_remove.append(col)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        print(\"Number of constant columns droped:\", len(self._cols_to_remove))\n",
    "        return df.drop(self._cols_to_remove, axis=\"columns\")\n",
    "                \n",
    "        \n",
    "class SameCatColsRemover(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, threshold=0.99):\n",
    "        self._threshold = threshold\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        ncols = len(cols)\n",
    "        \n",
    "        self._cols_to_drop = []\n",
    "        for i in range(ncols - 1):\n",
    "            col_i = cols[i]\n",
    "            if col_i in self._cols_to_drop:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = cols[j]\n",
    "                if col_j in self._cols_to_drop:\n",
    "                    continue\n",
    "                \n",
    "                if (df_train[col_i].astype(\"object\") == df_train[col_j].astype(\"object\")).mean() > self._threshold:\n",
    "                    self._cols_to_drop.append(col_j)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        print(\"Number of same columns droped:\", len(self._cols_to_drop))\n",
    "        return df.drop(self._cols_to_drop, axis=\"columns\")\n",
    "\n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df\n",
    "        \n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        cols_to_drop = [col for col in df_cat.columns if col not in self._cat_cols_ohe]\n",
    "        df_cat = df_cat.drop(cols_to_drop, axis=\"columns\")\n",
    "        \n",
    "        # change to float32\n",
    "        for col in df_cat.columns:\n",
    "            df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        df = pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "\n",
    "class Standardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._mean = {col: df_train[col].mean() for col in num_cols}\n",
    "        self._std = {col: df_train[col].std() for col in num_cols}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        for col in self._mean:\n",
    "            if self._std[col] > 0:\n",
    "                df[col] = (df[col] - self._mean[col]) / self._std[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "            else:\n",
    "                print(\"WARNING: \" + col + \" has zero std.\")\n",
    "                df[col] = df[col] - self._mean[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "                \n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumColsQCuter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, labels=None, exclude_cols=None, \n",
    "                 nunique_min=1000, keep_old=True, \n",
    "                 output_type=np.int32):\n",
    "        \n",
    "        if labels is None:\n",
    "            self._labels = list(range(1, 11))\n",
    "        else:\n",
    "            self._labels = list(labels)\n",
    "        self._nbins = len(self._labels)\n",
    "        assert self._isunique(self._labels), \"labels must be unique\"\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "        \n",
    "        self._nunique_min = nunique_min\n",
    "        \n",
    "        self._suffix = \"_%dQCUT\" % len(self._labels)\n",
    "        \n",
    "        self._keep_old = keep_old\n",
    "        self._output_type = output_type\n",
    "    \n",
    "    def _isunique(self, x):\n",
    "        return len(np.unique(x)) == len(x)\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        \n",
    "        quantiles = np.linspace(0, 1, self._nbins + 1)\n",
    "        quantiles = quantiles[1: -1]\n",
    "        \n",
    "        sel_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        sel_cols = [col for col in sel_cols if df_train[col].nunique() >= self._nunique_min]\n",
    "        \n",
    "        self._bins = {}\n",
    "        for col in sel_cols:\n",
    "            #if df_train[col].isnull().any():\n",
    "            #    raise ValueError(col + \" has null values\")\n",
    "            \n",
    "            bins = df_train[col].quantile(quantiles).values\n",
    "            bins = np.array([-np.inf] + list(bins) + [np.inf])\n",
    "            \n",
    "            if self._isunique(bins):\n",
    "                self._bins[col] = bins\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        for col, bins in self._bins.items():\n",
    "            new_col_name = col + self._suffix\n",
    "            df[new_col_name] = pd.cut(df[col], bins, labels=self._labels)\n",
    "            df[new_col_name] = df[new_col_name].astype(self._output_type)\n",
    "            \n",
    "            new_cols.append(new_col_name)\n",
    "            \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n",
    "\n",
    "\n",
    "class NumColsBin(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, nbins=10, include_cols=None, exclude_cols=None, \n",
    "                 nunique_min=1000, keep_old=True, \n",
    "                 output_type=np.object):\n",
    "         \n",
    "        self._nbins = nbins\n",
    "        self._include_cols = include_cols\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "        \n",
    "        self._nunique_min = nunique_min\n",
    "        \n",
    "        self._suffix = \"_BIN\"\n",
    "        self._labels = [\"b%d\" %i for i in range(self._nbins)]\n",
    "        \n",
    "        self._keep_old = keep_old\n",
    "        self._output_type = output_type\n",
    "    \n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        if self._include_cols is None:\n",
    "            sel_cols = num_cols\n",
    "        else:\n",
    "            sel_cols = self._include_cols\n",
    "        \n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        sel_cols = [col for col in sel_cols if df_train[col].nunique() >= self._nunique_min]\n",
    "        \n",
    "        self._bins = {}\n",
    "        for col in sel_cols:\n",
    "            lower, upper = df_train[col].quantile([0.02, 0.98]).values\n",
    "            bins = np.linspace(lower, upper, self._nbins - 1)\n",
    "            bins = np.array([-np.inf] + list(bins) + [np.inf])\n",
    "            self._bins[col] = bins\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        for col, bins in self._bins.items():\n",
    "            new_col_name = col + self._suffix\n",
    "            df[new_col_name] = pd.cut(df[col], bins, labels=self._labels)\n",
    "            df[new_col_name] = df[new_col_name].astype(self._output_type)\n",
    "            \n",
    "            new_cols.append(new_col_name)\n",
    "            \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n",
    "\n",
    "\n",
    "\n",
    "class CatValueCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keep_old=True, exclude_cols=None, suffix=\"_VALCOUNT\"):\n",
    "        self._keep_old = keep_old\n",
    "        self._suffix = suffix\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        sel_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        \n",
    "        self._val_counts = {}\n",
    "        for col in sel_cols:\n",
    "            self._val_counts[col] = df_train[col].value_counts(normalize=True)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        for col, val_count in self._val_counts.items():\n",
    "            new_col = col + self._suffix\n",
    "            df[new_col] = df[col].map(val_count).astype(np.float32)\n",
    "            df[new_col] = df[new_col].fillna(0)\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n",
    "\n",
    "\n",
    "class NumValueCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nunique_min=1000, bins=100, keep_old=True, exclude_cols=None, suffix=\"_VALCOUNT\"):\n",
    "        self._nunique_min = nunique_min\n",
    "        self._bins = bins\n",
    "        self._keep_old = keep_old\n",
    "        self._suffix = suffix\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "    \n",
    "    def _cal_bin_edges(self, ser):\n",
    "        val_min = ser.min()\n",
    "        val_max = ser.max()\n",
    "        \n",
    "        edges = np.linspace(val_min, val_max, self._bins + 1)\n",
    "        edges[0] = -np.inf\n",
    "        edges[-1] = np.inf\n",
    "        return edges\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        sel_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        sel_cols = [col for col in sel_cols if df_train[col].nunique() >= self._nunique_min]\n",
    "        \n",
    "        self._val_counts = {}\n",
    "        self._bin_edges = {}\n",
    "        \n",
    "        for col in sel_cols:\n",
    "            edges = self._cal_bin_edges(df_train[col])\n",
    "            self._bin_edges[col] = edges\n",
    "            \n",
    "            discrete_ser = pd.cut(df_train[col], edges, labels=list(range(self._bins)))\n",
    "            self._val_counts[col] = discrete_ser.value_counts(normalize=True)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        \n",
    "        for col, edges in self._bin_edges.items():\n",
    "            discrete_ser = pd.cut(df[col], edges, labels=list(range(self._bins)))\n",
    "            \n",
    "            val_count = self._val_counts[col]\n",
    "            new_col = col + self._suffix\n",
    "            df[new_col] = discrete_ser.map(val_count).astype(np.float32)\n",
    "            df[new_col] = df[new_col].fillna(0)\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMeanFromNumCols(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nunique_min=1000, bins=10, keep_old=True,\n",
    "                 on=\"SK_ID_CURR\", target_col=\"TARGET\",\n",
    "                 include_cols=None, exclude_cols=None, suffix=\"_TARGETMEANNUM\"):\n",
    "        self._nunique_min = nunique_min\n",
    "        self._bins = bins\n",
    "        self._keep_old = keep_old\n",
    "        self._suffix = suffix\n",
    "        \n",
    "        self._on = on\n",
    "        self._target_col = target_col\n",
    "        \n",
    "        self._include_cols = include_cols\n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "    \n",
    "    def _cal_bin_edges(self, ser):\n",
    "        quantiles = np.linspace(0, 1, self._bins + 1)\n",
    "        quantiles = quantiles[1: -1]\n",
    "        \n",
    "        edges = ser.quantile(quantiles).values\n",
    "        edges = np.array([-np.inf] + list(edges) + [np.inf])\n",
    "        return edges\n",
    "    \n",
    "    def _isunique(self, x):\n",
    "        return len(np.unique(x)) == len(x)\n",
    "    \n",
    "    def fit(self, df_X_train, df_y_train):\n",
    "        assert isinstance(df_y_train, pd.DataFrame)\n",
    "        \n",
    "        self._y_mean = df_y_train[self._target_col].mean()\n",
    "        \n",
    "        if self._include_cols is None:\n",
    "            sel_cols = df_X_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        else:\n",
    "            sel_cols = self._include_cols\n",
    "            \n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        sel_cols = [col for col in sel_cols if df_X_train[col].nunique() >= self._nunique_min]\n",
    "        \n",
    "        self._target_means = {}\n",
    "        self._bin_edges = {}\n",
    "        \n",
    "        for col in sel_cols:\n",
    "            edges = self._cal_bin_edges(df_X_train[col])\n",
    "            \n",
    "            if self._isunique(edges):\n",
    "                self._bin_edges[col] = edges\n",
    "                \n",
    "                tmp_df = df_X_train[[self._on, col]].set_index(self._on)\n",
    "                \n",
    "                bin_label = pd.cut(tmp_df[col], edges, labels=list(range(self._bins)))\n",
    "                \n",
    "                bin_label = pd.DataFrame(bin_label).reset_index()\n",
    "                tmp_df = bin_label.merge(df_y_train, how=\"left\", on=self._on)\n",
    "                \n",
    "                #tmp_df = pd.DataFrame({\"bin_label\": bin_label, \"target\": y_train})\n",
    "                \n",
    "                self._target_means[col] = tmp_df.groupby([col])[self._target_col].mean()\n",
    "                self._target_means[col] = self._target_means[col].fillna(self._y_mean)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        \n",
    "        for col, edges in self._bin_edges.items():\n",
    "            bin_label = pd.cut(df[col], edges, labels=list(range(self._bins)))\n",
    "            \n",
    "            target_mean = self._target_means[col]\n",
    "            new_col = col + self._suffix\n",
    "            df[new_col] = bin_label.map(target_mean).astype(np.float32)\n",
    "            df[new_col] = df[new_col].fillna(self._y_mean)\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n",
    "\n",
    "\n",
    "class TargetMeanFromCatCols(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keep_old=True, \n",
    "                 include_cols=None, exclude_cols=None,\n",
    "                 on=\"SK_ID_CURR\", target_col=\"TARGET\",\n",
    "                 suffix=\"_TARGETMEANCAT\"):\n",
    "        self._keep_old = keep_old\n",
    "        self._suffix = suffix\n",
    "        \n",
    "        self._on = on\n",
    "        self._target_col = target_col\n",
    "        \n",
    "        self._include_cols = include_cols\n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "    \n",
    "    def fit(self, df_X_train, df_y_train):\n",
    "        assert isinstance(df_y_train, pd.DataFrame)\n",
    "        \n",
    "        self._y_mean = df_y_train[self._target_col].mean()\n",
    "        \n",
    "        if self._include_cols is None:\n",
    "            sel_cols = df_X_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        else:\n",
    "            sel_cols = self._include_cols\n",
    "            \n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        \n",
    "        self._target_means = {}\n",
    "        for col in sel_cols:\n",
    "            tmp_df = df_X_train[[self._on, col]].merge(df_y_train, how=\"left\", on=self._on)\n",
    "            target_mean = tmp_df.groupby([col])[self._target_col].mean()\n",
    "            target_mean = target_mean.fillna(self._y_mean)\n",
    "            self._target_means[col] = target_mean\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        \n",
    "        for col, target_mean in self._target_means.items():\n",
    "            \n",
    "            new_col = col + self._suffix\n",
    "            df[new_col] = df[col].map(target_mean).astype(np.float32)\n",
    "            df[new_col] = df[new_col].fillna(self._y_mean)\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n",
    "\n",
    "\n",
    "class WeightOfEvidenceNum(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nunique_min=1000, bins=10, keep_old=True, exclude_cols=None, suffix=\"_WOENUM\"):\n",
    "        self._nunique_min = nunique_min\n",
    "        self._bins = bins\n",
    "        self._keep_old = keep_old\n",
    "        self._suffix = suffix\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "    \n",
    "    def _cal_bin_edges(self, ser):\n",
    "        quantiles = np.linspace(0, 1, self._bins + 1)\n",
    "        quantiles = quantiles[1: -1]\n",
    "        \n",
    "        edges = ser.quantile(quantiles).values\n",
    "        edges = np.array([-np.inf] + list(edges) + [np.inf])\n",
    "        return edges\n",
    "    \n",
    "    def _isunique(self, x):\n",
    "        return len(np.unique(x)) == len(x)\n",
    "    \n",
    "    def fit(self, df_train, y_train):\n",
    "        assert isinstance(y_train, pd.Series)\n",
    "        assert df_train.shape[0] == y_train.shape[0]\n",
    "        assert (df_train.index == y_train.index).all()\n",
    "        \n",
    "        sel_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        sel_cols = [col for col in sel_cols if df_train[col].nunique() >= self._nunique_min]\n",
    "        \n",
    "        self._woes = {}\n",
    "        self._bin_edges = {}\n",
    "        \n",
    "        for col in sel_cols:\n",
    "            edges = self._cal_bin_edges(df_train[col])\n",
    "            \n",
    "            if self._isunique(edges):\n",
    "                self._bin_edges[col] = edges\n",
    "                \n",
    "                bin_label = pd.cut(df_train[col], edges, labels=list(range(self._bins)))\n",
    "                tmp_df = pd.DataFrame({\"bin_label\": bin_label, \"target\": y_train})\n",
    "                \n",
    "                bad_dist = tmp_df.groupby([\"bin_label\"])[\"target\"].mean()\n",
    "                good_dist = 1. - bad_dist\n",
    "                \n",
    "                woe = np.log(good_dist / bad_dist) * 100\n",
    "                woe = woe.fillna(0.)\n",
    "                woe_min = woe.replace(-np.inf, np.nan).min() / 100.\n",
    "                woe_max = woe.replace(np.inf, np.nan).max() * 100.\n",
    "                \n",
    "                woe = woe.replace(-np.inf, woe_min)\n",
    "                woe = woe.replace(np.inf, woe_max)\n",
    "                self._woes[col] = woe\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        \n",
    "        for col, edges in self._bin_edges.items():\n",
    "            bin_label = pd.cut(df[col], edges, labels=list(range(self._bins)))\n",
    "            \n",
    "            woe = self._woes[col]\n",
    "            new_col = col + self._suffix\n",
    "            df[new_col] = bin_label.map(woe).astype(np.float32)\n",
    "            df[new_col] = df[new_col].fillna(0.)\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]\n",
    "\n",
    "\n",
    "class WeightOfEvidenceCat(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keep_old=True, exclude_cols=None, suffix=\"_WOECAT\"):\n",
    "        self._keep_old = keep_old\n",
    "        self._suffix = suffix\n",
    "        \n",
    "        self._exclude_cols = [\"SK_ID_CURR\", \"TARGET\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"]\n",
    "        if exclude_cols is not None:\n",
    "            self._exclude_cols = self._exclude_cols + list(exclude_cols)\n",
    "        \n",
    "    \n",
    "    def fit(self, df_train, y_train):\n",
    "        assert isinstance(y_train, pd.Series)\n",
    "        assert df_train.shape[0] == y_train.shape[0]\n",
    "        assert (df_train.index == y_train.index).all()\n",
    "        \n",
    "        sel_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        sel_cols = [col for col in sel_cols if col not in self._exclude_cols]\n",
    "        \n",
    "        self._woes = {}\n",
    "        for col in sel_cols:\n",
    "            tmp_df = pd.DataFrame({\"col\": df_train[col], \"target\": y_train})\n",
    "            \n",
    "            bad_dist = tmp_df.groupby([\"col\"])[\"target\"].mean()\n",
    "            good_dist = 1. - bad_dist\n",
    "            \n",
    "            woe = np.log(good_dist / bad_dist) * 100\n",
    "            woe = woe.fillna(0.)\n",
    "            woe_min = woe.replace(-np.inf, np.nan).min() / 100.\n",
    "            woe_max = woe.replace(np.inf, np.nan).max() * 100.\n",
    "            \n",
    "            woe = woe.replace(-np.inf, woe_min)\n",
    "            woe = woe.replace(np.inf, woe_max)\n",
    "            self._woes[col] = woe\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        new_cols = []\n",
    "        \n",
    "        for col, woe in self._woes.items():\n",
    "            new_col = col + self._suffix\n",
    "            df[new_col] = df[col].map(woe).astype(np.float32)\n",
    "            df[new_col] = df[new_col].fillna(0.)\n",
    "            \n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        if self._keep_old:\n",
    "            return df\n",
    "        else:\n",
    "            return df[new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex_cols(columns):\n",
    "    fat_cols = [\"_\".join([str(c) for c in flat_col]) for flat_col in columns.to_flat_index()]\n",
    "    return fat_cols\n",
    "\n",
    "\n",
    "def agg_num_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    cat_cols = df.select_dtypes([\"object\", \"category\"]).columns.to_list()\n",
    "    if len(cat_cols) > 0:\n",
    "        raise ValueError(\"There are non-number cols: \" + \", \".join(cat_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def agg_cat_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "    if len(num_cols) > 0:\n",
    "        raise ValueError(\"There are number cols: \" + \", \".join(num_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "class Aggregator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, by_list_cols, \n",
    "                 num_stats, bool_stats, cat_stats, \n",
    "                 ohe_cat_stats=None,\n",
    "                 ohe_cat_max_class=None,\n",
    "                 iqr=False, minmax_range=False, mean_median_diff=False):\n",
    "        \"\"\"\n",
    "        :param by_list_cols: list of str, cols by which the dataframe is grouped\n",
    "        :param num_stats: list, aggregating functions for numerical columns\n",
    "        :param bool_stats: list, aggregating functions for bool columns\n",
    "        :param cat_stats: list, aggregating functions for category columns\n",
    "        :param ohe_cat_stats: list, aggregating functions for category columns after one-hot encoded\n",
    "        :param ohe_cat_max_class: int, category columns with at most ohe_cat_max_class classes \n",
    "                                 will be one-hot encoded before aggregating, \n",
    "                                 If None, no one-hot encoding will be done.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._by_list_cols = by_list_cols\n",
    "        \n",
    "        self._num_stats = num_stats\n",
    "        self._bool_stats = bool_stats\n",
    "        self._cat_stats = cat_stats\n",
    "        \n",
    "        if ohe_cat_stats is not None:\n",
    "            self._ohe_cat_stats = ohe_cat_stats\n",
    "        else:\n",
    "            self._ohe_cat_stats = [\"mean\", \"sum\"]\n",
    "        \n",
    "        self._ohe_cat_max_class = ohe_cat_max_class\n",
    "        \n",
    "        self._iqr = iqr\n",
    "        self._minmax_range = minmax_range\n",
    "        self._mean_median_diff = mean_median_diff\n",
    "    \n",
    "    def _num_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._num_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _bool_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._bool_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _cat_agg(self, df, by_sers):\n",
    "        agg_df =  agg_cat_cols(df, by_sers, stats=self._cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _ohe_cat_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._ohe_cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    \n",
    "    def _iqr_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        iqr_df = grouped.quantile(0.75) - grouped.quantile(0.25)\n",
    "        iqr_df.columns = [col + \"_iqr\" for col in iqr_df.columns]\n",
    "        return iqr_df\n",
    "    \n",
    "    def _range_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        range_df = grouped.max() - grouped.min()\n",
    "        range_df.columns = [col + \"_range\" for col in range_df.columns]\n",
    "        return range_df\n",
    "    \n",
    "    def _mm_diff_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        diff_df = grouped.mean() - grouped.median()\n",
    "        diff_df.columns = [col + \"_mm_diff\" for col in diff_df.columns]\n",
    "        return diff_df\n",
    "    \n",
    "    def _cat_cols_to_ohe(self, df_train):\n",
    "        cat_cols = []\n",
    "        if (self._ohe_cat_max_class is None) or (self._ohe_cat_max_class == 0):\n",
    "            return cat_cols\n",
    "        \n",
    "        for col in self._cat_cols:\n",
    "            if df_train[col].nunique() <=  self._ohe_cat_max_class:\n",
    "                for cl in df_train[col].unique():\n",
    "                    cat_cols.append(col + \"_\" + str(cl))\n",
    "        return cat_cols\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        df_train = df_train.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        self._ohe = OneHotEncoder()\n",
    "        self._ohe.fit(df_train)\n",
    "        \n",
    "        self._bool_cols = df_train.select_dtypes([\"bool\"]).columns.to_list()\n",
    "        self._cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        self._num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        \n",
    "        # cat column names after being one-hot encoded\n",
    "        all_cat_ohe_cols = self._ohe.transform(df_train).columns.to_list()\n",
    "        self._cat_cols_ohe = []\n",
    "        if self._ohe_cat_max_class is not None:\n",
    "            for col in self._cat_cols:\n",
    "                if df_train[col].nunique() <=  self._ohe_cat_max_class:\n",
    "                    for cl in df_train[col].unique():\n",
    "                        ohe_col = col + \"_\" + str(cl)\n",
    "                        \n",
    "                        if ohe_col in all_cat_ohe_cols:\n",
    "                            self._cat_cols_ohe.append(ohe_col)\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        by_sers = [df[col] for col in self._by_list_cols]\n",
    "        df = df.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        for col in self._bool_cols + self._cat_cols + self._num_cols:\n",
    "            if col not in all_cols:\n",
    "                raise ValueError(col + \" exists in train but not in test\")\n",
    "        \n",
    "        dfs = []\n",
    "        \n",
    "        # bool cols\n",
    "        if len(self._bool_cols) > 0:\n",
    "            df_bool = df[self._bool_cols]\n",
    "            print(\"Aggregating bool df with shape:\", df_bool.shape)\n",
    "            df_bool = self._bool_agg(df_bool, by_sers)\n",
    "            \n",
    "            for col in df_bool.columns:\n",
    "                if col.endswith(\"_mean\") or col.endswith(\"_sum\") or col.endswith(\"_entropy\"):\n",
    "                    df_bool[col] = df_bool[col].astype(\"float32\")\n",
    "                \n",
    "                if col.endswith(\"_mode\"):\n",
    "                    df_bool[col] = df_bool[col].astype(\"category\")\n",
    "            dfs.append(df_bool)\n",
    "        \n",
    "        # categorical cols\n",
    "        if len(self._cat_cols) > 0:\n",
    "            df_cat = df[self._cat_cols]\n",
    "            print(\"Aggregating cat df with shape:\", df_cat.shape)\n",
    "            df_cat = self._cat_agg(df_cat, by_sers)\n",
    "            \n",
    "            for col in df_cat.columns:\n",
    "                if col.endswith(\"_mean\") or col.endswith(\"_entropy\"):\n",
    "                    df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "                \n",
    "                if col.endswith(\"_mode\"):\n",
    "                    df_cat[col] = df_cat[col].astype(\"category\")\n",
    "            \n",
    "            dfs.append(df_cat)\n",
    "        \n",
    "        # number cols\n",
    "        df_num = df[self._num_cols]\n",
    "        if df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with shape:\", df_num.shape)\n",
    "            df_num = self._num_agg(df_num, by_sers)\n",
    "            dfs.append(df_num)\n",
    "        \n",
    "        # ohe cat cols\n",
    "        if len(self._cat_cols_ohe) > 0:\n",
    "            df_ohe = self._ohe.transform(df)\n",
    "            for col in self._cat_cols_ohe:\n",
    "                if col not in df_ohe.columns:\n",
    "                    raise ValueError(col + \" is not in cols of df_ohe\")\n",
    "                    \n",
    "            df_ohe = df_ohe[self._cat_cols_ohe]\n",
    "            print(\"Aggregating ohe cat df with shape:\", df_ohe.shape)\n",
    "            \n",
    "            df_ohe = self._ohe_cat_agg(df_ohe, by_sers)\n",
    "            dfs.append(df_ohe)\n",
    "        \n",
    "        # aggregate num cols with iqr, range and mean-median difference\n",
    "        df_num = df.select_dtypes([\"number\"])\n",
    "        print(\"df_num.shape for iqr, mimmax_range, mean_median_diff\", df_num.shape)\n",
    "            \n",
    "        if self._iqr and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with iqr\")\n",
    "            df_iqr = self._iqr_agg(df_num, by_sers)\n",
    "            dfs.append(df_iqr)\n",
    "        \n",
    "        if self._minmax_range and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with range\")\n",
    "            df_range = self._range_agg(df_num, by_sers)\n",
    "            print(\"df_range.shape\", df_range.shape)\n",
    "            dfs.append(df_range)\n",
    "        \n",
    "        if self._mean_median_diff and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with mean-median difference\")\n",
    "            df_diff = self._mm_diff_agg(df_num, by_sers)\n",
    "            dfs.append(df_diff)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_mean_agg(df, by_cols, weight_col, weight_decay=1., suffix=\"_expmean\"):\n",
    "    \"\"\"multiply by exp(df[weight_col] * weight_decay)\"\"\"\n",
    "    assert isinstance(by_cols, list), \"by_cols must be a list\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"weights\"] = np.exp(df[weight_col] * weight_decay)\n",
    "    \n",
    "    by_sers = [df[col] for col in by_cols]\n",
    "    df = df.drop(by_cols + [weight_col], axis=\"columns\")\n",
    "    \n",
    "    cols = [col for col in df.columns if col != \"weights\"]\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(np.float32) * df[\"weights\"]\n",
    "    \n",
    "    df = df.groupby(by_sers).sum()\n",
    "    for col in cols:\n",
    "        df[col] = df[col] / df[\"weights\"]\n",
    "    \n",
    "    df = df.drop([\"weights\"], axis=\"columns\")\n",
    "    cols = [col + suffix for col in df.columns]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "\n",
    "class CatColsRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"remove cat columns having too many classes\"\"\"\n",
    "    def __init__(self, max_class=20):\n",
    "        self._max_class = max_class\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        self._cols_to_drop = [col for col in cat_cols if df_train[col].nunique() > self._max_class]\n",
    "        print(\"Number of cols droped %d\" % len(self._cols_to_drop))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.drop(self._cols_to_drop, axis=\"columns\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(df, matching_key, train_id_ser):\n",
    "    is_train = df[matching_key].isin(train_id_ser.values)\n",
    "    \n",
    "    train = df.loc[is_train, :]\n",
    "    test = df.loc[~is_train, :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(ser):\n",
    "    return ser.mode().values[0]\n",
    "\n",
    "\n",
    "def entropy(ser):\n",
    "    pk = ser.value_counts(normalize=True)\n",
    "    return stats.entropy(pk)\n",
    "\n",
    "\n",
    "def gini_index(ser):\n",
    "    pk = ser.value_counts(normalize=True)\n",
    "    return 1. - (pk * pk).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG_STATS_1 = {\"num_stats\":     [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"],\n",
    "               \"bool_stats\":    [\"sum\", \"mean\", \"var\", gini_index],\n",
    "               \"cat_stats\":     [\"count\", \"nunique\", mode, gini_index],\n",
    "               \"ohe_cat_stats\": [\"sum\", \"mean\", \"var\"]}\n",
    "\n",
    "AGG_STATS_2 = {\"num_stats\":     [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"],\n",
    "               \"bool_stats\":    [\"sum\", \"mean\", \"var\", gini_index],\n",
    "               \"cat_stats\":     [\"count\", \"nunique\", mode, gini_index],\n",
    "               \"ohe_cat_stats\": [\"sum\", \"mean\", \"var\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `application_[train|test]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = [\"^APARTMENTS_\", \"^BASEMENTAREA_\", \"^YEARS_B\", \"^COMMONAREA_\", \n",
    "                               \"^ELEVATORS_\", \"^ENTRANCES_\", \"^FLOORS\", \"^LANDAREA_\", \"^LIVING\", \n",
    "                               \"^NONLIVING\", \"AMT_REQ_CREDIT_BUREAU_\"]\n",
    "        self._spec_impt_regex_val_num = -1.\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"OWN_CAR_AGE\": -1.,\n",
    "                                    \"EXT_SOURCE_1\": 0.,\n",
    "                                    \"EXT_SOURCE_3\": 0.,\n",
    "                                    \"TOTALAREA_MODE\": -1.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "        \n",
    "def hour_period_bin(hours):\n",
    "    hours = hours.values\n",
    "    hour_bin = np.array([\"evening\"] * len(hours), dtype=\"object\")\n",
    "    morning_mask = (hours > 5) & (hours < 12)\n",
    "    afternoon_mask = (hours >= 12) & (hours < 18)\n",
    "    \n",
    "    hour_bin[morning_mask] = \"morning\"\n",
    "    hour_bin[afternoon_mask] = \"afternoon\"\n",
    "    return hour_bin\n",
    "\n",
    "\n",
    "def hour_cat(hours):\n",
    "    h_map = {h: \"H%d\"%h for h in hours.values}\n",
    "    return hours.map(h_map).astype(\"category\")\n",
    "\n",
    "\n",
    "class ApplNewColsAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df_train):\n",
    "        credit_to_income = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_INCOME_TOTAL\"]\n",
    "        self._cti_min = credit_to_income.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._cti_max = credit_to_income.replace(np.inf, np.nan).max() * 10.\n",
    "        \n",
    "        credit_to_goods = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_GOODS_PRICE\"]\n",
    "        self._ctg_min = credit_to_goods.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._ctg_max = credit_to_goods.replace(np.inf, np.nan).max() * 10.\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        df_new[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df_new[\"AMT_INCOME_TOTAL\"])\n",
    "        df_new[\"DAYS_EMPLOYED_POSITIVE\"] = df_new[\"DAYS_EMPLOYED\"] > 0\n",
    "        days_emp_max = df_new[\"DAYS_EMPLOYED\"].max()\n",
    "        if days_emp_max > 0:\n",
    "            df_new[\"DAYS_EMPLOYED\"] = df_new[\"DAYS_EMPLOYED\"].replace({days_emp_max: 100.})\n",
    "        \n",
    "        df_new[\"DAYS_EMPLOYED_TO_DAYS_BIRTH\"] = df_new[\"DAYS_EMPLOYED\"] / df_new[\"DAYS_BIRTH\"]\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_INCOME_TOTAL\"]\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(-np.inf, self._cti_min)\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(np.inf, self._cti_max)\n",
    "        \n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_GOODS_PRICE\"]\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(-np.inf, self._ctg_min)\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(np.inf, self._ctg_max)\n",
    "        \n",
    "        df_new[\"HOUR_APPR_PROCESS_START_PERIOD\"] = hour_period_bin(df_new[\"HOUR_APPR_PROCESS_START\"])\n",
    "        df_new[\"HOUR_APPR_PROCESS_START_CAT\"] = hour_cat(df_new[\"HOUR_APPR_PROCESS_START\"])\n",
    "        \n",
    "        interact_cols = [\"CNT_CHILDREN\", \"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \n",
    "                         \"AMT_GOODS_PRICE\",\"DAYS_EMPLOYED_TO_DAYS_BIRTH\", \"OWN_CAR_AGE\", \n",
    "                         \"REGION_RATING_CLIENT\", \"REGION_RATING_CLIENT_W_CITY\",\n",
    "                         \"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n",
    "        for i in range(len(interact_cols) - 1):\n",
    "            col_i = interact_cols[i]\n",
    "            \n",
    "            for j in range(i+1, len(interact_cols)):\n",
    "                col_j = interact_cols[j]\n",
    "                \n",
    "                new_col = col_i + \"_\" + col_j  + \"_INTERACT\"\n",
    "                df_new[new_col] = df_new[col_i] * df_new[col_j]\n",
    "        \n",
    "        \n",
    "        df_new[\"EXT_SOURCE_INTERACT_ALL\"] = df_new[\"EXT_SOURCE_1\"]\n",
    "        interact_cols = [\"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]\n",
    "        for col in interact_cols:\n",
    "            df_new[\"EXT_SOURCE_INTERACT_ALL\"] = df_new[\"EXT_SOURCE_INTERACT_ALL\"] * df_new[col]\n",
    "            \n",
    "        \n",
    "        interact_cols = [\"REG_REGION_NOT_LIVE_REGION\", \"REG_REGION_NOT_WORK_REGION\", \n",
    "                         \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\", \n",
    "                         \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\"]\n",
    "        for i in range(len(interact_cols) - 1):\n",
    "            col_i = interact_cols[i]\n",
    "            \n",
    "            for j in range(i+1, len(interact_cols)):\n",
    "                col_j = interact_cols[j]\n",
    "                \n",
    "                new_col = col_i + \"_\" + col_j  + \"_INTERACT\"\n",
    "                df_new[new_col] = df_new[col_i] * df_new[col_j]\n",
    "        \n",
    "        df_new[\"REG_REGION_INTERACT_ALL\"] = df_new[interact_cols[0]]\n",
    "        for col in interact_cols[1:]:\n",
    "            df_new[\"REG_REGION_INTERACT_ALL\"] = df_new[\"REG_REGION_INTERACT_ALL\"] * df_new[col]\n",
    "        \n",
    "        \n",
    "        interact_cols = [\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\", \"FLAG_WORK_PHONE\", \n",
    "                         \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\n",
    "        for i in range(len(interact_cols) - 1):\n",
    "            col_i = interact_cols[i]\n",
    "            \n",
    "            for j in range(i+1, len(interact_cols)):\n",
    "                col_j = interact_cols[j]\n",
    "                \n",
    "                new_col = col_i + \"_\" + col_j  + \"_INTERACT\"\n",
    "                df_new[new_col] = df_new[col_i] * df_new[col_j]\n",
    "        \n",
    "        df_new[\"FLAG_PHONE_INTERACT_ALL\"] = df_new[interact_cols[0]]\n",
    "        for col in interact_cols[1:]:\n",
    "            df_new[\"FLAG_PHONE_INTERACT_ALL\"] = df_new[\"FLAG_PHONE_INTERACT_ALL\"] * df_new[col]\n",
    "                \n",
    "        \n",
    "        interact_cols = [\"FLAG_DOCUMENT_2\",\"FLAG_DOCUMENT_3\", \"FLAG_DOCUMENT_4\", \"FLAG_DOCUMENT_5\",\n",
    "                         \"FLAG_DOCUMENT_6\", \"FLAG_DOCUMENT_7\", \"FLAG_DOCUMENT_8\", \"FLAG_DOCUMENT_9\",\n",
    "                         \"FLAG_DOCUMENT_10\", \"FLAG_DOCUMENT_11\", \"FLAG_DOCUMENT_12\", \"FLAG_DOCUMENT_13\",\n",
    "                         \"FLAG_DOCUMENT_14\", \"FLAG_DOCUMENT_15\", \"FLAG_DOCUMENT_16\", \"FLAG_DOCUMENT_17\",\n",
    "                         \"FLAG_DOCUMENT_18\", \"FLAG_DOCUMENT_19\", \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\"]\n",
    "        \n",
    "        \n",
    "        df_new[\"FLAG_DOCUMENT_INTERACT_1_5\"] = df_new[interact_cols[0]]\n",
    "        for col in interact_cols[1:5]:\n",
    "            df_new[\"FLAG_DOCUMENT_INTERACT_1_5\"] = df_new[\"FLAG_DOCUMENT_INTERACT_1_5\"] * df_new[col]\n",
    "        \n",
    "        \n",
    "        df_new[\"FLAG_DOCUMENT_INTERACT_6_10\"] = df_new[interact_cols[5]]\n",
    "        for col in interact_cols[6:10]:\n",
    "            df_new[\"FLAG_DOCUMENT_INTERACT_6_10\"] = df_new[\"FLAG_DOCUMENT_INTERACT_6_10\"] * df_new[col]\n",
    "            \n",
    "            \n",
    "        df_new[\"FLAG_DOCUMENT_INTERACT_11_15\"] = df_new[interact_cols[10]]\n",
    "        for col in interact_cols[11:15]:\n",
    "            df_new[\"FLAG_DOCUMENT_INTERACT_11_15\"] = df_new[\"FLAG_DOCUMENT_INTERACT_11_15\"] * df_new[col]\n",
    "        \n",
    "        \n",
    "        df_new[\"FLAG_DOCUMENT_INTERACT_ALL\"] = df_new[interact_cols[0]]\n",
    "        for col in interact_cols[1:]:\n",
    "            df_new[\"FLAG_DOCUMENT_INTERACT_ALL\"] = df_new[\"FLAG_DOCUMENT_INTERACT_ALL\"] * df_new[col]\n",
    "        \n",
    "        return df_new\n",
    "    \n",
    "\n",
    "cat_cols = [\"HOUR_APPR_PROCESS_START\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 300.13 MB\n",
      "Memory usage after changing types 104.87 MB\n",
      "Memory usage before changing types 47.18 MB\n",
      "Memory usage after changing types 18.19 MB\n",
      "(307511, 122) (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "application_train = load_csv(\"data/download/application_train.csv\")\n",
    "application_test = load_csv(\"data/download/application_test.csv\")\n",
    "\n",
    "appl_train_key = application_train[\"SK_ID_CURR\"]\n",
    "appl_test_key = application_test[\"SK_ID_CURR\"]\n",
    "print(application_train.shape, application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "appl_train = application_train.copy()\n",
    "appl_test = application_test.copy()\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "y_train = appl_train[\"TARGET\"]\n",
    "y_train_df = appl_train[[\"TARGET\", \"SK_ID_CURR\"]]\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = ApplImputer()\n",
    "imputer.fit(appl_train)\n",
    "appl_train = imputer.transform(appl_train)\n",
    "appl_test = imputer.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.99, col_regex=\"_ISNULL$\")\n",
    "remover.fit(appl_train)\n",
    "appl_train = remover.transform(appl_train)\n",
    "appl_test = remover.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add new columns\")\n",
    "adder = ApplNewColsAdder()\n",
    "adder.fit(appl_train)\n",
    "appl_train = adder.transform(appl_train)\n",
    "appl_test = adder.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Bin numerical columns\")\n",
    "include_cols = [\"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\",\n",
    "               \"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_2\"]\n",
    "\n",
    "numcolsbin = NumColsBin(nbins=10, include_cols=include_cols)\n",
    "numcolsbin.fit(appl_train)\n",
    "appl_train = numcolsbin.transform(appl_train)\n",
    "appl_test = numcolsbin.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"Count cat columns\")\n",
    "exclude_cols = [col for col in appl_train.columns if numcolsbin._suffix  in col]\n",
    "print(\"len(exclude_cols):\", len(exclude_cols))\n",
    "\n",
    "catvaluecounter = CatValueCounter(exclude_cols=exclude_cols)\n",
    "catvaluecounter.fit(appl_train)\n",
    "appl_train = catvaluecounter.transform(appl_train)\n",
    "appl_test = catvaluecounter.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Count num columns\")\n",
    "exclude_cols = exclude_cols + [col for col in appl_train.columns if \"_VALCOUNT\" in col]\n",
    "print(\"len(exclude_cols):\", len(exclude_cols))\n",
    "\n",
    "numvaluecounter = NumValueCounter(exclude_cols=exclude_cols)\n",
    "numvaluecounter.fit(appl_train)\n",
    "appl_train = numvaluecounter.transform(appl_train)\n",
    "appl_test = numvaluecounter.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Mean target for num cols\")\n",
    "exclude_cols = exclude_cols + [col for col in appl_train.columns if \"_VALCOUNT\" in col]\n",
    "print(\"len(exclude_cols):\", len(exclude_cols))\n",
    "\n",
    "target_mean_num = TargetMeanFromNumCols(exclude_cols=exclude_cols)\n",
    "target_mean_num.fit(appl_train, y_train_df)\n",
    "appl_train = target_mean_num.transform(appl_train)\n",
    "appl_test = target_mean_num.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Mean target for cat cols\")\n",
    "exclude_cols = exclude_cols + [col for col in appl_train.columns if \"_TARGETMEANNUM\" in col]\n",
    "print(\"len(exclude_cols):\", len(exclude_cols))\n",
    "\n",
    "target_mean_cat = TargetMeanFromCatCols(exclude_cols=exclude_cols)\n",
    "target_mean_cat.fit(appl_train, y_train_df)\n",
    "appl_train = target_mean_cat.transform(appl_train)\n",
    "appl_test = target_mean_cat.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"weight of evidence for num cols\")\n",
    "exclude_cols = exclude_cols + [col for col in appl_train.columns if \"_TARGETMEANCAT\" in col]\n",
    "print(\"len(exclude_cols):\", len(exclude_cols))\n",
    "\n",
    "woe_num = WeightOfEvidenceNum(exclude_cols=exclude_cols)\n",
    "woe_num.fit(appl_train, y_train)\n",
    "appl_train = woe_num.transform(appl_train)\n",
    "appl_test = woe_num.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"weight of evidence for cat cols\")\n",
    "exclude_cols = exclude_cols + [col for col in appl_train.columns if \"_WOENUM\" in col]\n",
    "print(\"len(exclude_cols):\", len(exclude_cols))\n",
    "\n",
    "woe_cat = WeightOfEvidenceCat(exclude_cols=exclude_cols)\n",
    "woe_cat.fit(appl_train, y_train)\n",
    "appl_train = woe_cat.transform(appl_train)\n",
    "appl_test = woe_cat.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(appl_train)\n",
    "appl_train = remover.transform(appl_train)\n",
    "appl_test = remover.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.90, col_regex=\"_BIN|_VALCOUNT|_TARGETMEANNUM|_TARGETMEANCAT|_WOENUM|_WOECAT\")\n",
    "remover.fit(appl_train)\n",
    "appl_train = remover.transform(appl_train)\n",
    "appl_test = remover.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    appl_train.to_csv(\"data/data_/application_train.csv\", index=False)\n",
    "    appl_test.to_csv(\"data/data_/application_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `bureau` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BureauImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"DAYS_ENDDATE_FACT\": 100.,\n",
    "                                    \"AMT_CREDIT_MAX_OVERDUE\": -1000.,\n",
    "                                    \"AMT_CREDIT_SUM_DEBT\": 0.,\n",
    "                                    \"AMT_CREDIT_SUM_LIMIT\": 0.,\n",
    "                                    \"AMT_ANNUITY\": -1000.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "\n",
    "def drop_cols_bureau(df):\n",
    "    # mostly zero\n",
    "    cols_to_drop = [\"CREDIT_DAY_OVERDUE\", \"CNT_CREDIT_PROLONG\", \"AMT_CREDIT_SUM_OVERDUE\"]\n",
    "    return df.drop(cols_to_drop, axis=\"columns\")\n",
    "\n",
    "\n",
    "def add_new_cols_bureau(df):\n",
    "    df[\"DAYS_CREDIT_ENDDATE_ISPOSITIVE\"] = df[\"DAYS_CREDIT_ENDDATE\"] > 0\n",
    "\n",
    "    df[\"AMT_CREDIT_MAX_OVERDUE_ISPOSITIVE\"] = df[\"AMT_CREDIT_MAX_OVERDUE\"] > 0\n",
    "    df[\"AMT_CREDIT_SUM_DEBT_ISPOSITIVE\"] = df[\"AMT_CREDIT_SUM_DEBT\"] > 0\n",
    "    df[\"AMT_CREDIT_SUM_LIMIT_ISPOSITIVE\"] = df[\"AMT_CREDIT_SUM_LIMIT\"] > 0\n",
    "    df[\"AMT_ANNUITY_ISPOSITIVE\"] =  df[\"AMT_ANNUITY\"] > 0\n",
    "    \n",
    "    amt_credt_sum = df[\"AMT_CREDIT_SUM\"] + 1\n",
    "    df[\"AMT_CREDIT_MAX_OVERDUE_TO_SUM\"] = df[\"AMT_CREDIT_MAX_OVERDUE\"] / amt_credt_sum\n",
    "    df[\"AMT_CREDIT_SUM_DEBT_TO_SUM\"] = df[\"AMT_CREDIT_SUM_DEBT\"] / amt_credt_sum\n",
    "    df[\"AMT_CREDIT_SUM_LIMIT_TO_SUM\"] = df[\"AMT_CREDIT_SUM_LIMIT\"] / amt_credt_sum\n",
    "    return df\n",
    "\n",
    "\n",
    "def bu_nearest_status(df):\n",
    "    return df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[0]\n",
    "\n",
    "def bu_mode_status_three_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[: 3]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def bu_mode_status_six_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[: 6]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "\n",
    "def add_new_agg_cols_bu(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_CREDIT_ACTIVE\"] = df.groupby(\"SK_ID_CURR\").apply(bu_nearest_status)\n",
    "    results[\"MODE_CREDIT_ACTIVE_THREE\"] = df.groupby(\"SK_ID_CURR\").apply(bu_mode_status_three_nearest)\n",
    "    results[\"MODE_CREDIT_ACTIVE_SIX\"] = df.groupby(\"SK_ID_CURR\").apply(bu_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 233.43 MB\n",
      "Memory usage after changing types 101.27 MB\n",
      "bureau.shape (1716428, 17)\n",
      "bureau_train.shape (1465325, 17)\n",
      "bureau_test.shape (251103, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CREDIT_ACTIVE</th>\n",
       "      <th>CREDIT_CURRENCY</th>\n",
       "      <th>DAYS_CREDIT</th>\n",
       "      <th>CREDIT_DAY_OVERDUE</th>\n",
       "      <th>DAYS_CREDIT_ENDDATE</th>\n",
       "      <th>DAYS_ENDDATE_FACT</th>\n",
       "      <th>AMT_CREDIT_MAX_OVERDUE</th>\n",
       "      <th>CNT_CREDIT_PROLONG</th>\n",
       "      <th>AMT_CREDIT_SUM</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT</th>\n",
       "      <th>AMT_CREDIT_SUM_LIMIT</th>\n",
       "      <th>AMT_CREDIT_SUM_OVERDUE</th>\n",
       "      <th>CREDIT_TYPE</th>\n",
       "      <th>DAYS_CREDIT_UPDATE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215354</td>\n",
       "      <td>Closed</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-497</td>\n",
       "      <td>0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>91323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215354</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-208</td>\n",
       "      <td>0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>171342.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215354</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-203</td>\n",
       "      <td>0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>464323.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215354</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-203</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215354</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-629</td>\n",
       "      <td>0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77674.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2700000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR CREDIT_ACTIVE CREDIT_CURRENCY  DAYS_CREDIT  CREDIT_DAY_OVERDUE  \\\n",
       "0      215354        Closed      currency 1         -497                   0   \n",
       "1      215354        Active      currency 1         -208                   0   \n",
       "2      215354        Active      currency 1         -203                   0   \n",
       "3      215354        Active      currency 1         -203                   0   \n",
       "4      215354        Active      currency 1         -629                   0   \n",
       "\n",
       "   DAYS_CREDIT_ENDDATE  DAYS_ENDDATE_FACT  AMT_CREDIT_MAX_OVERDUE  \\\n",
       "0               -153.0             -153.0                     NaN   \n",
       "1               1075.0                NaN                     NaN   \n",
       "2                528.0                NaN                     NaN   \n",
       "3                  NaN                NaN                     NaN   \n",
       "4               1197.0                NaN                 77674.5   \n",
       "\n",
       "   CNT_CREDIT_PROLONG  AMT_CREDIT_SUM  AMT_CREDIT_SUM_DEBT  \\\n",
       "0                   0         91323.0                  0.0   \n",
       "1                   0        225000.0             171342.0   \n",
       "2                   0        464323.5                  NaN   \n",
       "3                   0         90000.0                  NaN   \n",
       "4                   0       2700000.0                  NaN   \n",
       "\n",
       "   AMT_CREDIT_SUM_LIMIT  AMT_CREDIT_SUM_OVERDUE      CREDIT_TYPE  \\\n",
       "0                   NaN                     0.0  Consumer credit   \n",
       "1                   NaN                     0.0      Credit card   \n",
       "2                   NaN                     0.0  Consumer credit   \n",
       "3                   NaN                     0.0      Credit card   \n",
       "4                   NaN                     0.0  Consumer credit   \n",
       "\n",
       "   DAYS_CREDIT_UPDATE  AMT_ANNUITY  \n",
       "0                -131          NaN  \n",
       "1                 -20          NaN  \n",
       "2                 -16          NaN  \n",
       "3                 -16          NaN  \n",
       "4                 -21          NaN  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau = load_csv(\"data/download/bureau.csv\")\n",
    "print(\"bureau.shape\", bureau.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_partition(bureau, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"bureau_train.shape\", bureau_train.shape)\n",
    "print(\"bureau_test.shape\", bureau_test.shape)\n",
    "\n",
    "bureau_train_keys = bureau_train[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "bureau_test_keys = bureau_test[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "\n",
    "bureau_train = bureau_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_test = bureau_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "\n",
    "bureau_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggragate with many statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "print(\"Aggragate with many statistics\")\n",
    "\n",
    "bureau_agg_train = bureau_train.copy()\n",
    "bureau_agg_test = bureau_test.copy()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Drop some columns\")\n",
    "bureau_agg_train = drop_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = drop_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = BureauImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add some cols\")\n",
    "bureau_agg_train = add_new_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = add_new_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "new_agg_cols_train = add_new_agg_cols_bu(bureau_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bu(bureau_agg_test)\n",
    "\n",
    "\n",
    "print(\"aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "aggregator.fit(bureau_agg_train)\n",
    "bureau_agg_train = aggregator.transform(bureau_agg_train)\n",
    "bureau_agg_test = aggregator.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "bureau_agg_train = bureau_agg_train.reset_index()\n",
    "bureau_agg_test = bureau_agg_test.reset_index()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    bureau_agg_train.to_csv(\"data/data_/bureau_agg_train.csv\", index=False)\n",
    "    bureau_agg_test.to_csv(\"data/data_/bureau_agg_test.csv\", index=False)\n",
    "\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate with exponential mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate with exponential mean\n",
      "bureau_agg_train.shape (1465325, 16)\n",
      "bureau_agg_test.shape (251103, 16)\n",
      "Drop some columns\n",
      "bureau_agg_train.shape (1465325, 13)\n",
      "bureau_agg_test.shape (251103, 13)\n",
      "Impute missing values\n",
      "bureau_agg_train.isnull().sum().sum() 0\n",
      "bureau_agg_test.isnull().sum().sum() 0\n",
      "bureau_agg_train.shape (1465325, 18)\n",
      "bureau_agg_test.shape (251103, 18)\n",
      "Add some cols\n",
      "bureau_agg_train.shape (1465325, 26)\n",
      "bureau_agg_test.shape (251103, 26)\n",
      "Remove cat cols with many classes\n",
      "Number of cols droped 0\n",
      "bureau_agg_train.shape (1465325, 26)\n",
      "bureau_agg_test.shape (251103, 26)\n",
      "One-hot encoding\n",
      "bureau_agg_train.isnull().sum().sum() 0\n",
      "bureau_agg_test.isnull().sum().sum() 0\n",
      "bureau_agg_train.shape (1465325, 43)\n",
      "bureau_agg_test.shape (251103, 43)\n",
      "Aggregating\n",
      "bureau_agg_train.isnull().sum().sum() 0\n",
      "bureau_agg_test.isnull().sum().sum() 0\n",
      "bureau_agg_train.shape (263491, 41)\n",
      "bureau_agg_test.shape (42320, 41)\n",
      "Remove constant columns\n",
      "Number of constant columns droped: 0\n",
      "Number of constant columns droped: 0\n",
      "bureau_agg_train.shape (263491, 41)\n",
      "bureau_agg_test.shape (42320, 41)\n",
      "Remove collinear columns\n",
      "Number of columns droped due to collinearity: 1\n",
      "Number of columns droped due to collinearity: 1\n",
      "bureau_agg_train.shape (263491, 40)\n",
      "bureau_agg_test.shape (42320, 40)\n",
      "bureau_agg_train.shape (263491, 41)\n",
      "bureau_agg_test.shape (42320, 41)\n",
      "Elapsed Time 16.383594036102295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregate with exponential weighted mean\")\n",
    "time_start = time.time()\n",
    "\n",
    "bureau_agg_train = bureau_train.copy()\n",
    "bureau_agg_test = bureau_test.copy()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Drop some columns\")\n",
    "bureau_agg_train = drop_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = drop_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = BureauImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add some cols\")\n",
    "bureau_agg_train = add_new_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = add_new_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove cat cols with many classes\")\n",
    "remover = CatColsRemover(max_class=20)\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder(to_array=False)\n",
    "ohe.fit(bureau_agg_train)\n",
    "bureau_agg_train = ohe.transform(bureau_agg_train)\n",
    "bureau_agg_test = ohe.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregating\")\n",
    "by_cols = [\"SK_ID_CURR\"]\n",
    "weight_decay = 1./300\n",
    "weight_col = \"DAYS_CREDIT\"\n",
    "\n",
    "bureau_agg_train = exp_mean_agg(bureau_agg_train, by_cols, weight_col, weight_decay=weight_decay)\n",
    "bureau_agg_test = exp_mean_agg(bureau_agg_test, by_cols, weight_col, weight_decay=weight_decay)\n",
    "\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "bureau_agg_train = bureau_agg_train.reset_index()\n",
    "bureau_agg_test = bureau_agg_test.reset_index()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    bureau_agg_train.to_csv(\"data/data_/bureau_expmean_agg_train.csv\", index=False)\n",
    "    bureau_agg_test.to_csv(\"data/data_/bureau_expmean_agg_test.csv\", index=False)\n",
    "\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau_balance.csv` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_nearest_status(df):\n",
    "    return df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[0]\n",
    "\n",
    "def bb_mode_status_three_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 3]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def bb_mode_status_six_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 6]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "\n",
    "def add_new_agg_cols_bb(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_STATUS\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_nearest_status)\n",
    "    results[\"MODE_STATUS_THREE_NEAREST\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_mode_status_three_nearest)\n",
    "    results[\"MODE_STATUS_SIX_NEAREST\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 655.20 MB\n",
      "Memory usage after changing types 245.70 MB\n",
      "bureau_balance.shape (27299925, 3)\n",
      "bureau_balance.shape (24179741, 4)\n",
      "bureau_balance_train.shape: (14701612, 3)\n",
      "bureau_balance_test.shape: (9478129, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5715448</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_BUREAU  MONTHS_BALANCE STATUS\n",
       "0       5715448               0      C\n",
       "1       5715448              -1      C\n",
       "2       5715448              -2      C\n",
       "3       5715448              -3      C\n",
       "4       5715448              -4      C"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance = load_csv(\"data/download/bureau_balance.csv\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance = bureau_balance.merge(bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]], how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance = bureau_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "bureau_balance[\"SK_ID_CURR\"] = bureau_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_partition(bureau_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "bureau_balance_train = bureau_balance_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "bureau_balance_test = bureau_balance_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"bureau_balance_train.shape:\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape:\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate with many statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggragate with many statistics\")\n",
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "new_agg_cols_train = add_new_agg_cols_bb(bureau_balance_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bb(bureau_balance_agg_test)\n",
    "\n",
    "\n",
    "#print(\"Drop MONTHS_BALANCE\")\n",
    "#bureau_balance_agg_train = bureau_balance_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "#bureau_balance_agg_test = bureau_balance_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_BUREAU\")\n",
    "by_list_cols = [\"SK_ID_BUREAU\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new aggs cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_balance_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_balance_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\", index=False)\n",
    "\"\"\"\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "bureau_balance_agg_train = load_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\")\n",
    "bureau_balance_agg_test = load_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\"\"\"\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.merge(bureau_train_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.merge(bureau_test_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_2[\"num_stats\"], \n",
    "                        AGG_STATS_2[\"bool_stats\"], \n",
    "                        AGG_STATS_2[\"cat_stats\"],\n",
    "                        AGG_STATS_2[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate with exponential mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau_balance_agg_train.shape (14701612, 3)\n",
      "bureau_balance_agg_test.shape (9478129, 3)\n",
      "Remove cat cols with many classes\n",
      "Number of cols droped 0\n",
      "bureau_balance_agg_train.shape (14701612, 3)\n",
      "bureau_balance_agg_test.shape (9478129, 3)\n",
      "One-hot encoding\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (14701612, 9)\n",
      "bureau_balance_agg_test.shape (9478129, 9)\n",
      "Aggregate over SK_ID_BUREAU\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (523515, 7)\n",
      "bureau_balance_agg_test.shape (250839, 7)\n",
      "Prepare to aggregate over SK_ID_CURR\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (523515, 9)\n",
      "bureau_balance_agg_test.shape (250839, 9)\n",
      "bureau_balance_agg_train.shape (523515, 8)\n",
      "bureau_balance_agg_test.shape (250839, 8)\n",
      "Aggregate over SK_ID_CURR\n",
      "No cat cols in df_train, so do nothing.\n",
      "Aggregating num df with shape: (523515, 7)\n",
      "df_num.shape for iqr, mimmax_range, mean_median_diff (523515, 7)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with shape: (250839, 7)\n",
      "df_num.shape for iqr, mimmax_range, mean_median_diff (250839, 7)\n",
      "Aggregating num df with iqr\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 85302\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 38178\n",
      "In case\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "Remove constant columns\n",
      "Number of constant columns droped: 0\n",
      "Number of constant columns droped: 0\n",
      "bureau_balance_agg_train.shape (92231, 49)\n",
      "bureau_balance_agg_test.shape (42311, 49)\n",
      "Remove collinear columns\n",
      "Number of columns droped due to collinearity: 0\n",
      "Number of columns droped due to collinearity: 0\n",
      "bureau_balance_agg_train.shape (92231, 49)\n",
      "bureau_balance_agg_test.shape (42311, 49)\n",
      "bureau_balance_agg_train.shape (92231, 50)\n",
      "bureau_balance_agg_test.shape (42311, 50)\n",
      "Elapsed Time 15.024099349975586\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregate with exponential weighted mean\")\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove cat cols with many classes\")\n",
    "remover = CatColsRemover(max_class=20)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder(to_array=False)\n",
    "ohe.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = ohe.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = ohe.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_BUREAU\")\n",
    "by_cols = [\"SK_ID_BUREAU\"]\n",
    "weight_decay = 1./10\n",
    "weight_col = \"MONTHS_BALANCE\"\n",
    "\n",
    "bureau_balance_agg_train = exp_mean_agg(bureau_balance_agg_train, by_cols, weight_col, weight_decay=weight_decay)\n",
    "bureau_balance_agg_test = exp_mean_agg(bureau_balance_agg_test, by_cols, weight_col, weight_decay=weight_decay)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Prepare to aggregate over SK_ID_CURR\")\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.merge(bureau_train_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.merge(bureau_test_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_2[\"num_stats\"], \n",
    "                        AGG_STATS_2[\"bool_stats\"], \n",
    "                        AGG_STATS_2[\"cat_stats\"],\n",
    "                        AGG_STATS_2[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_expmean_agg_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_expmean_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_agg_cols_bb(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_STATUS\"] = df.groupby(\"SK_ID_CURR\").apply(bb_nearest_status)\n",
    "    results[\"MODE_STATUS_THREE_NEAREST\"] = df.groupby(\"SK_ID_CURR\").apply(bb_mode_status_three_nearest)\n",
    "    results[\"MODE_STATUS_SIX_NEAREST\"] = df.groupby(\"SK_ID_CURR\").apply(bb_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 655.20 MB\n",
      "Memory usage after changing types 245.70 MB\n",
      "bureau_balance.shape (27299925, 3)\n",
      "bureau_balance.shape (24179741, 4)\n",
      "bureau_balance_train.shape: (14701612, 3)\n",
      "bureau_balance_test.shape: (9478129, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>380361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "      <td>380361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "      <td>380361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3</td>\n",
       "      <td>C</td>\n",
       "      <td>380361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4</td>\n",
       "      <td>C</td>\n",
       "      <td>380361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MONTHS_BALANCE STATUS  SK_ID_CURR\n",
       "0               0      C      380361\n",
       "1              -1      C      380361\n",
       "2              -2      C      380361\n",
       "3              -3      C      380361\n",
       "4              -4      C      380361"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance = load_csv(\"data/download/bureau_balance.csv\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance = bureau_balance.merge(bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]], how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance = bureau_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "bureau_balance[\"SK_ID_CURR\"] = bureau_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_partition(bureau_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "bureau_balance_train = bureau_balance_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_balance_test = bureau_balance_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "\n",
    "print(\"bureau_balance_train.shape:\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape:\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate with many statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregate with many statistics\")\n",
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "new_agg_cols_train = add_new_agg_cols_bb(bureau_balance_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bb(bureau_balance_agg_test)\n",
    "\n",
    "\n",
    "#print(\"Drop MONTHS_BALANCE\")\n",
    "#bureau_balance_agg_train = bureau_balance_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "#bureau_balance_agg_test = bureau_balance_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"One-step Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new aggs cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_balance_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_balance_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_onestep_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_onestep_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate with exponentially weighted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate with exponentially weighted mean\n",
      "bureau_balance_agg_train.shape (14701612, 3)\n",
      "bureau_balance_agg_test.shape (9478129, 3)\n",
      "Remove cat cols with many classes\n",
      "Number of cols droped 0\n",
      "bureau_balance_agg_train.shape (14701612, 3)\n",
      "bureau_balance_agg_test.shape (9478129, 3)\n",
      "One-hot encoding\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (14701612, 9)\n",
      "bureau_balance_agg_test.shape (9478129, 9)\n",
      "Aggregate over SK_ID_CURR\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (92231, 7)\n",
      "bureau_balance_agg_test.shape (42311, 7)\n",
      "Remove constant columns\n",
      "Number of constant columns droped: 0\n",
      "Number of constant columns droped: 0\n",
      "bureau_balance_agg_train.shape (92231, 7)\n",
      "bureau_balance_agg_test.shape (42311, 7)\n",
      "remove collinear columns\n",
      "Number of columns droped due to collinearity: 0\n",
      "Number of columns droped due to collinearity: 0\n",
      "bureau_balance_agg_train.shape (92231, 7)\n",
      "bureau_balance_agg_test.shape (42311, 7)\n",
      "bureau_balance_agg_train.shape (92231, 8)\n",
      "bureau_balance_agg_test.shape (42311, 8)\n",
      "Elapsed Time 7.91322135925293\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregate with exponentially weighted mean\")\n",
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove cat cols with many classes\")\n",
    "remover = CatColsRemover(max_class=20)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder(to_array=False)\n",
    "ohe.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = ohe.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = ohe.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_cols = [\"SK_ID_CURR\"]\n",
    "weight_decay = 1./10\n",
    "weight_col = \"MONTHS_BALANCE\"\n",
    "\n",
    "bureau_balance_agg_train = exp_mean_agg(bureau_balance_agg_train, by_cols, weight_col, weight_decay=weight_decay)\n",
    "bureau_balance_agg_test = exp_mean_agg(bureau_balance_agg_test, by_cols, weight_col, weight_decay=weight_decay)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_expmean_agg_onestep_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_expmean_agg_onestep_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `previous application` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `previous_application.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"RATE_DOWN_PAYMENT\": -1.,\n",
    "                                   \"CNT_PAYMENT\": -10.,\n",
    "                                   \"DAYS_FIRST_DRAWING\": 0., \n",
    "                                   \"DAYS_FIRST_DUE\": 0.,\n",
    "                                   \"DAYS_LAST_DUE_1ST_VERSION\": 0.,\n",
    "                                   \"DAYS_LAST_DUE\": 0.,\n",
    "                                   \"DAYS_TERMINATION\": 0.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {\"NAME_TYPE_SUITE\": \"missing_value\",\n",
    "                                    \"NFLAG_INSURED_ON_APPROVAL\": \"missing_value\"}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "\n",
    "def hour_period_bin(hours):\n",
    "    hours = hours.values\n",
    "    hour_bin = np.array([\"evening\"] * len(hours), dtype=\"object\")\n",
    "    morning_mask = (hours > 5) & (hours < 12)\n",
    "    afternoon_mask = (hours >= 12) & (hours < 18)\n",
    "    \n",
    "    hour_bin[morning_mask] = \"morning\"\n",
    "    hour_bin[afternoon_mask] = \"afternoon\"\n",
    "    return hour_bin\n",
    "\n",
    "\n",
    "def add_new_cols_prev_appl(df):\n",
    "    # add to bool cols to identify if values are non-negative\n",
    "    cols_is_nonneg = [\"DAYS_FIRST_DRAWING\", \"DAYS_FIRST_DUE\", \n",
    "                      \"DAYS_LAST_DUE_1ST_VERSION\", \n",
    "                      \"DAYS_LAST_DUE\", \"DAYS_TERMINATION\"]\n",
    "    for col in cols_is_nonneg:\n",
    "        df[col + \"_IS_NONNEG\"] = df[col] >= 0\n",
    "        \n",
    "        df[\"PERIOD_APPR_PROCESS_START\"] = hour_period_bin(df[\"HOUR_APPR_PROCESS_START\"])\n",
    "        df[\"PERIOD_APPR_PROCESS_START\"] = df[\"PERIOD_APPR_PROCESS_START\"].astype(\"category\")\n",
    "    \n",
    "    df[\"AMT_APPLICATION_TO_CREDIT\"] = df[\"AMT_APPLICATION\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    df[\"AMT_DOWN_PAY_TO_CREDIT\"] = df[\"AMT_DOWN_PAYMENT\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    df[\"AMT_GOODS_PRICE_TO_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def mean_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.mean()\n",
    "\n",
    "def min_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.min()\n",
    "\n",
    "def max_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.max()\n",
    "\n",
    "\n",
    "def add_new_agg_cols_prev_appl(df):\n",
    "    sort_by = [\"DAYS_DECISION\"]\n",
    "    orig_cols = [\"AMT_APPLICATION_TO_CREDIT\", \"AMT_DOWN_PAY_TO_CREDIT\", \"AMT_GOODS_PRICE_TO_CREDIT\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 494.38 MB\n",
      "Memory usage after changing types 162.02 MB\n",
      "previous_application.shape (1670214, 37)\n",
      "prev_appl_train.shape (1413701, 37)\n",
      "prev_appl_test.shape (256513, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_APPLICATION</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_DOWN_PAYMENT</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>...</th>\n",
       "      <th>NAME_SELLER_INDUSTRY</th>\n",
       "      <th>CNT_PAYMENT</th>\n",
       "      <th>NAME_YIELD_GROUP</th>\n",
       "      <th>PRODUCT_COMBINATION</th>\n",
       "      <th>DAYS_FIRST_DRAWING</th>\n",
       "      <th>DAYS_FIRST_DUE</th>\n",
       "      <th>DAYS_LAST_DUE_1ST_VERSION</th>\n",
       "      <th>DAYS_LAST_DUE</th>\n",
       "      <th>DAYS_TERMINATION</th>\n",
       "      <th>NFLAG_INSURED_ON_APPROVAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030495</td>\n",
       "      <td>271877</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>1730.430054</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>Connectivity</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>POS mobile with interest</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2802425</td>\n",
       "      <td>108129</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>25188.615234</td>\n",
       "      <td>607500.0</td>\n",
       "      <td>679671.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>607500.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>36.0</td>\n",
       "      <td>low_action</td>\n",
       "      <td>Cash X-Sell: low</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>916.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2523466</td>\n",
       "      <td>122040</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>15060.735352</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>136444.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>high</td>\n",
       "      <td>Cash X-Sell: high</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-271.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2819243</td>\n",
       "      <td>176158</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>47041.335938</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>470790.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>Cash X-Sell: middle</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-482.0</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>-182.0</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1784265</td>\n",
       "      <td>202054</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>31924.394531</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>404055.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>24.0</td>\n",
       "      <td>high</td>\n",
       "      <td>Cash Street: high</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV  SK_ID_CURR NAME_CONTRACT_TYPE   AMT_ANNUITY  AMT_APPLICATION  \\\n",
       "0     2030495      271877     Consumer loans   1730.430054          17145.0   \n",
       "1     2802425      108129         Cash loans  25188.615234         607500.0   \n",
       "2     2523466      122040         Cash loans  15060.735352         112500.0   \n",
       "3     2819243      176158         Cash loans  47041.335938         450000.0   \n",
       "4     1784265      202054         Cash loans  31924.394531         337500.0   \n",
       "\n",
       "   AMT_CREDIT  AMT_DOWN_PAYMENT  AMT_GOODS_PRICE WEEKDAY_APPR_PROCESS_START  \\\n",
       "0     17145.0               0.0          17145.0                   SATURDAY   \n",
       "1    679671.0               NaN         607500.0                   THURSDAY   \n",
       "2    136444.5               NaN         112500.0                    TUESDAY   \n",
       "3    470790.0               NaN         450000.0                     MONDAY   \n",
       "4    404055.0               NaN         337500.0                   THURSDAY   \n",
       "\n",
       "   HOUR_APPR_PROCESS_START  ... NAME_SELLER_INDUSTRY CNT_PAYMENT  \\\n",
       "0                       15  ...         Connectivity        12.0   \n",
       "1                       11  ...                  XNA        36.0   \n",
       "2                       11  ...                  XNA        12.0   \n",
       "3                        7  ...                  XNA        12.0   \n",
       "4                        9  ...                  XNA        24.0   \n",
       "\n",
       "   NAME_YIELD_GROUP       PRODUCT_COMBINATION  DAYS_FIRST_DRAWING  \\\n",
       "0            middle  POS mobile with interest            365243.0   \n",
       "1        low_action          Cash X-Sell: low            365243.0   \n",
       "2              high         Cash X-Sell: high            365243.0   \n",
       "3            middle       Cash X-Sell: middle            365243.0   \n",
       "4              high         Cash Street: high                 NaN   \n",
       "\n",
       "  DAYS_FIRST_DUE DAYS_LAST_DUE_1ST_VERSION  DAYS_LAST_DUE DAYS_TERMINATION  \\\n",
       "0          -42.0                     300.0          -42.0            -37.0   \n",
       "1         -134.0                     916.0       365243.0         365243.0   \n",
       "2         -271.0                      59.0       365243.0         365243.0   \n",
       "3         -482.0                    -152.0         -182.0           -177.0   \n",
       "4            NaN                       NaN            NaN              NaN   \n",
       "\n",
       "  NFLAG_INSURED_ON_APPROVAL  \n",
       "0                       0.0  \n",
       "1                       1.0  \n",
       "2                       1.0  \n",
       "3                       1.0  \n",
       "4                       NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_application = load_csv(\"data/download/previous_application.csv\")\n",
    "print(\"previous_application.shape\", previous_application.shape)\n",
    "\n",
    "prev_appl_train, prev_appl_test = train_test_partition(previous_application, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_train_keys = prev_appl_train[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "prev_appl_test_keys = prev_appl_test[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "\n",
    "prev_appl_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate with many statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "prev_appl_agg_train = prev_appl_train.copy()\n",
    "prev_appl_agg_test = prev_appl_test.copy()\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# drop cols with high percentage of null\n",
    "cols_to_drop = [\"RATE_INTEREST_PRIMARY\", \"RATE_INTEREST_PRIVILEGED\"]\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop(cols_to_drop, axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop(cols_to_drop, axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = PrevApplImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add new cols\")\n",
    "prev_appl_agg_train = add_new_cols_prev_appl(prev_appl_agg_train)\n",
    "prev_appl_agg_test = add_new_cols_prev_appl(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/prev_appl_new_agg_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/prev_appl_new_agg_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_prev_appl(prev_appl_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_prev_appl(prev_appl_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/prev_appl_new_agg_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/prev_appl_new_agg_cols_test.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols,\n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = aggregator.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = aggregator.transform(prev_appl_agg_test)\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    prev_appl_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    prev_appl_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = remover.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = remover.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = remover.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = remover.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.reset_index()\n",
    "prev_appl_agg_test = prev_appl_agg_test.reset_index()\n",
    "\n",
    "if True:\n",
    "    prev_appl_agg_train.to_csv(\"data/data_/previous_application_agg_train.csv\", index=False)\n",
    "    prev_appl_agg_test.to_csv(\"data/data_/previous_application_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate with exponetially weighted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "prev_appl_agg_train = prev_appl_train.copy()\n",
    "prev_appl_agg_test = prev_appl_test.copy()\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# drop cols with high percentage of null\n",
    "cols_to_drop = [\"RATE_INTEREST_PRIMARY\", \"RATE_INTEREST_PRIVILEGED\"]\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop(cols_to_drop, axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop(cols_to_drop, axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = PrevApplImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add new cols\")\n",
    "prev_appl_agg_train = add_new_cols_prev_appl(prev_appl_agg_train)\n",
    "prev_appl_agg_test = add_new_cols_prev_appl(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `POS_CASH_balance.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCBalImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"CNT_INSTALMENT\": -1.,\n",
    "                                   \"CNT_INSTALMENT_FUTURE\": -1.,\n",
    "                                   }\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "\n",
    "def add_new_agg_cols_pc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\", \"SK_DPD\", \"SK_DPD_DEF\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance = load_csv(\"data/download/POS_CASH_balance.csv\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "POS_CASH_balance = POS_CASH_balance.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "POS_CASH_balance = POS_CASH_balance.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]], \n",
    "                                          how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "# drop rows that does not have \"SK_ID_PREV\" in previous_application\n",
    "POS_CASH_balance = POS_CASH_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "POS_CASH_balance[\"SK_ID_CURR\"] = POS_CASH_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "PC_bal_train, PC_bal_test = train_test_partition(POS_CASH_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "PC_bal_train = PC_bal_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "PC_bal_test = PC_bal_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"PC_bal_train.shape:\", PC_bal_train.shape)\n",
    "print(\"PC_bal_test.shape:\", PC_bal_test.shape)\n",
    "\n",
    "PC_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "PC_bal_agg_train = PC_bal_train.copy()\n",
    "PC_bal_agg_test = PC_bal_test.copy()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = PCBalImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/pos_bal_new_agg_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/pos_bal_new_agg_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_pc_bal(PC_bal_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_pc_bal(PC_bal_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/pos_bal_new_agg_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/pos_bal_new_agg_cols_test.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "#PC_bal_agg_train = PC_bal_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "#PC_bal_agg_test = PC_bal_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_PREV\")\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    PC_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    PC_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if True:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_train_tmp.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_test_tmp.csv\", index=False)\n",
    "\"\"\"\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over SK_ID_CURR\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# this turns [0, 1] into bool\n",
    "PC_bal_agg_train = load_csv(\"data/data_/POS_CASH_balance_agg_train_tmp.csv\")\n",
    "PC_bal_agg_test = load_csv(\"data/data_/POS_CASH_balance_agg_test_tmp.csv\")\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\"\"\"\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_2[\"num_stats\"], \n",
    "                        AGG_STATS_2[\"bool_stats\"], \n",
    "                        AGG_STATS_2[\"cat_stats\"],\n",
    "                        AGG_STATS_2[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_train.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_agg_cols_pc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\", \"SK_DPD\", \"SK_DPD_DEF\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance = load_csv(\"data/download/POS_CASH_balance.csv\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "PC_bal_train, PC_bal_test = train_test_partition(POS_CASH_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "PC_bal_train = PC_bal_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "PC_bal_test = PC_bal_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "\n",
    "print(\"PC_bal_train.shape:\", PC_bal_train.shape)\n",
    "print(\"PC_bal_test.shape:\", PC_bal_test.shape)\n",
    "\n",
    "PC_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "PC_bal_agg_train = PC_bal_train.copy()\n",
    "PC_bal_agg_test = PC_bal_test.copy()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = PCBalImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/pos_bal_new_agg_onestep_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/pos_bal_new_agg_onestep_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_pc_bal(PC_bal_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_pc_bal(PC_bal_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/pos_bal_new_agg_onestep_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/pos_bal_new_agg_onestep_cols_test.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "#PC_bal_agg_train = PC_bal_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "#PC_bal_agg_test = PC_bal_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    PC_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    PC_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "if True:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_onestep_train.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_onestep_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `installments_payments.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstalPayImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._days_entry_payment_impute = df_train[\"DAYS_ENTRY_PAYMENT\"].min() - 10\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df[\"DAYS_ENTRY_PAYMENT\"] = df[\"DAYS_ENTRY_PAYMENT\"].fillna(self._days_entry_payment_impute)\n",
    "        df[\"AMT_PAYMENT\"] = df[\"AMT_PAYMENT\"].fillna(0.)\n",
    "        return df\n",
    "\n",
    "\n",
    "def add_new_cols_inst_pay(df):\n",
    "    df[\"DAYS_INSTAL_PAY_DIFF\"] = df[\"DAYS_ENTRY_PAYMENT\"] - df[\"DAYS_INSTALMENT\"]\n",
    "    df[\"DAYS_INSTAL_PAY_DIFF_ISPOSITIVE\"] = df[\"DAYS_INSTAL_PAY_DIFF\"] > 0\n",
    "    \n",
    "    df[\"AMT_INSTAL_PAY_DIFF\"] = df[\"AMT_PAYMENT\"] - df[\"AMT_INSTALMENT\"]\n",
    "    df[\"AMT_INSTAL_PAY_DIFF_ISPOSITIVE\"] = df[\"AMT_INSTAL_PAY_DIFF\"] > 0\n",
    "    \n",
    "    df[\"AMT_PAY_INSTAL_RATIO\"] = np.clip((df[\"AMT_PAYMENT\"] + 1) / (df[\"AMT_INSTALMENT\"] + 1), 0., 10.)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_agg_cols_inst_pay(df):\n",
    "    sort_by = [\"DAYS_INSTALMENT\"]\n",
    "    orig_cols = [\"DAYS_INSTAL_PAY_DIFF\", \"AMT_INSTAL_PAY_DIFF\", \"AMT_PAY_INSTAL_RATIO\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments = load_csv(\"data/download/installments_payments.csv\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "installments_payments = installments_payments.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "installments_payments = installments_payments.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]],\n",
    "                                                   how=\"left\", on=\"SK_ID_PREV\")\n",
    "\n",
    "# drop rows that does not have \"SK_ID_PREV\" in previous_application\n",
    "installments_payments = installments_payments.dropna(subset=[\"SK_ID_CURR\"])\n",
    "installments_payments[\"SK_ID_CURR\"] = installments_payments[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "inst_pay_train, inst_pay_test = train_test_partition(installments_payments, \"SK_ID_CURR\", appl_train_key)\n",
    "inst_pay_train = inst_pay_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "inst_pay_test = inst_pay_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"inst_pay_train.shape:\", inst_pay_train.shape)\n",
    "print(\"inst_pay_test.shape:\", inst_pay_test.shape)\n",
    "\n",
    "inst_pay_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "inst_pay_agg_train = inst_pay_train.copy()\n",
    "inst_pay_agg_test = inst_pay_test.copy()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute\")\n",
    "imputer = InstalPayImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"default imputer\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add some more cols\")\n",
    "inst_pay_agg_train = add_new_cols_inst_pay(inst_pay_agg_train)\n",
    "inst_pay_agg_test = add_new_cols_inst_pay(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/inst_pay_new_agg_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/inst_pay_new_agg_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_inst_pay(inst_pay_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_inst_pay(inst_pay_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/inst_pay_new_agg_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/inst_pay_new_agg_cols_test.pickle\", \"wb\"))\n",
    "    \n",
    "#inst_pay_agg_train = inst_pay_agg_train.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "#inst_pay_agg_test = inst_pay_agg_test.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_PREV\")\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    inst_pay_agg_train[col] = new_agg_cols_train[col]\n",
    "    \n",
    "for col in new_agg_cols_test:\n",
    "    inst_pay_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Just in case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "if True:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_train_tmp.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_test_tmp.csv\", index=False)\n",
    "\"\"\"\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\"\"\"\n",
    "inst_pay_agg_train = load_csv(\"data/data_/installments_payments_agg_train_tmp.csv\")\n",
    "inst_pay_agg_test = load_csv(\"data/data_/installments_payments_agg_test_tmp.csv\")\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\"\"\"\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_2[\"num_stats\"], \n",
    "                        AGG_STATS_2[\"bool_stats\"], \n",
    "                        AGG_STATS_2[\"cat_stats\"],\n",
    "                        AGG_STATS_2[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Just in case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_train.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_agg_cols_inst_pay(df):\n",
    "    sort_by = [\"DAYS_INSTALMENT\"]\n",
    "    orig_cols = [\"DAYS_INSTAL_PAY_DIFF\", \"AMT_INSTAL_PAY_DIFF\", \"AMT_PAY_INSTAL_RATIO\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments = load_csv(\"data/download/installments_payments.csv\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "inst_pay_train, inst_pay_test = train_test_partition(installments_payments, \"SK_ID_CURR\", appl_train_key)\n",
    "inst_pay_train = inst_pay_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "inst_pay_test = inst_pay_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"inst_pay_train.shape:\", inst_pay_train.shape)\n",
    "print(\"inst_pay_test.shape:\", inst_pay_test.shape)\n",
    "\n",
    "inst_pay_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "inst_pay_agg_train = inst_pay_train.copy()\n",
    "inst_pay_agg_test = inst_pay_test.copy()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute\")\n",
    "imputer = InstalPayImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"default imputer\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add some more cols\")\n",
    "inst_pay_agg_train = add_new_cols_inst_pay(inst_pay_agg_train)\n",
    "inst_pay_agg_test = add_new_cols_inst_pay(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/inst_pay_new_agg_onestep_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/inst_pay_new_agg_onestep_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_inst_pay(inst_pay_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_inst_pay(inst_pay_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/inst_pay_new_agg_onestep_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/inst_pay_new_agg_onestep_cols_test.pickle\", \"wb\"))\n",
    "    \n",
    "#inst_pay_agg_train = inst_pay_agg_train.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "#inst_pay_agg_test = inst_pay_agg_test.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    inst_pay_agg_train[col] = new_agg_cols_train[col]\n",
    "    \n",
    "for col in new_agg_cols_test:\n",
    "    inst_pay_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Just in case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_onestep_train.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_onestep_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `credit_card_balance.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCBalImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"AMT_DRAWINGS_ATM_CURRENT\": 0.,\n",
    "                                   \"AMT_DRAWINGS_OTHER_CURRENT\": 0., \n",
    "                                    \"AMT_DRAWINGS_POS_CURRENT\": 0.,\n",
    "                                    \"AMT_INST_MIN_REGULARITY\": 0.,\n",
    "                                    \"AMT_PAYMENT_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_ATM_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_OTHER_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_POS_CURRENT\": 0.,\n",
    "                                    \"CNT_INSTALMENT_MATURE_CUM\": 0.,\n",
    "                                    \n",
    "                                   }\n",
    "        self._default_imput_vals_num = 0.\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "        \n",
    "\n",
    "def add_new_cols_cc_bal(df):\n",
    "    df[\"AMT_PAYMENT_TO_BALANCE\"] = df[\"AMT_PAYMENT_TOTAL_CURRENT\"] / df[\"AMT_BALANCE\"].replace(0., 1.)\n",
    "    df[\"AMT_BALANCE_TO_CREDIT_LIMIT\"] = (df[\"AMT_BALANCE\"] + 1) / (df[\"AMT_CREDIT_LIMIT_ACTUAL\"] + 1)\n",
    "    df[\"AMT_DRAWING_TOTAL\"] = (df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_CURRENT\"] + \n",
    "                              df[\"AMT_DRAWINGS_OTHER_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"])\n",
    "    df[\"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\"] = df[\"AMT_DRAWING_TOTAL\"] / (df[\"AMT_CREDIT_LIMIT_ACTUAL\"] + 1)\n",
    "    df[\"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\"] = (df[\"AMT_PAYMENT_TOTAL_CURRENT\"] / \n",
    "                                                df[\"AMT_DRAWING_TOTAL\"].replace(0., 1.))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_agg_cols_cc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"AMT_PAYMENT_TO_BALANCE\", \"AMT_BALANCE_TO_CREDIT_LIMIT\", \n",
    "                 \"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\", \"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\", \n",
    "                 \"SK_DPD\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance = load_csv(\"data/download/credit_card_balance.csv\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "credit_card_balance = credit_card_balance.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "credit_card_balance = credit_card_balance.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]],\n",
    "                                               how=\"left\", on=\"SK_ID_PREV\")\n",
    "\n",
    "credit_card_balance = credit_card_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "credit_card_balance[\"SK_ID_CURR\"] = credit_card_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "cc_bal_train, cc_bal_test = train_test_partition(credit_card_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "cc_bal_train = cc_bal_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "cc_bal_test = cc_bal_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"cc_bal_train.shape:\", cc_bal_train.shape)\n",
    "print(\"cc_bal_test.shape:\", cc_bal_test.shape)\n",
    "\n",
    "cc_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "cc_bal_agg_train = cc_bal_train.copy()\n",
    "cc_bal_agg_test = cc_bal_test.copy()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = CCBalImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add new cols\")\n",
    "cc_bal_agg_train = add_new_cols_cc_bal(cc_bal_agg_train)\n",
    "cc_bal_agg_test = add_new_cols_cc_bal(cc_bal_agg_test)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/cc_bal_new_agg_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/cc_bal_new_agg_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_cc_bal(cc_bal_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_cc_bal(cc_bal_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/cc_bal_new_agg_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/cc_bal_new_agg_cols_test.pickle\", \"wb\"))\n",
    "\n",
    "    \n",
    "#cc_bal_agg_train = cc_bal_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "#cc_bal_agg_test = cc_bal_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.99)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_PREV\")\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    cc_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    cc_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Just in case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "if True:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_train_tmp.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_test_tmp.csv\", index=False)\n",
    "\"\"\"\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\"\"\"\n",
    "cc_bal_agg_train = load_csv(\"data/data_/credit_card_balance_agg_train_tmp.csv\")\n",
    "cc_bal_agg_test = load_csv(\"data/data_/credit_card_balance_agg_test_tmp.csv\")\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\"\"\"\n",
    "\n",
    "cc_bal_agg_train = cc_bal_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "cc_bal_agg_test = cc_bal_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "cc_bal_agg_train = cc_bal_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "cc_bal_agg_test = cc_bal_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols,\n",
    "                        AGG_STATS_2[\"num_stats\"], \n",
    "                        AGG_STATS_2[\"bool_stats\"], \n",
    "                        AGG_STATS_2[\"cat_stats\"],\n",
    "                        AGG_STATS_2[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"In case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_train.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_agg_cols_cc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"AMT_PAYMENT_TO_BALANCE\", \"AMT_BALANCE_TO_CREDIT_LIMIT\", \n",
    "                 \"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\", \"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\", \n",
    "                 \"SK_DPD\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance = load_csv(\"data/download/credit_card_balance.csv\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "cc_bal_train, cc_bal_test = train_test_partition(credit_card_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "cc_bal_train = cc_bal_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "cc_bal_test = cc_bal_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "\n",
    "print(\"cc_bal_train.shape:\", cc_bal_train.shape)\n",
    "print(\"cc_bal_test.shape:\", cc_bal_test.shape)\n",
    "\n",
    "cc_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "cc_bal_agg_train = cc_bal_train.copy()\n",
    "cc_bal_agg_test = cc_bal_test.copy()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute missing values\")\n",
    "imputer = CCBalImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Add new cols\")\n",
    "cc_bal_agg_train = add_new_cols_cc_bal(cc_bal_agg_train)\n",
    "cc_bal_agg_test = add_new_cols_cc_bal(cc_bal_agg_test)\n",
    "\n",
    "\n",
    "print(\"New agg cols\")\n",
    "\n",
    "load_prev = True\n",
    "\n",
    "if load_prev:\n",
    "    print(\"Load previous\")\n",
    "    new_agg_cols_train = pickle.load(open(\"data/tmp_/cc_bal_new_agg_onestep_cols_train.pickle\", \"rb\"))\n",
    "    new_agg_cols_test = pickle.load(open(\"data/tmp_/cc_bal_new_agg_onestep_cols_test.pickle\", \"rb\"))\n",
    "else:\n",
    "    print(\"New calculation\")\n",
    "    new_agg_cols_train = add_new_agg_cols_cc_bal(cc_bal_agg_train)\n",
    "    new_agg_cols_test = add_new_agg_cols_cc_bal(cc_bal_agg_test)\n",
    "    pickle.dump(new_agg_cols_train, open(\"data/tmp_/cc_bal_new_agg_onestep_cols_train.pickle\", \"wb\"))\n",
    "    pickle.dump(new_agg_cols_test, open(\"data/tmp_/cc_bal_new_agg_onestep_cols_test.pickle\", \"wb\"))\n",
    "\n",
    "    \n",
    "#cc_bal_agg_train = cc_bal_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "#cc_bal_agg_test = cc_bal_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.99)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Aggregate over SK_ID_CURR\")\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, \n",
    "                        AGG_STATS_1[\"num_stats\"], \n",
    "                        AGG_STATS_1[\"bool_stats\"], \n",
    "                        AGG_STATS_1[\"cat_stats\"],\n",
    "                        AGG_STATS_1[\"ohe_cat_stats\"],\n",
    "                        ohe_cat_max_class=20,\n",
    "                        iqr=True, minmax_range=False)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Add new agg cols\")\n",
    "for col in new_agg_cols_train:\n",
    "    cc_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    cc_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Just in case\")\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"Remove constant columns\")\n",
    "remover = ConstantColumnsRemover()\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "print(\"Remove collinear columns\")\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_onestep_train.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_onestep_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_col_align(df_train, df_test, exclude_cols=None):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    cols_train = df_train.columns.to_list()\n",
    "    \n",
    "    for col in exclude_cols:\n",
    "        assert col in cols_train, col + \" is not in df_train\"\n",
    "        \n",
    "    test_cols = [col for col in cols_train if col not in exclude_cols]\n",
    "    return df_train[test_cols + exclude_cols], df_test[test_cols]\n",
    "\n",
    "\n",
    "def add_prefix_to_cols(df, prefix, exclude_cols=None):\n",
    "    df = df.copy()\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "        \n",
    "    cols = df.columns.to_list()\n",
    "    for i, col in enumerate(cols):\n",
    "        if col not in exclude_cols:\n",
    "            cols[i] = prefix + col\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframes(df_left, dfs_right, \n",
    "                     left_prefix, right_prefixes, \n",
    "                     on_col=\"SK_ID_CURR\"):\n",
    "    assert isinstance(dfs_right, list), \"dfs_right must be a list\"\n",
    "    assert isinstance(right_prefixes, list), \"right_prefixes must be a list\"\n",
    "    assert len(dfs_right) == len(right_prefixes), \"dfs_right and right_prefixes must have the same len\"\n",
    "    \n",
    "    ncols = np.sum([df.shape[1] for df in dfs_right]) + df_left.shape[1]\n",
    "    print(\"Total number of cols is at most %d\" %ncols)\n",
    "    \n",
    "    result = df_left\n",
    "    result = add_prefix_to_cols(result, left_prefix, exclude_cols=[on_col])\n",
    "    \n",
    "    for df, prefix in zip(dfs_right, right_prefixes):\n",
    "        print(\"Merging with \" + prefix)\n",
    "        df = add_prefix_to_cols(df, prefix, exclude_cols=[on_col])\n",
    "        result = result.merge(df, how=\"left\", on=on_col)\n",
    "        \n",
    "        df_cols = [col for col in result.columns if col.startswith(prefix)]\n",
    "        # add mask column to tell which rows in df are missing\n",
    "        result[prefix + \"MISSING_ROW\"] = result[df_cols[0]].isnull()\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "class MergeMissingImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cols = df_train.columns.to_list()\n",
    "        cols_num = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        cols_cat = [col for col in cols if col not in cols_num]\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in cols_num:\n",
    "            if df_train[col].isnull().sum() > 0:\n",
    "                self._impute_values[col] = df_train[col].median()\n",
    "        \n",
    "        for col in cols_cat:\n",
    "            if df_train[col].isnull().sum() > 0:\n",
    "                #print(col)\n",
    "                self._impute_values[col] = mode(df_train[col])\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            try:\n",
    "                df[col] = df[col].fillna(val)\n",
    "            except AttributeError:\n",
    "                print(\"problem with \" + col)\n",
    "                raise AttributeError()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = load_csv(\"data/data_/application_train.csv\", make_01_cat=False)\n",
    "application_test = load_csv(\"data/data_/application_test.csv\", make_01_cat=False)\n",
    "\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)\n",
    "\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.99)\n",
    "remover.fit(application_train)\n",
    "application_train = remover.transform(application_train)\n",
    "application_test = remover.transform(application_test)\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)\n",
    "\n",
    "\n",
    "application_train, application_test = train_test_col_align(application_train, application_test, \n",
    "                                                           exclude_cols=[\"TARGET\"])\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_train = load_csv(\"data/data_/bureau_agg_train.csv\", make_01_cat=False)\n",
    "bureau_test = load_csv(\"data/data_/bureau_agg_test.csv\", make_01_cat=False)\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.95)\n",
    "remover.fit(bureau_train)\n",
    "bureau_train = remover.transform(bureau_train)\n",
    "bureau_test = remover.transform(bureau_test)\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_col_align(bureau_train, bureau_test)\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_train = load_csv(\"data/data_/bureau_balance_agg_train.csv\", make_01_cat=False)\n",
    "bureau_balance_test = load_csv(\"data/data_/bureau_balance_agg_test.csv\", make_01_cat=False)\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(bureau_balance_train)\n",
    "bureau_balance_train = remover.transform(bureau_balance_train)\n",
    "bureau_balance_test = remover.transform(bureau_balance_test)\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_col_align(bureau_balance_train, bureau_balance_test)\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `previous_application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application_train = load_csv(\"data/data_/previous_application_agg_train.csv\", make_01_cat=False)\n",
    "previous_application_test = load_csv(\"data/data_/previous_application_agg_test.csv\", make_01_cat=False)\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)\n",
    "\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.95)\n",
    "remover.fit(previous_application_train)\n",
    "previous_application_train = remover.transform(previous_application_train)\n",
    "previous_application_test = remover.transform(previous_application_test)\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)\n",
    "\n",
    "\n",
    "previous_application_train, previous_application_test = train_test_col_align(previous_application_train, \n",
    "                                                                             previous_application_test)\n",
    "\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `POS_CASH_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance_train = load_csv(\"data/data_/POS_CASH_balance_agg_train.csv\", make_01_cat=False)\n",
    "POS_CASH_balance_test = load_csv(\"data/data_/POS_CASH_balance_agg_test.csv\", make_01_cat=False)\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(POS_CASH_balance_train)\n",
    "POS_CASH_balance_train = remover.transform(POS_CASH_balance_train)\n",
    "POS_CASH_balance_test = remover.transform(POS_CASH_balance_test)\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)\n",
    "\n",
    "\n",
    "POS_CASH_balance_train, POS_CASH_balance_test = train_test_col_align(POS_CASH_balance_train, POS_CASH_balance_test)\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `installments_payments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments_train = load_csv(\"data/data_/installments_payments_agg_train.csv\", make_01_cat=False)\n",
    "installments_payments_test = load_csv(\"data/data_/installments_payments_agg_test.csv\", make_01_cat=False)\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(installments_payments_train)\n",
    "installments_payments_train = remover.transform(installments_payments_train)\n",
    "installments_payments_test = remover.transform(installments_payments_test)\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)\n",
    "\n",
    "\n",
    "installments_payments_train, installments_payments_test = train_test_col_align(installments_payments_train, \n",
    "                                                                               installments_payments_test)\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `credit_card_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance_train = load_csv(\"data/data_/credit_card_balance_agg_train.csv\", make_01_cat=False)\n",
    "credit_card_balance_test = load_csv(\"data/data_/credit_card_balance_agg_test.csv\", make_01_cat=False)\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)\n",
    "\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(credit_card_balance_train)\n",
    "credit_card_balance_train = remover.transform(credit_card_balance_train)\n",
    "credit_card_balance_test = remover.transform(credit_card_balance_test)\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)\n",
    "\n",
    "\n",
    "credit_card_balance_train, credit_card_balance_test = train_test_col_align(credit_card_balance_train, \n",
    "                                                                           credit_card_balance_test)\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-step aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau_balance_agg_onestep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_onestep_train = load_csv(\"data/data_/bureau_balance_agg_onestep_train.csv\", make_01_cat=False)\n",
    "bureau_balance_onestep_test = load_csv(\"data/data_/bureau_balance_agg_onestep_test.csv\", make_01_cat=False)\n",
    "print(\"bureau_balance_onestep_train.shape\", bureau_balance_onestep_train.shape)\n",
    "print(\"bureau_balance_onestep_test.shape\", bureau_balance_onestep_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(bureau_balance_onestep_train)\n",
    "bureau_balance_onestep_train = remover.transform(bureau_balance_onestep_train)\n",
    "bureau_balance_onestep_test = remover.transform(bureau_balance_onestep_test)\n",
    "print(\"bureau_balance_onestep_train.shape\", bureau_balance_onestep_train.shape)\n",
    "print(\"bureau_balance_onestep_test.shape\", bureau_balance_onestep_test.shape)\n",
    "\n",
    "bureau_balance_onestep_train, bureau_balance_onestep_test = train_test_col_align(\n",
    "                                        bureau_balance_onestep_train, bureau_balance_onestep_test)\n",
    "print(\"bureau_balance_onestep_train.shape\", bureau_balance_onestep_train.shape)\n",
    "print(\"bureau_balance_onestep_test.shape\", bureau_balance_onestep_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `POS_CASH_balance_agg_onestep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance_onestep_train = load_csv(\"data/data_/POS_CASH_balance_agg_onestep_train.csv\", make_01_cat=False)\n",
    "POS_CASH_balance_onestep_test = load_csv(\"data/data_/POS_CASH_balance_agg_onestep_test.csv\", make_01_cat=False)\n",
    "print(\"POS_CASH_balance_onestep_train.shape\", POS_CASH_balance_onestep_train.shape)\n",
    "print(\"POS_CASH_balance_onestep_test.shape\", POS_CASH_balance_onestep_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(POS_CASH_balance_onestep_train)\n",
    "POS_CASH_balance_onestep_train = remover.transform(POS_CASH_balance_onestep_train)\n",
    "POS_CASH_balance_onestep_test = remover.transform(POS_CASH_balance_onestep_test)\n",
    "print(\"POS_CASH_balance_onestep_train.shape\", POS_CASH_balance_onestep_train.shape)\n",
    "print(\"POS_CASH_balance_onestep_test.shape\", POS_CASH_balance_onestep_test.shape)\n",
    "\n",
    "POS_CASH_balance_onestep_train, POS_CASH_balance_onestep_test = train_test_col_align(\n",
    "    POS_CASH_balance_onestep_train, POS_CASH_balance_onestep_test)\n",
    "print(\"POS_CASH_balance_onestep_train.shape\", POS_CASH_balance_onestep_train.shape)\n",
    "print(\"POS_CASH_balance_onestep_test.shape\", POS_CASH_balance_onestep_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `installments_payments_agg_onestep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments_onestep_train = load_csv(\"data/data_/installments_payments_agg_onestep_train.csv\", \n",
    "                                               make_01_cat=False)\n",
    "installments_payments_onestep_test = load_csv(\"data/data_/installments_payments_agg_onestep_test.csv\", \n",
    "                                              make_01_cat=False)\n",
    "print(\"installments_payments_onestep_train.shape\", installments_payments_onestep_train.shape)\n",
    "print(\"installments_payments_onestep_test.shape\", installments_payments_onestep_test.shape)\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(installments_payments_onestep_train)\n",
    "installments_payments_onestep_train = remover.transform(installments_payments_onestep_train)\n",
    "installments_payments_onestep_test = remover.transform(installments_payments_onestep_test)\n",
    "print(\"installments_payments_onestep_train.shape\", installments_payments_onestep_train.shape)\n",
    "print(\"installments_payments_onestep_test.shape\", installments_payments_onestep_test.shape)\n",
    "\n",
    "\n",
    "installments_payments_onestep_train, installments_payments_onestep_test = train_test_col_align(\n",
    "    installments_payments_onestep_train, installments_payments_onestep_test)\n",
    "print(\"installments_payments_onestep_train.shape\", installments_payments_onestep_train.shape)\n",
    "print(\"installments_payments_onestep_test.shape\", installments_payments_onestep_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading `credit_card_balance_agg_onestep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance_onestep_train = load_csv(\"data/data_/credit_card_balance_agg_onestep_train.csv\", \n",
    "                                             make_01_cat=False)\n",
    "credit_card_balance_onestep_test = load_csv(\"data/data_/credit_card_balance_agg_onestep_test.csv\", \n",
    "                                            make_01_cat=False)\n",
    "print(\"credit_card_balance_onestep_train.shape\", credit_card_balance_onestep_train.shape)\n",
    "print(\"credit_card_balance_onestep_test.shape\", credit_card_balance_onestep_test.shape)\n",
    "\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.90)\n",
    "remover.fit(credit_card_balance_onestep_train)\n",
    "credit_card_balance_onestep_train = remover.transform(credit_card_balance_onestep_train)\n",
    "credit_card_balance_onestep_test = remover.transform(credit_card_balance_onestep_test)\n",
    "print(\"credit_card_balance_onestep_train.shape\", credit_card_balance_onestep_train.shape)\n",
    "print(\"credit_card_balance_onestep_test.shape\", credit_card_balance_onestep_test.shape)\n",
    "\n",
    "\n",
    "credit_card_balance_onestep_train, credit_card_balance_onestep_test = train_test_col_align(\n",
    "    credit_card_balance_onestep_train, credit_card_balance_onestep_test)\n",
    "print(\"credit_card_balance_onestep_train.shape\", credit_card_balance_onestep_train.shape)\n",
    "print(\"credit_card_balance_onestep_test.shape\", credit_card_balance_onestep_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"APPL_\", \"BURE_\", \"BUBA_\", \"PRAP_\", \"POBA_\", \"INPA_\", \"CCBA_\", \"BBOS_\", \"PCOS_\", \"IPOS_\", \"CBOS_\"]\n",
    "\n",
    "dfs_train = [application_train, \n",
    "             bureau_train, \n",
    "             bureau_balance_train, \n",
    "             previous_application_train, \n",
    "             POS_CASH_balance_train,\n",
    "             installments_payments_train, \n",
    "             credit_card_balance_train,\n",
    "             bureau_balance_onestep_train, \n",
    "             POS_CASH_balance_onestep_train, \n",
    "             installments_payments_onestep_train, \n",
    "             credit_card_balance_onestep_train]\n",
    "\n",
    "dfs_test = [application_test, \n",
    "            bureau_test, \n",
    "            bureau_balance_test, \n",
    "            previous_application_test,\n",
    "            POS_CASH_balance_test,\n",
    "            installments_payments_test, \n",
    "            credit_card_balance_test, \n",
    "            bureau_balance_onestep_test, \n",
    "            POS_CASH_balance_onestep_test, \n",
    "            installments_payments_onestep_test, \n",
    "            credit_card_balance_onestep_test]\n",
    "\n",
    "\n",
    "merge_train = merge_dataframes(dfs_train[0], dfs_train[1:], prefixes[0], prefixes[1:])\n",
    "\n",
    "merge_test = merge_dataframes(dfs_test[0], dfs_test[1:], prefixes[0], prefixes[1:])\n",
    "\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "print(\"merge_train.isnull().sum().sum()\", merge_train.isnull().sum().sum())\n",
    "print(\"merge_test.isnull().sum().sum()\", merge_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "remover = ConstantColumnsRemover(exclude_cols=[\"APPL_TARGET\", \"SK_ID_CURR\"])\n",
    "remover.fit(merge_train)\n",
    "merge_train = remover.transform(merge_train)\n",
    "merge_test = remover.transform(merge_test)\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "\n",
    "imputer = MergeMissingImputer()\n",
    "imputer.fit(merge_train)\n",
    "merge_train = imputer.transform(merge_train)\n",
    "merge_test = imputer.transform(merge_test)\n",
    "\n",
    "print(\"merge_train.isnull().sum().sum()\", merge_train.isnull().sum().sum())\n",
    "print(\"merge_test.isnull().sum().sum()\", merge_test.isnull().sum().sum())\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "\n",
    "remover = SameCatColsRemover(threshold=0.95)\n",
    "remover.fit(merge_train)\n",
    "merge_train = remover.transform(merge_train)\n",
    "merge_test = remover.transform(merge_test)\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "\n",
    "remover = CollinearColumnRemover(0.95, \n",
    "                                 col_regex=\"^BURE_|^BUBA_|^PRAP_|^POBA_|^INPA_|^CCBA_|^BBOS_|^PCOS_|^IPOS_|^CBOS_\")\n",
    "remover.fit(merge_train)\n",
    "merge_train = remover.transform(merge_train)\n",
    "merge_test = remover.transform(merge_test)\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "\n",
    "merge_train, merge_test = train_test_col_align(merge_train, merge_test, exclude_cols=[\"APPL_TARGET\"])\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train.to_csv(\"data/data_/X_y_train.csv\", index=False)\n",
    "merge_test.to_csv(\"data/data_/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del application_train, application_test\n",
    "del bureau_train, bureau_test\n",
    "del bureau_balance_train, bureau_balance_test\n",
    "del previous_application_train, previous_application_test\n",
    "del POS_CASH_balance_train, POS_CASH_balance_test\n",
    "del installments_payments_train, installments_payments_test\n",
    "del credit_card_balance_train, credit_card_balance_test\n",
    "del merge_train, merge_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportantFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator, threshold):\n",
    "        self._fea_impt = estimator.feature_importances_\n",
    "        self._total_features = len(self._fea_impt)\n",
    "        \n",
    "        if isinstance(threshold, int):\n",
    "            self._n_sel_features = threshold\n",
    "        elif isinstance(threshold, float) and (0 <= threshold <= 1):\n",
    "            self._n_sel_features = int(np.ceil(self._total_features * threshold))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown value of threshold\" + str(threshold))\n",
    "    \n",
    "    def _fit_df(self, df_train):\n",
    "        features = df_train.columns.to_list()\n",
    "        assert len(features) == self._total_features\n",
    "        \n",
    "        feature_imp = pd.DataFrame({\"feature\": features, \"importance\": self._fea_impt})\n",
    "        feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "        \n",
    "        self._sel_cols = feature_imp[\"feature\"][: self._n_sel_features].values\n",
    "        self._sel_cols = list(self._sel_cols)\n",
    "        return self\n",
    "    \n",
    "    def _fit_array(self, df_train):\n",
    "        features = list(range(df_train.shape[-1]))\n",
    "        assert len(features) == self._total_features\n",
    "        \n",
    "        feature_imp = pd.DataFrame({\"feature\": features, \"importance\": self._fea_impt})\n",
    "        feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "        \n",
    "        self._sel_cols = feature_imp[\"feature\"][: self._n_sel_features].values\n",
    "        self._sel_cols = list(self._sel_cols)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        if isinstance(df_train, pd.DataFrame):\n",
    "            self._dtype = \"dataframe\"\n",
    "            self._fit_df(df_train)\n",
    "            \n",
    "        elif isinstance(df_train, np.ndarray):\n",
    "            self._dtype = \"array\"\n",
    "            self._fit_array(df_train)\n",
    "        else:\n",
    "            raise TypeError(\"Unknown df type\")\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self._dtype == \"dataframe\":\n",
    "            return df.loc[:, self._sel_cols]\n",
    "        \n",
    "        elif self._dtype == \"array\":\n",
    "            return df[:, self._sel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def feature_importance_df(estimator, features):\n",
    "    \"\"\"\n",
    "    :param estimator: an estimator object that has feature_importances_ attribute\n",
    "    :param features: list of str, list of feature names\n",
    "    :return: feature_imp, dataframe\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\"feature\": features, \"importance\": estimator.feature_importances_})\n",
    "    feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(\"data/data_/X_y_train.csv\")\n",
    "X_test = load_csv(\"data/data_/X_test.csv\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"SK_ID_CURR\", \"APPL_TARGET\"], axis=\"columns\")\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "sk_id_test.to_csv(\"data/data_/sk_id_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "# make sure that columns in train and test are aligned\n",
    "X_train, X_test = train_test_col_align(X_train, X_test)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"APPL_TARGET\"] = y_train\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "X_train.to_csv(\"data/data_/X_y_ohe_train.csv\", index=False)\n",
    "X_test.to_csv(\"data/data_/X_ohe_test.csv\", index=False)\n",
    "\n",
    "X_train = X_train.drop([\"APPL_TARGET\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(\"data/data_/X_y_ohe_train.csv\")\n",
    "X_test = load_csv(\"data/data_/X_ohe_test.csv\")\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "sk_id_test = load_csv(\"data/data_/sk_id_test.csv\")\n",
    "features = list(X_train.columns)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Standardizer(to_array=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use feature importance from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, min_samples_leaf=200, n_jobs=20, random_state=42239)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "auc_rf_train = roc_auc(rf, X_train, y_train)\n",
    "print(\"AUC of Random Forest model on the train set: %0.5f\" % auc_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance_df(rf, features)\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[\"importance_cumsum\"] = feature_importance[\"importance\"].cumsum()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "ax.plot(np.arange(feature_importance.shape[0]) + 1, feature_importance[\"importance_cumsum\"].values, lw=\"2.\")\n",
    "ax.set_xlabel(\"# of features\")\n",
    "ax.set_ylabel(\"Cumulative feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[\"importance_cumsum\"].iloc[1500:2250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ImportantFeatureSelector(rf, threshold=2250)\n",
    "selector.fit(X_train)\n",
    "X_sel_train = selector.transform(X_train)\n",
    "X_sel_test = selector.transform(X_test)\n",
    "\n",
    "print(\"X_sel_train.shape\", X_sel_train.shape)\n",
    "print(\"X_sel_test.shape\", X_sel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = [features[i] for i in selector._sel_cols]\n",
    "X_sel_train = pd.DataFrame(X_sel_train, columns=sel_features)\n",
    "X_sel_test = pd.DataFrame(X_sel_test, columns=sel_features)\n",
    "\n",
    "print(\"X_sel_train.shape\", X_sel_train.shape)\n",
    "print(\"X_sel_test.shape\", X_sel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_train[\"APPL_TARGET\"] = y_train\n",
    "\n",
    "X_sel_train.to_csv(\"data/data_/X_y_sel_rf_train.csv\", index=False)\n",
    "X_sel_test.to_csv(\"data/data_/X_sel_rf_test.csv\", index=False)\n",
    "\n",
    "del X_sel_train, X_sel_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use feature importance from XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.050, \n",
    "                    max_depth=8, min_child_weight=1, \n",
    "                    colsample_bytree=0.86, subsample=0.67,\n",
    "                    reg_lambda=450,\n",
    "                    n_jobs=20)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance_df(xgb, features)\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[\"importance_cumsum\"] = feature_importance[\"importance\"].cumsum()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "ax.plot(np.arange(feature_importance.shape[0]) + 1, feature_importance[\"importance_cumsum\"].values, lw=\"2.\")\n",
    "ax.set_xlabel(\"# of features\")\n",
    "ax.set_ylabel(\"Cumulative feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.iloc[:2455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ImportantFeatureSelector(xgb, threshold=2455)\n",
    "selector.fit(X_train)\n",
    "X_sel_train = selector.transform(X_train)\n",
    "X_sel_test = selector.transform(X_test)\n",
    "\n",
    "print(\"X_sel_train.shape\", X_sel_train.shape)\n",
    "print(\"X_sel_test.shape\", X_sel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = [features[i] for i in selector._sel_cols]\n",
    "X_sel_train = pd.DataFrame(X_sel_train, columns=sel_features)\n",
    "X_sel_test = pd.DataFrame(X_sel_test, columns=sel_features)\n",
    "\n",
    "print(\"X_sel_train.shape\", X_sel_train.shape)\n",
    "print(\"X_sel_test.shape\", X_sel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_train[\"APPL_TARGET\"] = y_train\n",
    "\n",
    "X_sel_train.to_csv(\"data/data_/X_y_sel_xgb_train.csv\", index=False)\n",
    "X_sel_test.to_csv(\"data/data_/X_sel_xgb_test.csv\", index=False)\n",
    "\n",
    "del X_sel_train, X_sel_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
