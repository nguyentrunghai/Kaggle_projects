{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == \"object\") and (df[col].nunique() < df.shape[0]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "        elif set(df[col].unique()) == set([0, 1]):\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        elif df[col].dtype == int:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtypes(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type(val):\n",
    "    if type(val) == str:\n",
    "        return \"string\"\n",
    "    \n",
    "    if np.issubsctype(type(val), np.number):\n",
    "        return \"number\"\n",
    "    \n",
    "    if callable(val):\n",
    "        return \"function\"\n",
    "    \n",
    "    return str(type(val))\n",
    "\n",
    "\n",
    "class NumColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"median\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (float)}, impute values for some specific columns\n",
    "        :param default: str, function or float, value or function used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if self._specified_values is not None:\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) == \"number\", \"Impute value for \" + col + \" is not number.\"\n",
    "        \n",
    "        self._default = default\n",
    "        self._default_type = check_type(self._default)\n",
    "        if self._default_type not in [\"number\", \"string\", \"function\"]:\n",
    "            raise ValueError(\"Unsupported stat type \" + self._default_type)\n",
    "    \n",
    "    def _cal_imput_vals(self, df):\n",
    "        cat_cols = df.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        if len(cat_cols) > 0:\n",
    "            raise ValueError(\"There are non-number columns: \" + \", \".join(cat_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        if self._default_type == \"number\":\n",
    "            impute_values = {col: self._default for col in all_cols}\n",
    "            \n",
    "        elif self._default_type == \"string\":\n",
    "            impute_values = getattr(df, self._default)()\n",
    "        \n",
    "        elif self._default_type == \"function\":\n",
    "            impute_values = df.apply(self._default)\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        impute_values = dict(impute_values)\n",
    "        if self._specified_values is None:\n",
    "            return impute_values\n",
    "        \n",
    "        for col in self._specified_values:\n",
    "            impute_values[col] = self._specified_values[col]\n",
    "            \n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        for k, v in self._impute_values.items():\n",
    "            if np.isnan(v):\n",
    "                raise ValueError(\"One of the impute_values is NaN: \" + k)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.fillna(self._impute_values)\n",
    "\n",
    "\n",
    "class CatColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"missing_value\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (str, float, function)}, \n",
    "                                 impute values for some specific columns\n",
    "        :param default: str, used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if self._specified_values is not None:\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) in [\"string\", \n",
    "                                           \"function\"], \"Impute value for \" + col + \" is \" + check_type(val)\n",
    "        \n",
    "        self._default = default\n",
    "        assert check_type(self._default) == \"string\", \"default must be string\"\n",
    "        \n",
    "        \n",
    "    def _cal_imput_vals(self, df):\n",
    "        num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "        if len(num_cols) > 0:\n",
    "            raise ValueError(\"There are number columns: \" + \", \".join(num_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        impute_values = {col: self._default for col in all_cols}\n",
    "        if self._specified_values is None:\n",
    "            return impute_values\n",
    "        \n",
    "        for col, val in self._specified_values.items():\n",
    "            dtype = check_type(val)\n",
    "            if dtype == \"string\":\n",
    "                impute_values[col] = val\n",
    "            \n",
    "            elif dtype == \"function\":\n",
    "                impute_values[col] = val(df[col])\n",
    "            \n",
    "            else:\n",
    "                return None\n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            df_new[col] = df_new[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "            \n",
    "        return df_new\n",
    "\n",
    "\n",
    "class CollinearColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold, col_regex=None):\n",
    "        \"\"\"\n",
    "        :param threshold: float in [0, 1], if two columns have correlation greater than threshold\n",
    "                          one of them will be removed\n",
    "        :param col_regex: str, regular expression to select columns\n",
    "        \"\"\"\n",
    "        self._threshold = threshold\n",
    "        self._col_regex = col_regex\n",
    "    \n",
    "    def _collinear_columns_NOTUSED(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = df_sel[[col_i]].corrwith(df_sel[col_j]).values[0]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    def _collinear_columns(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        corr_mat = df_sel.corr().abs()\n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = corr_mat.loc[col_i, col_j]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        self._collin_cols = self._collinear_columns(df, self._threshold)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        all_cols = df.columns.to_list()\n",
    "        nonexist_cols = [col for col in self._collin_cols if col not in all_cols]\n",
    "        if len(nonexist_cols) > 0:\n",
    "            print(\"WARNING: These collinear cols to be droped do not exist in df:\", nonexist_cols)\n",
    "            \n",
    "        droped_col = [col for col in self._collin_cols if col in all_cols]\n",
    "        return df.drop(droped_col, axis=\"columns\")\n",
    "    \n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, train_df):\n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        self._cat_cols_ohe = pd.get_dummies(df_cat).columns.to_list()\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop redundant classes which my be present in test_df\n",
    "        for col in df_cat.columns:\n",
    "            if col not in self._cat_cols_ohe:\n",
    "                df_cat = df_cat.drop([col], axis=\"columns\")\n",
    "        \n",
    "        # if some some colums are lacking in test but present in train, make them will all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        return pd.concat([df_num, df_cat], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colnames_from_regex(df, regex_strings):\n",
    "    cols = []\n",
    "    for regex_str in regex_strings:\n",
    "        cols.extend(df.filter(regex=regex_str).columns.to_list())\n",
    "    return cols\n",
    "\n",
    "\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "        \n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = None\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        if self._regex_strings is not None:\n",
    "            cols_imput_with_regex = get_colnames_from_regex(df_train, self._regex_strings)\n",
    "            self._spec_impt_vals_num.update({col: self._spec_impt_regex_val_num \n",
    "                                             for col in cols_imput_with_regex})\n",
    "        \n",
    "        df_num = df_train.select_dtypes([\"number\"])\n",
    "        self._imputer_num = NumColsImputer(specified_values=self._spec_impt_vals_num, \n",
    "                                           default=self._default_imput_vals_num)\n",
    "        self._imputer_num.fit(df_num)\n",
    "        \n",
    "        df_cat = df_train.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        self._imputer_cat = CatColsImputer(specified_values=self._spec_impt_vals_cat, \n",
    "                                           default=self._default_imput_vals_cat)\n",
    "        self._imputer_cat.fit(df_cat)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        num_df = df.select_dtypes([\"number\"])\n",
    "        \n",
    "        some_col = list(self._spec_impt_vals_num.keys())[0]\n",
    "        isnull_df = num_df[[some_col]]\n",
    "        for col in self._spec_impt_vals_num:\n",
    "            isnull_df[col + \"_ISNULL\"] = num_df[col].isnull()\n",
    "            \n",
    "        isnull_df = isnull_df.drop([some_col], axis=\"columns\")\n",
    "        num_df = self._imputer_num.transform(num_df)\n",
    "        \n",
    "        # cat\n",
    "        cat_df = df.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        cat_df = self._imputer_cat.transform(cat_df)\n",
    "        \n",
    "        return pd.concat([num_df, isnull_df, cat_df], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex_cols(columns):\n",
    "    fat_cols = [\"_\".join([str(c) for c in flat_col]) for flat_col in columns.to_flat_index()]\n",
    "    return fat_cols\n",
    "\n",
    "\n",
    "def agg_num_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    cat_cols = df.select_dtypes([\"object\", \"category\"]).columns.to_list()\n",
    "    if len(cat_cols) > 0:\n",
    "        raise ValueError(\"There are non-number cols: \" + \", \".join(cat_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def agg_cat_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "    if len(num_cols) > 0:\n",
    "        raise ValueError(\"There are number cols: \" + \", \".join(num_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    \n",
    "    def __init__(self, by_list_cols, \n",
    "                 num_stats, bool_stats, cat_stats,\n",
    "                 one_hot_encode_cat,\n",
    "                 iqr=False, minmax_range=False, mean_median_diff=False):\n",
    "        \n",
    "        self._by_list_cols = by_list_cols\n",
    "        \n",
    "        self._num_stats = num_stats\n",
    "        self._bool_stats = bool_stats\n",
    "        self._cat_stats = cat_stats\n",
    "        \n",
    "        self._one_hot_encode_cat = one_hot_encode_cat\n",
    "        \n",
    "        self._iqr = iqr\n",
    "        self._minmax_range = minmax_range\n",
    "        self._mean_median_diff = mean_median_diff\n",
    "    \n",
    "    def _num_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._num_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _bool_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._bool_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _cat_agg(self, df, by_sers):\n",
    "        agg_df =  agg_cat_cols(df, by_sers, stats=self._cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    \n",
    "    def _iqr_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        iqr_df = grouped.quantile(0.75) - grouped.quantile(0.25)\n",
    "        iqr_df.columns = [col + \"_iqr\" for col in iqr_df.columns]\n",
    "        return iqr_df\n",
    "    \n",
    "    def _range_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        range_df = grouped.max() - grouped.min()\n",
    "        range_df.columns = [col + \"_range\" for col in range_df.columns]\n",
    "        return range_df\n",
    "    \n",
    "    def _mm_diff_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        diff_df = grouped.mean() - grouped.median()\n",
    "        diff_df.columns = [col + \"_mm_diff\" for col in diff_df.columns]\n",
    "        return diff_df\n",
    "    \n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._ohe = OneHotEncoder()\n",
    "        self._ohe.fit(df_train)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        by_sers = [df[col] for col in self._by_list_cols]\n",
    "        df = df.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        dfs = []\n",
    "        \n",
    "        df_bool = df.select_dtypes([\"bool\"])\n",
    "        if df_bool.shape[1] > 0:\n",
    "            print(\"Aggregating bool df with shape:\", df_bool.shape)\n",
    "            df_bool = self._bool_agg(df_bool, by_sers)\n",
    "            dfs.append(df_bool)\n",
    "        \n",
    "        # one-hot encode\n",
    "        if self._one_hot_encode_cat:\n",
    "            print(\"One-hot encoding for categoricals and treating the results as numericals\")\n",
    "            df_num = self._ohe.transform(df)\n",
    "            df_num = df_num.select_dtypes([\"number\"])\n",
    "        else:\n",
    "            print(\"Do not one-hot encode categoricals\")\n",
    "            df_num = df.select_dtypes([\"number\"])\n",
    "            \n",
    "        if df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with shape:\", df_num.shape)\n",
    "            df_num = self._num_agg(df_num, by_sers)\n",
    "            dfs.append(df_num)\n",
    "        \n",
    "        df_cat = df.select_dtypes([\"category\"])\n",
    "        if df_cat.shape[1] > 0:\n",
    "            print(\"Aggregating cat df with shape:\", df_cat.shape)\n",
    "            df_cat = self._cat_agg(df_cat, by_sers)\n",
    "            dfs.append(df_cat)\n",
    "        \n",
    "        # aggregate num cols with iqr, range and mean-median difference\n",
    "        df_num = df.select_dtypes([\"number\"])\n",
    "            \n",
    "        if self._iqr:\n",
    "            print(\"Aggregating num df with iqr\")\n",
    "            df_iqr = self._iqr_agg(df_num, by_sers)\n",
    "            dfs.append(df_iqr)\n",
    "        \n",
    "        if self._minmax_range:\n",
    "            print(\"Aggregating num df with range\")\n",
    "            df_range = self._range_agg(df_num, by_sers)\n",
    "            dfs.append(df_range)\n",
    "        \n",
    "        if self._mean_median_diff:\n",
    "            print(\"Aggregating num df with mean-median difference\")\n",
    "            df_diff = self._mm_diff_agg(df_num, by_sers)\n",
    "            dfs.append(df_diff)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(df, matching_key, train_id_ser):\n",
    "    is_train = df[matching_key].isin(train_id_ser.values)\n",
    "    \n",
    "    train = df.loc[is_train, :]\n",
    "    test = df.loc[~is_train, :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(ser):\n",
    "    return ser.mode().values[0]\n",
    "\n",
    "\n",
    "def entropy(ser):\n",
    "    pk = ser.value_counts(normalize=True)\n",
    "    return stats.entropy(pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 706.62 MB\n",
      "Memory usage after changing types 341.79 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3840312, 23)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card_balance = load_csv(\"data/download/credit_card_balance.csv\")\n",
    "credit_card_balance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `application_[train|test]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = [\"^APARTMENTS_\", \"^BASEMENTAREA_\", \"^YEARS_B\", \"^COMMONAREA_\", \n",
    "                               \"^ELEVATORS_\", \"^ENTRANCES_\", \"^FLOORS\", \"^LANDAREA_\", \"^LIVING\", \n",
    "                               \"^NONLIVING\", \"AMT_REQ_CREDIT_BUREAU_\"]\n",
    "        self._spec_impt_regex_val_num = -1.\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"OWN_CAR_AGE\": -1.,\n",
    "                                    \"EXT_SOURCE_1\": 0.,\n",
    "                                    \"EXT_SOURCE_3\": 0.,\n",
    "                                    \"TOTALAREA_MODE\": -1.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "        \n",
    "\n",
    "class ApplNewColsAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df_train):\n",
    "        credit_to_income = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_INCOME_TOTAL\"]\n",
    "        self._cti_min = credit_to_income.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._cti_max = credit_to_income.replace(np.inf, np.nan).max() * 10.\n",
    "        \n",
    "        credit_to_goods = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_GOODS_PRICE\"]\n",
    "        self._ctg_min = credit_to_goods.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._ctg_max = credit_to_goods.replace(np.inf, np.nan).max() * 10.\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        df_new[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df_new[\"AMT_INCOME_TOTAL\"])\n",
    "        df_new[\"DAYS_EMPLOYED_POSITIVE\"] = df_new[\"DAYS_EMPLOYED\"] > 0\n",
    "        days_emp_max = df_new[\"DAYS_EMPLOYED\"].max()\n",
    "        if days_emp_max > 0:\n",
    "            df_new[\"DAYS_EMPLOYED\"] = df_new[\"DAYS_EMPLOYED\"].replace({days_emp_max: 100.})\n",
    "        \n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_INCOME_TOTAL\"]\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(-np.inf, self._cti_min)\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(np.inf, self._cti_max)\n",
    "        \n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_GOODS_PRICE\"]\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(-np.inf, self._ctg_min)\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(np.inf, self._ctg_max)\n",
    "        \n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 300.13 MB\n",
      "Memory usage after changing types 104.87 MB\n",
      "Memory usage before changing types 47.18 MB\n",
      "Memory usage after changing types 18.19 MB\n",
      "(307511, 122) (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "application_train = load_csv(\"data/download/application_train.csv\")\n",
    "application_test = load_csv(\"data/download/application_test.csv\")\n",
    "\n",
    "appl_train_key = application_train[\"SK_ID_CURR\"]\n",
    "appl_test_key = application_test[\"SK_ID_CURR\"]\n",
    "print(application_train.shape, application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appl_train.shape (307511, 145)\n",
      "appl_test.shape (48744, 144)\n",
      "Elapsed Time 30.59755253791809\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "appl_train = application_train.copy()\n",
    "appl_test = application_test.copy()\n",
    "\n",
    "imputer = ApplImputer()\n",
    "imputer.fit(appl_train)\n",
    "appl_train = imputer.transform(appl_train)\n",
    "appl_test = imputer.transform(appl_test)\n",
    "\n",
    "remover = CollinearColumnRemover(0.99, col_regex=\"_ISNULL$\")\n",
    "remover.fit(appl_train)\n",
    "appl_train = remover.transform(appl_train)\n",
    "appl_test = remover.transform(appl_test)\n",
    "\n",
    "adder = ApplNewColsAdder()\n",
    "adder.fit(appl_train)\n",
    "appl_train = adder.transform(appl_train)\n",
    "appl_test = adder.transform(appl_test)\n",
    "\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "if False:\n",
    "    appl_train.to_csv(\"data/data_/application_train.csv\", index=False)\n",
    "    appl_test.to_csv(\"data/data_/application_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `bureau` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BureauImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"DAYS_ENDDATE_FACT\": 100.,\n",
    "                                    \"AMT_CREDIT_MAX_OVERDUE\": -1000.,\n",
    "                                    \"AMT_CREDIT_SUM_DEBT\": 0.,\n",
    "                                    \"AMT_CREDIT_SUM_LIMIT\": 0.,\n",
    "                                    \"AMT_ANNUITY\": -1000.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "        \n",
    "def bureau_add_cols(bu_df):\n",
    "    df = bu_df.copy()\n",
    "    df[\"DAYS_CREDIT_ENDDATE_ISPOSITIVE\"] = df[\"DAYS_CREDIT_ENDDATE\"] > 0\n",
    "    df[\"DAYS_CREDIT_UPDATE_ISPOSITIVE\"] = df[\"DAYS_CREDIT_UPDATE\"] > 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 233.43 MB\n",
      "Memory usage after changing types 101.27 MB\n",
      "bureau.shape (1716428, 17)\n",
      "bureau_train.shape (1465325, 17)\n",
      "bureau_test.shape (251103, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>CREDIT_ACTIVE</th>\n",
       "      <th>CREDIT_CURRENCY</th>\n",
       "      <th>DAYS_CREDIT</th>\n",
       "      <th>CREDIT_DAY_OVERDUE</th>\n",
       "      <th>DAYS_CREDIT_ENDDATE</th>\n",
       "      <th>DAYS_ENDDATE_FACT</th>\n",
       "      <th>AMT_CREDIT_MAX_OVERDUE</th>\n",
       "      <th>CNT_CREDIT_PROLONG</th>\n",
       "      <th>AMT_CREDIT_SUM</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT</th>\n",
       "      <th>AMT_CREDIT_SUM_LIMIT</th>\n",
       "      <th>AMT_CREDIT_SUM_OVERDUE</th>\n",
       "      <th>CREDIT_TYPE</th>\n",
       "      <th>DAYS_CREDIT_UPDATE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714462</td>\n",
       "      <td>Closed</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-497</td>\n",
       "      <td>0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>91323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714463</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-208</td>\n",
       "      <td>0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>171342.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714464</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-203</td>\n",
       "      <td>0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>464323.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714465</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-203</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215354</td>\n",
       "      <td>5714466</td>\n",
       "      <td>Active</td>\n",
       "      <td>currency 1</td>\n",
       "      <td>-629</td>\n",
       "      <td>0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77674.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2700000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer credit</td>\n",
       "      <td>-21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY  DAYS_CREDIT  \\\n",
       "0      215354       5714462        Closed      currency 1         -497   \n",
       "1      215354       5714463        Active      currency 1         -208   \n",
       "2      215354       5714464        Active      currency 1         -203   \n",
       "3      215354       5714465        Active      currency 1         -203   \n",
       "4      215354       5714466        Active      currency 1         -629   \n",
       "\n",
       "   CREDIT_DAY_OVERDUE  DAYS_CREDIT_ENDDATE  DAYS_ENDDATE_FACT  \\\n",
       "0                   0               -153.0             -153.0   \n",
       "1                   0               1075.0                NaN   \n",
       "2                   0                528.0                NaN   \n",
       "3                   0                  NaN                NaN   \n",
       "4                   0               1197.0                NaN   \n",
       "\n",
       "   AMT_CREDIT_MAX_OVERDUE  CNT_CREDIT_PROLONG  AMT_CREDIT_SUM  \\\n",
       "0                     NaN                   0         91323.0   \n",
       "1                     NaN                   0        225000.0   \n",
       "2                     NaN                   0        464323.5   \n",
       "3                     NaN                   0         90000.0   \n",
       "4                 77674.5                   0       2700000.0   \n",
       "\n",
       "   AMT_CREDIT_SUM_DEBT  AMT_CREDIT_SUM_LIMIT  AMT_CREDIT_SUM_OVERDUE  \\\n",
       "0                  0.0                   NaN                     0.0   \n",
       "1             171342.0                   NaN                     0.0   \n",
       "2                  NaN                   NaN                     0.0   \n",
       "3                  NaN                   NaN                     0.0   \n",
       "4                  NaN                   NaN                     0.0   \n",
       "\n",
       "       CREDIT_TYPE  DAYS_CREDIT_UPDATE  AMT_ANNUITY  \n",
       "0  Consumer credit                -131          NaN  \n",
       "1      Credit card                 -20          NaN  \n",
       "2  Consumer credit                 -16          NaN  \n",
       "3      Credit card                 -16          NaN  \n",
       "4  Consumer credit                 -21          NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau = load_csv(\"data/download/bureau.csv\")\n",
    "print(\"bureau.shape\", bureau.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_partition(bureau, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"bureau_train.shape\", bureau_train.shape)\n",
    "print(\"bureau_test.shape\", bureau_test.shape)\n",
    "\n",
    "bureau_train_keys = bureau_train[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "bureau_test_keys = bureau_test[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "\n",
    "bureau_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau_agg_train.shape (1465325, 24)\n",
      "bureau_agg_test.shape (251103, 24)\n",
      "Aggregating bool df with shape: (1465325, 7)\n",
      "Aggregating num df with shape: (1465325, 35)\n",
      "Aggregating cat df with shape: (1465325, 3)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "Aggregating bool df with shape: (251103, 7)\n",
      "Aggregating num df with shape: (251103, 35)\n",
      "Aggregating cat df with shape: (251103, 3)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "bureau_agg_train.shape (263491, 363)\n",
      "bureau_agg_test.shape (42320, 363)\n",
      "bureau_agg_train.shape (263491, 293)\n",
      "bureau_agg_test.shape (42320, 293)\n",
      "Elapsed Time 3633.656004190445\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "bureau_agg_train = bureau_train.copy()\n",
    "bureau_agg_test = bureau_test.copy()\n",
    "\n",
    "imputer = BureauImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "\n",
    "bureau_agg_train = bureau_add_cols(bureau_agg_train)\n",
    "bureau_agg_test = bureau_add_cols(bureau_agg_test)\n",
    "\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "num_stats = [\"count\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"count\", \"mean\", \"var\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "bureau_agg_train = bureau_agg_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_agg_test = bureau_agg_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        one_hot_encode_cat=True,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "aggregator.fit(bureau_agg_train)\n",
    "bureau_agg_train = aggregator.transform(bureau_agg_train)\n",
    "bureau_agg_test = aggregator.transform(bureau_agg_test)\n",
    "\n",
    "bureau_agg_train = bureau_agg_train.reset_index()\n",
    "bureau_agg_test = bureau_agg_test.reset_index()\n",
    "\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "bureau_agg_train = bureau_agg_train.fillna(0.)\n",
    "bureau_agg_test = bureau_agg_test.fillna(0.)\n",
    "\n",
    "\n",
    "remover = CollinearColumnRemover(0.99)\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    bureau_agg_train.to_csv(\"data/data_/bureau_agg_train.csv\", index=False)\n",
    "    bureau_agg_test.to_csv(\"data/data_/bureau_agg_test.csv\", index=False)\n",
    "\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bureau_agg_train, bureau_agg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_status(df):\n",
    "    return df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[0]\n",
    "\n",
    "def mode_status_five_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 5]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def mode_status_ten_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 10]\n",
    "    return statuses.mode().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 655.20 MB\n",
      "Memory usage after changing types 245.70 MB\n",
      "bureau_balance.shape (27299925, 3)\n",
      "bureau_balance.shape (24179741, 4)\n",
      "bureau_balance_train.shape: (14701612, 3)\n",
      "bureau_balance_test.shape: (9478129, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5715448</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_BUREAU  MONTHS_BALANCE STATUS\n",
       "0       5715448               0      C\n",
       "1       5715448              -1      C\n",
       "2       5715448              -2      C\n",
       "3       5715448              -3      C\n",
       "4       5715448              -4      C"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance = load_csv(\"data/download/bureau_balance.csv\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance = bureau_balance.merge(bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]], how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance = bureau_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "bureau_balance[\"SK_ID_CURR\"] = bureau_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_partition(bureau_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "bureau_balance_train = bureau_balance_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "bureau_balance_test = bureau_balance_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"bureau_balance_train.shape:\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape:\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bureau_balance_agg_train.shape (14701612, 3)\n",
      "bureau_balance_agg_test.shape (9478129, 3)\n",
      "Do not one-hot encode categoricals\n",
      "Aggregating num df with shape: (14701612, 1)\n",
      "Aggregating cat df with shape: (14701612, 1)\n",
      "Do not one-hot encode categoricals\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "Do not one-hot encode categoricals\n",
      "Aggregating num df with shape: (9478129, 1)\n",
      "Aggregating cat df with shape: (9478129, 1)\n",
      "Do not one-hot encode categoricals\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (523515, 10)\n",
      "bureau_balance_agg_test.shape (250839, 10)\n",
      "bureau_balance_agg_train.shape (523515, 13)\n",
      "bureau_balance_agg_test.shape (250839, 13)\n",
      "bureau_balance_agg_train.shape (523515, 10)\n",
      "bureau_balance_agg_test.shape (250839, 10)\n",
      "bureau_balance_agg_train.shape (523515, 11)\n",
      "bureau_balance_agg_test.shape (250839, 11)\n",
      "Elapsed Time 2888.5385682582855\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "NEAREST_STATUS_train = bureau_balance_agg_train.groupby(\"SK_ID_BUREAU\").apply(nearest_status)\n",
    "MODE_STATUS_FIVE_NEAREST_train = bureau_balance_agg_train.groupby(\"SK_ID_BUREAU\").apply(mode_status_five_nearest)\n",
    "MODE_STATUS_TEN_NEAREST_train = bureau_balance_agg_train.groupby(\"SK_ID_BUREAU\").apply(mode_status_ten_nearest)\n",
    "\n",
    "NEAREST_STATUS_test = bureau_balance_agg_test.groupby(\"SK_ID_BUREAU\").apply(nearest_status)\n",
    "MODE_STATUS_FIVE_NEAREST_test = bureau_balance_agg_test.groupby(\"SK_ID_BUREAU\").apply(mode_status_five_nearest)\n",
    "MODE_STATUS_TEN_NEAREST_test = bureau_balance_agg_test.groupby(\"SK_ID_BUREAU\").apply(mode_status_ten_nearest)\n",
    "\n",
    "num_stats = [\"count\", \"mean\", \"min\"]\n",
    "bool_stats = []\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "\n",
    "# aggregate over \"SK_ID_BUREAU\"\n",
    "by_list_cols = [\"SK_ID_BUREAU\"]\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        one_hot_encode_cat=False,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "bureau_balance_agg_train[\"NEAREST_STATUS\"] = NEAREST_STATUS_train\n",
    "bureau_balance_agg_train[\"MODE_STATUS_FIVE_NEAREST\"] = MODE_STATUS_FIVE_NEAREST_train\n",
    "bureau_balance_agg_train[\"MODE_STATUS_TEN_NEAREST\"] = MODE_STATUS_TEN_NEAREST_train\n",
    "\n",
    "bureau_balance_agg_test[\"NEAREST_STATUS\"] = NEAREST_STATUS_test\n",
    "bureau_balance_agg_test[\"MODE_STATUS_FIVE_NEAREST\"] = MODE_STATUS_FIVE_NEAREST_test\n",
    "bureau_balance_agg_test[\"MODE_STATUS_TEN_NEAREST\"] = MODE_STATUS_TEN_NEAREST_test\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.99)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if True:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 180.09 MB\n",
      "Memory usage after changing types 63.35 MB\n",
      "Memory usage before changing types 86.29 MB\n",
      "Memory usage after changing types 30.35 MB\n",
      "bureau_balance_agg_train.shape (523515, 43)\n",
      "bureau_balance_agg_test.shape (250839, 43)\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 0\n",
      "bureau_balance_agg_train.shape (523515, 44)\n",
      "bureau_balance_agg_test.shape (250839, 44)\n",
      "bureau_balance_agg_train.shape (523515, 43)\n",
      "bureau_balance_agg_test.shape (250839, 43)\n",
      "Aggregating bool df with shape: (523515, 13)\n",
      "Aggregating num df with shape: (523515, 57)\n",
      "Aggregating cat df with shape: (523515, 4)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "Aggregating bool df with shape: (250839, 13)\n",
      "Aggregating num df with shape: (250839, 57)\n",
      "Aggregating cat df with shape: (250839, 4)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "bureau_balance_agg_train.isnull().sum().sum(): 853020\n",
      "bureau_balance_agg_test.isnull().sum().sum(): 381780\n",
      "bureau_balance_agg_train.shape (92231, 594)\n",
      "bureau_balance_agg_test.shape (42311, 594)\n",
      "bureau_balance_agg_train.shape (92231, 486)\n",
      "bureau_balance_agg_test.shape (42311, 486)\n",
      "bureau_balance_agg_train.shape (92231, 487)\n",
      "bureau_balance_agg_test.shape (42311, 487)\n",
      "Elapsed Time 2785.011474132538\n"
     ]
    }
   ],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "# this will change [0, 1] into bool, and it is ok\n",
    "bureau_balance_agg_train = load_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\")\n",
    "bureau_balance_agg_test = load_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.merge(bureau_train_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.merge(bureau_test_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"count\", \"mean\", \"median\", \"min\"]\n",
    "bool_stats = [\"count\", \"mean\", \"var\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        one_hot_encode_cat=True,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.fillna(0.)\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.fillna(0.)\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.99)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bureau_balance_agg_train, bureau_balance_agg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `previous application` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `previous_application.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"RATE_DOWN_PAYMENT\": -1.,\n",
    "                                   \"CNT_PAYMENT\": -10.,\n",
    "                                   \"DAYS_FIRST_DRAWING\": 0., \n",
    "                                   \"DAYS_FIRST_DUE\": 0.,\n",
    "                                   \"DAYS_LAST_DUE_1ST_VERSION\": 0.,\n",
    "                                   \"DAYS_LAST_DUE\": 0.,\n",
    "                                   \"DAYS_TERMINATION\": 0.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {\"NAME_TYPE_SUITE\": \"missing_value\",\n",
    "                                    \"NFLAG_INSURED_ON_APPROVAL\": \"missing_value\"}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "\n",
    "def hour_period_bin(hours):\n",
    "    hours = hours.values\n",
    "    hour_bin = np.array([\"evening\"] * len(hours), dtype=\"object\")\n",
    "    morning_mask = (hours > 5) & (hours < 12)\n",
    "    afternoon_mask = (hours >= 12) & (hours < 18)\n",
    "    \n",
    "    hour_bin[morning_mask] = \"morning\"\n",
    "    hour_bin[afternoon_mask] = \"afternoon\"\n",
    "    return hour_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 494.38 MB\n",
      "Memory usage after changing types 162.02 MB\n",
      "previous_application.shape (1670214, 37)\n",
      "prev_appl_train.shape (1413701, 37)\n",
      "prev_appl_test.shape (256513, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_APPLICATION</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_DOWN_PAYMENT</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>...</th>\n",
       "      <th>NAME_SELLER_INDUSTRY</th>\n",
       "      <th>CNT_PAYMENT</th>\n",
       "      <th>NAME_YIELD_GROUP</th>\n",
       "      <th>PRODUCT_COMBINATION</th>\n",
       "      <th>DAYS_FIRST_DRAWING</th>\n",
       "      <th>DAYS_FIRST_DUE</th>\n",
       "      <th>DAYS_LAST_DUE_1ST_VERSION</th>\n",
       "      <th>DAYS_LAST_DUE</th>\n",
       "      <th>DAYS_TERMINATION</th>\n",
       "      <th>NFLAG_INSURED_ON_APPROVAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030495</td>\n",
       "      <td>271877</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>1730.430054</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>Connectivity</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>POS mobile with interest</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2802425</td>\n",
       "      <td>108129</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>25188.615234</td>\n",
       "      <td>607500.0</td>\n",
       "      <td>679671.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>607500.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>36.0</td>\n",
       "      <td>low_action</td>\n",
       "      <td>Cash X-Sell: low</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>916.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2523466</td>\n",
       "      <td>122040</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>15060.735352</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>136444.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>high</td>\n",
       "      <td>Cash X-Sell: high</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-271.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2819243</td>\n",
       "      <td>176158</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>47041.335938</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>470790.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>Cash X-Sell: middle</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-482.0</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>-182.0</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1784265</td>\n",
       "      <td>202054</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>31924.394531</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>404055.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>24.0</td>\n",
       "      <td>high</td>\n",
       "      <td>Cash Street: high</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV  SK_ID_CURR NAME_CONTRACT_TYPE   AMT_ANNUITY  AMT_APPLICATION  \\\n",
       "0     2030495      271877     Consumer loans   1730.430054          17145.0   \n",
       "1     2802425      108129         Cash loans  25188.615234         607500.0   \n",
       "2     2523466      122040         Cash loans  15060.735352         112500.0   \n",
       "3     2819243      176158         Cash loans  47041.335938         450000.0   \n",
       "4     1784265      202054         Cash loans  31924.394531         337500.0   \n",
       "\n",
       "   AMT_CREDIT  AMT_DOWN_PAYMENT  AMT_GOODS_PRICE WEEKDAY_APPR_PROCESS_START  \\\n",
       "0     17145.0               0.0          17145.0                   SATURDAY   \n",
       "1    679671.0               NaN         607500.0                   THURSDAY   \n",
       "2    136444.5               NaN         112500.0                    TUESDAY   \n",
       "3    470790.0               NaN         450000.0                     MONDAY   \n",
       "4    404055.0               NaN         337500.0                   THURSDAY   \n",
       "\n",
       "   HOUR_APPR_PROCESS_START  ... NAME_SELLER_INDUSTRY  CNT_PAYMENT  \\\n",
       "0                       15  ...         Connectivity         12.0   \n",
       "1                       11  ...                  XNA         36.0   \n",
       "2                       11  ...                  XNA         12.0   \n",
       "3                        7  ...                  XNA         12.0   \n",
       "4                        9  ...                  XNA         24.0   \n",
       "\n",
       "   NAME_YIELD_GROUP       PRODUCT_COMBINATION  DAYS_FIRST_DRAWING  \\\n",
       "0            middle  POS mobile with interest            365243.0   \n",
       "1        low_action          Cash X-Sell: low            365243.0   \n",
       "2              high         Cash X-Sell: high            365243.0   \n",
       "3            middle       Cash X-Sell: middle            365243.0   \n",
       "4              high         Cash Street: high                 NaN   \n",
       "\n",
       "  DAYS_FIRST_DUE DAYS_LAST_DUE_1ST_VERSION  DAYS_LAST_DUE DAYS_TERMINATION  \\\n",
       "0          -42.0                     300.0          -42.0            -37.0   \n",
       "1         -134.0                     916.0       365243.0         365243.0   \n",
       "2         -271.0                      59.0       365243.0         365243.0   \n",
       "3         -482.0                    -152.0         -182.0           -177.0   \n",
       "4            NaN                       NaN            NaN              NaN   \n",
       "\n",
       "  NFLAG_INSURED_ON_APPROVAL  \n",
       "0                       0.0  \n",
       "1                       1.0  \n",
       "2                       1.0  \n",
       "3                       1.0  \n",
       "4                       NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_application = load_csv(\"data/download/previous_application.csv\")\n",
    "print(\"previous_application.shape\", previous_application.shape)\n",
    "\n",
    "prev_appl_train, prev_appl_test = train_test_partition(previous_application, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_train_keys = prev_appl_train[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "prev_appl_test_keys = prev_appl_test[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "\n",
    "prev_appl_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_appl_train.shape (1413701, 37)\n",
      "prev_appl_test.shape (256513, 37)\n",
      "prev_appl_train.shape (1413701, 36)\n",
      "prev_appl_test.shape (256513, 36)\n",
      "prev_appl_train.shape (1413701, 34)\n",
      "prev_appl_test.shape (256513, 34)\n",
      "prev_appl_train.isnull().sum().sum() 0\n",
      "prev_appl_test.isnull().sum().sum() 0\n",
      "prev_appl_train.shape (1413701, 41)\n",
      "prev_appl_test.shape (256513, 41)\n",
      "prev_appl_train.shape (1413701, 46)\n",
      "prev_appl_test.shape (256513, 46)\n",
      "prev_appl_train.shape (1413701, 47)\n",
      "prev_appl_test.shape (256513, 47)\n",
      "Aggregating bool df with shape: (1413701, 13)\n",
      "Aggregating num df with shape: (1413701, 164)\n",
      "Aggregating cat df with shape: (1413701, 17)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "Aggregating bool df with shape: (256513, 13)\n",
      "Aggregating num df with shape: (256513, 164)\n",
      "Aggregating cat df with shape: (256513, 17)\n",
      "Aggregating num df with iqr\n",
      "Aggregating num df with range\n",
      "Aggregating num df with mean-median difference\n",
      "prev_appl_train.isnull().sum().sum() 8615412\n",
      "prev_appl_test.isnull().sum().sum() 1299700\n",
      "prev_appl_train.shape (291057, 1596)\n",
      "prev_appl_test.shape (47800, 1596)\n",
      "Use Version 2\n",
      "prev_appl_train.shape (291057, 1321)\n",
      "prev_appl_test.shape (47800, 1321)\n",
      "Elapsed Time 31861.041574954987\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "prev_appl_train, prev_appl_test = train_test_partition(previous_application, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_train_keys = prev_appl_train[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "prev_appl_test_keys = prev_appl_test[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "\n",
    "prev_appl_train = prev_appl_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "prev_appl_test = prev_appl_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "# drop cols with high percentage of null\n",
    "cols_to_drop = [\"RATE_INTEREST_PRIMARY\", \"RATE_INTEREST_PRIVILEGED\"]\n",
    "prev_appl_train = prev_appl_train.drop(cols_to_drop, axis=\"columns\")\n",
    "prev_appl_test = prev_appl_test.drop(cols_to_drop, axis=\"columns\")\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = PrevApplImputer()\n",
    "imputer.fit(prev_appl_train)\n",
    "prev_appl_train = imputer.transform(prev_appl_train)\n",
    "prev_appl_test = imputer.transform(prev_appl_test)\n",
    "\n",
    "print(\"prev_appl_train.isnull().sum().sum()\", prev_appl_train.isnull().sum().sum())\n",
    "print(\"prev_appl_test.isnull().sum().sum()\", prev_appl_test.isnull().sum().sum())\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "# add to bool cols to identify if values are non-negative\n",
    "cols_is_nonneg = [\"DAYS_FIRST_DRAWING\", \"DAYS_FIRST_DUE\", \"DAYS_LAST_DUE_1ST_VERSION\", \n",
    "                    \"DAYS_LAST_DUE\", \"DAYS_TERMINATION\"]\n",
    "for col in cols_is_nonneg:\n",
    "    prev_appl_train[col + \"_IS_NONNEG\"] = prev_appl_train[col] >= 0\n",
    "    prev_appl_test[col + \"_IS_NONNEG\"] = prev_appl_test[col] >= 0\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "prev_appl_train[\"PERIOD_APPR_PROCESS_START\"] = hour_period_bin(prev_appl_train[\"HOUR_APPR_PROCESS_START\"])\n",
    "prev_appl_test[\"PERIOD_APPR_PROCESS_START\"] = hour_period_bin(prev_appl_test[\"HOUR_APPR_PROCESS_START\"])\n",
    "prev_appl_train[\"PERIOD_APPR_PROCESS_START\"] = prev_appl_train[\"PERIOD_APPR_PROCESS_START\"].astype(\"category\")\n",
    "prev_appl_test[\"PERIOD_APPR_PROCESS_START\"] = prev_appl_test[\"PERIOD_APPR_PROCESS_START\"].astype(\"category\")\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_CURR\"\n",
    "num_stats = [\"count\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"count\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        one_hot_encode_cat=True,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "aggregator.fit(prev_appl_train)\n",
    "prev_appl_train = aggregator.transform(prev_appl_train)\n",
    "prev_appl_test = aggregator.transform(prev_appl_test)\n",
    "\n",
    "print(\"prev_appl_train.isnull().sum().sum()\", prev_appl_train.isnull().sum().sum())\n",
    "print(\"prev_appl_test.isnull().sum().sum()\", prev_appl_test.isnull().sum().sum())\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "# aggregating by var sometimes gives nan\n",
    "prev_appl_train = prev_appl_train.fillna(0.)\n",
    "prev_appl_test = prev_appl_test.fillna(0.)\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.99)\n",
    "remover.fit(prev_appl_train)\n",
    "prev_appl_train = remover.transform(prev_appl_train)\n",
    "prev_appl_test = remover.transform(prev_appl_test)\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "prev_appl_train = prev_appl_train.reset_index()\n",
    "prev_appl_test = prev_appl_test.reset_index()\n",
    "\n",
    "if False:\n",
    "    prev_appl_train.to_csv(\"data/data_/previous_application_agg_train.csv\", index=False)\n",
    "    prev_appl_test.to_csv(\"data/data_/previous_application_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "del prev_appl_train, prev_appl_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `POS_CASH_balance.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `installments_payments.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
