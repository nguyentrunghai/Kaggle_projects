{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == \"object\") and (df[col].nunique() < df.shape[0]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        elif df[col].dtype == int:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtypes(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type(val):\n",
    "    if type(val) == str:\n",
    "        return \"string\"\n",
    "    \n",
    "    if np.issubsctype(type(val), np.number):\n",
    "        return \"number\"\n",
    "    \n",
    "    if callable(val):\n",
    "        return \"function\"\n",
    "    \n",
    "    return str(type(val))\n",
    "\n",
    "\n",
    "class NumColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"median\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (float)}, impute values for some specific columns\n",
    "        :param default: str, function or float, value or function used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if (self._specified_values is not None) and (len(self._specified_values) > 0):\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) == \"number\", \"Impute value for \" + col + \" is not number.\"\n",
    "        \n",
    "        self._default = default\n",
    "        self._default_type = check_type(self._default)\n",
    "        if self._default_type not in [\"number\", \"string\", \"function\"]:\n",
    "            raise ValueError(\"Unsupported stat type \" + self._default_type)\n",
    "    \n",
    "    def _cal_imput_vals(self, df):\n",
    "        cat_cols = df.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        if len(cat_cols) > 0:\n",
    "            raise ValueError(\"There are non-number columns: \" + \", \".join(cat_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        if self._default_type == \"number\":\n",
    "            impute_values = {col: self._default for col in all_cols}\n",
    "            \n",
    "        elif self._default_type == \"string\":\n",
    "            impute_values = getattr(df, self._default)()\n",
    "        \n",
    "        elif self._default_type == \"function\":\n",
    "            impute_values = df.apply(self._default)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown default imputer:\", self._default)\n",
    "        \n",
    "        # if it is a pandas series, turn it into dict\n",
    "        impute_values = dict(impute_values)\n",
    "        if (self._specified_values is None) or (len(self._specified_values) == 0):\n",
    "            return impute_values\n",
    "        \n",
    "        for col in self._specified_values:\n",
    "            impute_values[col] = self._specified_values[col]\n",
    "            \n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        for k, v in self._impute_values.items():\n",
    "            if np.isnan(v):\n",
    "                raise ValueError(\"One of the impute_values is NaN: \" + k)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.fillna(self._impute_values)\n",
    "\n",
    "\n",
    "class CatColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"missing_value\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (str, float, function)}, \n",
    "                                 impute values for some specific columns\n",
    "        :param default: str, used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if (self._specified_values is not None) and (len(self._specified_values) > 0):\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) in [\"string\", \n",
    "                                           \"function\"], \"Impute value for \" + col + \" is \" + check_type(val)\n",
    "        \n",
    "        self._default = default\n",
    "        assert check_type(self._default) == \"string\", \"default must be string\"\n",
    "        \n",
    "        \n",
    "    def _cal_imput_vals(self, df):\n",
    "        num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "        if len(num_cols) > 0:\n",
    "            raise ValueError(\"There are number columns: \" + \", \".join(num_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        impute_values = {col: self._default for col in all_cols}\n",
    "        if (self._specified_values is None) or (len(self._specified_values) == 0):\n",
    "            return impute_values\n",
    "        \n",
    "        for col, val in self._specified_values.items():\n",
    "            dtype = check_type(val)\n",
    "            if dtype == \"string\":\n",
    "                impute_values[col] = val\n",
    "            \n",
    "            elif dtype == \"function\":\n",
    "                impute_values[col] = val(df[col])\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Unknown imputer for \" + col + \": \", str(val))\n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            df_new[col] = df_new[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "            \n",
    "        return df_new\n",
    "\n",
    "\n",
    "def get_colnames_from_regex(df, regex_strings):\n",
    "    cols = []\n",
    "    for regex_str in regex_strings:\n",
    "        cols.extend(df.filter(regex=regex_str).columns.to_list())\n",
    "    return cols\n",
    "\n",
    "\n",
    "class Imputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "        \n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        if self._regex_strings is not None:\n",
    "            cols_imput_with_regex = get_colnames_from_regex(df_train, self._regex_strings)\n",
    "            self._spec_impt_vals_num.update({col: self._spec_impt_regex_val_num \n",
    "                                             for col in cols_imput_with_regex})\n",
    "        \n",
    "        df_num = df_train.select_dtypes([\"number\"])\n",
    "        self._imputer_num = NumColsImputer(specified_values=self._spec_impt_vals_num, \n",
    "                                           default=self._default_imput_vals_num)\n",
    "        self._imputer_num.fit(df_num)\n",
    "        \n",
    "        df_cat = df_train.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        self._imputer_cat = CatColsImputer(specified_values=self._spec_impt_vals_cat, \n",
    "                                           default=self._default_imput_vals_cat)\n",
    "        self._imputer_cat.fit(df_cat)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        dfs = []\n",
    "        num_df = df.select_dtypes([\"number\"])\n",
    "        \n",
    "        if len(self._spec_impt_vals_num) > 0:\n",
    "            some_col = list(self._spec_impt_vals_num.keys())[0]\n",
    "            isnull_df = num_df[[some_col]]\n",
    "            for col in self._spec_impt_vals_num:\n",
    "                isnull_df[col + \"_ISNULL\"] = num_df[col].isnull()\n",
    "                \n",
    "            isnull_df = isnull_df.drop([some_col], axis=\"columns\")\n",
    "            dfs.append(isnull_df)\n",
    "        \n",
    "        num_df = self._imputer_num.transform(num_df)\n",
    "        dfs.append(num_df)\n",
    "        \n",
    "        # cat\n",
    "        cat_df = df.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        cat_df = self._imputer_cat.transform(cat_df)\n",
    "        dfs.append(cat_df)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")\n",
    "    \n",
    "\n",
    "class DefaultImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {}\n",
    "        self._default_imput_vals_num = 0.\n",
    "        \n",
    "        self._spec_impt_vals_cat = {}\n",
    "        self._default_imput_vals_cat = \"missing_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollinearColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold, col_regex=None):\n",
    "        \"\"\"\n",
    "        :param threshold: float in [0, 1], if two columns have correlation greater than threshold\n",
    "                          one of them will be removed\n",
    "        :param col_regex: str, regular expression to select columns\n",
    "        \"\"\"\n",
    "        self._threshold = threshold\n",
    "        self._col_regex = col_regex\n",
    "    \n",
    "    def _collinear_columns(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "            df_sel = df_sel.select_dtypes([\"number\", \"bool\"])\n",
    "        \n",
    "        df_sel = df_sel.astype(\"float32\")\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        corr_mat = df_sel.corr().abs()\n",
    "        self._corr_mat = corr_mat\n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = corr_mat.loc[col_i, col_j]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        self._collin_cols = self._collinear_columns(df, self._threshold)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        all_cols = df.columns.to_list()\n",
    "        nonexist_cols = [col for col in self._collin_cols if col not in all_cols]\n",
    "        if len(nonexist_cols) > 0:\n",
    "            print(\"WARNING: These collinear cols to be droped do not exist in df:\", nonexist_cols)\n",
    "            \n",
    "        droped_col = [col for col in self._collin_cols if col in all_cols]\n",
    "        print(\"Number of columns droped:\", len(droped_col))\n",
    "        return df.drop(droped_col, axis=\"columns\")\n",
    "\n",
    "    \n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, train_df):\n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df\n",
    "        \n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        for col in df_cat.columns:\n",
    "            if col not in self._cat_cols_ohe:\n",
    "                df_cat = df_cat.drop([col], axis=\"columns\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them will all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        return pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex_cols(columns):\n",
    "    fat_cols = [\"_\".join([str(c) for c in flat_col]) for flat_col in columns.to_flat_index()]\n",
    "    return fat_cols\n",
    "\n",
    "\n",
    "def agg_num_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    cat_cols = df.select_dtypes([\"object\", \"category\"]).columns.to_list()\n",
    "    if len(cat_cols) > 0:\n",
    "        raise ValueError(\"There are non-number cols: \" + \", \".join(cat_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def agg_cat_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "    if len(num_cols) > 0:\n",
    "        raise ValueError(\"There are number cols: \" + \", \".join(num_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "class Aggregator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, by_list_cols, \n",
    "                 num_stats, bool_stats, cat_stats, \n",
    "                 ohe_cat_stats=None,\n",
    "                 ohe_cat_max_class=None,\n",
    "                 iqr=False, minmax_range=False, mean_median_diff=False):\n",
    "        \"\"\"\n",
    "        :param by_list_cols: list of str, cols by which the dataframe is grouped\n",
    "        :param num_stats: list, aggregating functions for numerical columns\n",
    "        :param bool_stats: list, aggregating functions for bool columns\n",
    "        :param cat_stats: list, aggregating functions for category columns\n",
    "        :param ohe_cat_stats: list, aggregating functions for category columns after one-hot encoded\n",
    "        :param ohe_cat_max_class: int, category columns with at most ohe_cat_max_class classes \n",
    "                                 will be one-hot encoded before aggregating, \n",
    "                                 If None, no one-hot encoding will be done.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._by_list_cols = by_list_cols\n",
    "        \n",
    "        self._num_stats = num_stats\n",
    "        self._bool_stats = bool_stats\n",
    "        self._cat_stats = cat_stats\n",
    "        self._ohe_cat_stats = ohe_cat_stats\n",
    "        \n",
    "        if ohe_cat_max_class is None:\n",
    "            self._ohe_cat_max_class = [\"mean\", \"sum\"]\n",
    "        else:\n",
    "            self._ohe_cat_max_class = ohe_cat_max_class\n",
    "        \n",
    "        self._iqr = iqr\n",
    "        self._minmax_range = minmax_range\n",
    "        self._mean_median_diff = mean_median_diff\n",
    "    \n",
    "    def _num_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._num_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _bool_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._bool_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _cat_agg(self, df, by_sers):\n",
    "        agg_df =  agg_cat_cols(df, by_sers, stats=self._cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    def _ohe_cat_agg(self, df, by_sers):\n",
    "        agg_df = agg_num_cols(df, by_sers, stats=self._ohe_cat_stats)\n",
    "        return agg_df\n",
    "    \n",
    "    \n",
    "    def _iqr_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        iqr_df = grouped.quantile(0.75) - grouped.quantile(0.25)\n",
    "        iqr_df.columns = [col + \"_iqr\" for col in iqr_df.columns]\n",
    "        return iqr_df\n",
    "    \n",
    "    def _range_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        range_df = grouped.max() - grouped.min()\n",
    "        range_df.columns = [col + \"_range\" for col in range_df.columns]\n",
    "        return range_df\n",
    "    \n",
    "    def _mm_diff_agg(self, num_df, by_sers):\n",
    "        grouped = num_df.groupby(by_sers)\n",
    "        diff_df = grouped.mean() - grouped.median()\n",
    "        diff_df.columns = [col + \"_mm_diff\" for col in diff_df.columns]\n",
    "        return diff_df\n",
    "    \n",
    "    def _cat_cols_to_ohe(self, df_train):\n",
    "        cat_cols = []\n",
    "        if self._ohe_cat_max_class is None:\n",
    "            return cat_cols\n",
    "        \n",
    "        for col in self._cat_cols:\n",
    "            if df_train[col].nunique() <=  self._ohe_cat_max_class:\n",
    "                for cl in df_train[col].unique():\n",
    "                    cat_cols.append(col + \"_\" + str(cl))\n",
    "        return cat_cols\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        df_train = df_train.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        self._ohe = OneHotEncoder()\n",
    "        self._ohe.fit(df_train)\n",
    "        \n",
    "        self._bool_cols = df_train.select_dtypes([\"bool\"]).columns.to_list()\n",
    "        self._cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        self._num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        \n",
    "        # cat column names after being one-hot encoded\n",
    "        all_cat_ohe_cols = self._ohe.transform(df_train).columns.to_list()\n",
    "        self._cat_cols_ohe = []\n",
    "        if self._ohe_cat_max_class is not None:\n",
    "            for col in self._cat_cols:\n",
    "                if df_train[col].nunique() <=  self._ohe_cat_max_class:\n",
    "                    for cl in df_train[col].unique():\n",
    "                        ohe_col = col + \"_\" + str(cl)\n",
    "                        \n",
    "                        if ohe_col in all_cat_ohe_cols:\n",
    "                            self._cat_cols_ohe.append(ohe_col)\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        by_sers = [df[col] for col in self._by_list_cols]\n",
    "        df = df.drop(self._by_list_cols, axis=\"columns\")\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        for col in self._bool_cols + self._cat_cols + self._num_cols:\n",
    "            if col not in all_cols:\n",
    "                raise ValueError(col + \" exists in train but not in test\")\n",
    "        \n",
    "        dfs = []\n",
    "        \n",
    "        # bool cols\n",
    "        if len(self._bool_cols) > 0:\n",
    "            df_bool = df[self._bool_cols]\n",
    "            print(\"Aggregating bool df with shape:\", df_bool.shape)\n",
    "            df_bool = self._bool_agg(df_bool, by_sers)\n",
    "            \n",
    "            for col in df_bool.columns:\n",
    "                if col.endswith(\"_mean\") or col.endswith(\"_sum\") or col.endswith(\"_entropy\"):\n",
    "                    df_bool[col] = df_bool[col].astype(\"float32\")\n",
    "                \n",
    "                if col.endswith(\"_mode\"):\n",
    "                    df_bool[col] = df_bool[col].astype(\"category\")\n",
    "            dfs.append(df_bool)\n",
    "        \n",
    "        # categorical cols\n",
    "        if len(self._cat_cols) > 0:\n",
    "            df_cat = df[self._cat_cols]\n",
    "            print(\"Aggregating cat df with shape:\", df_cat.shape)\n",
    "            df_cat = self._cat_agg(df_cat, by_sers)\n",
    "            \n",
    "            for col in df_cat.columns:\n",
    "                if col.endswith(\"_mean\") or col.endswith(\"_entropy\"):\n",
    "                    df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "                \n",
    "                if col.endswith(\"_mode\"):\n",
    "                    df_cat[col] = df_cat[col].astype(\"category\")\n",
    "            \n",
    "            dfs.append(df_cat)\n",
    "        \n",
    "        # number cols\n",
    "        df_num = df[self._num_cols]\n",
    "        if df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with shape:\", df_num.shape)\n",
    "            df_num = self._num_agg(df_num, by_sers)\n",
    "            dfs.append(df_num)\n",
    "        \n",
    "        # ohe cat cols\n",
    "        if len(self._cat_cols_ohe) > 0:\n",
    "            df_ohe = self._ohe.transform(df)\n",
    "            for col in self._cat_cols_ohe:\n",
    "                if col not in df_ohe.columns:\n",
    "                    raise ValueError(col + \" is not in cols of df_ohe\")\n",
    "                    \n",
    "            df_ohe = df_ohe[self._cat_cols_ohe]\n",
    "            print(\"Aggregating ohe cat df with shape:\", df_ohe.shape)\n",
    "            \n",
    "            df_ohe = self._ohe_cat_agg(df_ohe, by_sers)\n",
    "            dfs.append(df_ohe)\n",
    "        \n",
    "        # aggregate num cols with iqr, range and mean-median difference\n",
    "        df_num = df.select_dtypes([\"number\"])\n",
    "        print(\"df_num.shape for iqr, mimmax_range, mean_median_diff\", df_num.shape)\n",
    "            \n",
    "        if self._iqr and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with iqr\")\n",
    "            df_iqr = self._iqr_agg(df_num, by_sers)\n",
    "            dfs.append(df_iqr)\n",
    "        \n",
    "        if self._minmax_range and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with range\")\n",
    "            df_range = self._range_agg(df_num, by_sers)\n",
    "            print(\"df_range.shape\", df_range.shape)\n",
    "            dfs.append(df_range)\n",
    "        \n",
    "        if self._mean_median_diff and df_num.shape[1] > 0:\n",
    "            print(\"Aggregating num df with mean-median difference\")\n",
    "            df_diff = self._mm_diff_agg(df_num, by_sers)\n",
    "            dfs.append(df_diff)\n",
    "        \n",
    "        return pd.concat(dfs, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(df, matching_key, train_id_ser):\n",
    "    is_train = df[matching_key].isin(train_id_ser.values)\n",
    "    \n",
    "    train = df.loc[is_train, :]\n",
    "    test = df.loc[~is_train, :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(ser):\n",
    "    return ser.mode().values[0]\n",
    "\n",
    "\n",
    "def entropy(ser):\n",
    "    pk = ser.value_counts(normalize=True)\n",
    "    return stats.entropy(pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `application_[train|test]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = [\"^APARTMENTS_\", \"^BASEMENTAREA_\", \"^YEARS_B\", \"^COMMONAREA_\", \n",
    "                               \"^ELEVATORS_\", \"^ENTRANCES_\", \"^FLOORS\", \"^LANDAREA_\", \"^LIVING\", \n",
    "                               \"^NONLIVING\", \"AMT_REQ_CREDIT_BUREAU_\"]\n",
    "        self._spec_impt_regex_val_num = -1.\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"OWN_CAR_AGE\": -1.,\n",
    "                                    \"EXT_SOURCE_1\": 0.,\n",
    "                                    \"EXT_SOURCE_3\": 0.,\n",
    "                                    \"TOTALAREA_MODE\": -1.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "        \n",
    "\n",
    "class ApplNewColsAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df_train):\n",
    "        credit_to_income = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_INCOME_TOTAL\"]\n",
    "        self._cti_min = credit_to_income.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._cti_max = credit_to_income.replace(np.inf, np.nan).max() * 10.\n",
    "        \n",
    "        credit_to_goods = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_GOODS_PRICE\"]\n",
    "        self._ctg_min = credit_to_goods.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._ctg_max = credit_to_goods.replace(np.inf, np.nan).max() * 10.\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        df_new[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df_new[\"AMT_INCOME_TOTAL\"])\n",
    "        df_new[\"DAYS_EMPLOYED_POSITIVE\"] = df_new[\"DAYS_EMPLOYED\"] > 0\n",
    "        days_emp_max = df_new[\"DAYS_EMPLOYED\"].max()\n",
    "        if days_emp_max > 0:\n",
    "            df_new[\"DAYS_EMPLOYED\"] = df_new[\"DAYS_EMPLOYED\"].replace({days_emp_max: 100.})\n",
    "        \n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_INCOME_TOTAL\"]\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(-np.inf, self._cti_min)\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(np.inf, self._cti_max)\n",
    "        \n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_GOODS_PRICE\"]\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(-np.inf, self._ctg_min)\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(np.inf, self._ctg_max)\n",
    "        \n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = load_csv(\"data/download/application_train.csv\")\n",
    "application_test = load_csv(\"data/download/application_test.csv\")\n",
    "\n",
    "appl_train_key = application_train[\"SK_ID_CURR\"]\n",
    "appl_test_key = application_test[\"SK_ID_CURR\"]\n",
    "print(application_train.shape, application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "appl_train = application_train.copy()\n",
    "appl_test = application_test.copy()\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "imputer = ApplImputer()\n",
    "imputer.fit(appl_train)\n",
    "appl_train = imputer.transform(appl_train)\n",
    "appl_test = imputer.transform(appl_test)\n",
    "print(\"appl_train.isnull().sum().sum()\", appl_train.isnull().sum().sum())\n",
    "print(\"appl_test.isnull().sum().sum()\", appl_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "remover = CollinearColumnRemover(0.95, col_regex=\"_ISNULL$\")\n",
    "remover.fit(appl_train)\n",
    "appl_train = remover.transform(appl_train)\n",
    "appl_test = remover.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "adder = ApplNewColsAdder()\n",
    "adder.fit(appl_train)\n",
    "appl_train = adder.transform(appl_train)\n",
    "appl_test = adder.transform(appl_test)\n",
    "print(\"appl_train.shape\", appl_train.shape)\n",
    "print(\"appl_test.shape\", appl_test.shape)\n",
    "\n",
    "\n",
    "if False:\n",
    "    appl_train.to_csv(\"data/data_/application_train.csv\", index=False)\n",
    "    appl_test.to_csv(\"data/data_/application_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `bureau` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BureauImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"DAYS_ENDDATE_FACT\": 100.,\n",
    "                                    \"AMT_CREDIT_MAX_OVERDUE\": -1000.,\n",
    "                                    \"AMT_CREDIT_SUM_DEBT\": 0.,\n",
    "                                    \"AMT_CREDIT_SUM_LIMIT\": 0.,\n",
    "                                    \"AMT_ANNUITY\": -1000.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "\n",
    "def drop_cols_bureau(df):\n",
    "    # mostly zero\n",
    "    cols_to_drop = [\"CREDIT_DAY_OVERDUE\", \"CNT_CREDIT_PROLONG\", \"AMT_CREDIT_SUM_OVERDUE\"]\n",
    "    return df.drop(cols_to_drop, axis=\"columns\")\n",
    "\n",
    "\n",
    "def add_new_cols_bureau(df):\n",
    "    df[\"DAYS_CREDIT_ENDDATE_ISPOSITIVE\"] = df[\"DAYS_CREDIT_ENDDATE\"] > 0\n",
    "\n",
    "    df[\"AMT_CREDIT_MAX_OVERDUE_ISPOSITIVE\"] = df[\"AMT_CREDIT_MAX_OVERDUE\"] > 0\n",
    "    df[\"AMT_CREDIT_SUM_DEBT_ISPOSITIVE\"] = df[\"AMT_CREDIT_SUM_DEBT\"] > 0\n",
    "    df[\"AMT_CREDIT_SUM_LIMIT_ISPOSITIVE\"] = df[\"AMT_CREDIT_SUM_LIMIT\"] > 0\n",
    "    df[\"AMT_ANNUITY_ISPOSITIVE\"] =  df[\"AMT_ANNUITY\"] > 0\n",
    "    \n",
    "    amt_credt_sum = df[\"AMT_CREDIT_SUM\"] + 1\n",
    "    df[\"AMT_CREDIT_MAX_OVERDUE_TO_SUM\"] = df[\"AMT_CREDIT_MAX_OVERDUE\"] / amt_credt_sum\n",
    "    df[\"AMT_CREDIT_SUM_DEBT_TO_SUM\"] = df[\"AMT_CREDIT_SUM_DEBT\"] / amt_credt_sum\n",
    "    df[\"AMT_CREDIT_SUM_LIMIT_TO_SUM\"] = df[\"AMT_CREDIT_SUM_LIMIT\"] / amt_credt_sum\n",
    "    return df\n",
    "\n",
    "\n",
    "def bu_nearest_status(df):\n",
    "    return df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[0]\n",
    "\n",
    "def bu_mode_status_three_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[: 3]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def bu_mode_status_six_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"DAYS_CREDIT\"], ascending=False)[\"CREDIT_ACTIVE\"].iloc[: 6]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "\n",
    "def add_new_agg_cols_bu(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_CREDIT_ACTIVE\"] = df.groupby(\"SK_ID_CURR\").apply(bu_nearest_status)\n",
    "    results[\"MODE_CREDIT_ACTIVE_THREE\"] = df.groupby(\"SK_ID_CURR\").apply(bu_mode_status_three_nearest)\n",
    "    results[\"MODE_CREDIT_ACTIVE_SIX\"] = df.groupby(\"SK_ID_CURR\").apply(bu_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = load_csv(\"data/download/bureau.csv\")\n",
    "print(\"bureau.shape\", bureau.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_partition(bureau, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"bureau_train.shape\", bureau_train.shape)\n",
    "print(\"bureau_test.shape\", bureau_test.shape)\n",
    "\n",
    "bureau_train_keys = bureau_train[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "bureau_test_keys = bureau_test[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]]\n",
    "\n",
    "bureau_train = bureau_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_test = bureau_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "\n",
    "bureau_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "bureau_agg_train = bureau_train.copy()\n",
    "bureau_agg_test = bureau_test.copy()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# drop some cols\n",
    "bureau_agg_train = drop_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = drop_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = BureauImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# add some cols\n",
    "bureau_agg_train = add_new_cols_bureau(bureau_agg_train)\n",
    "bureau_agg_test = add_new_cols_bureau(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_bu(bureau_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bu(bureau_agg_test)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_CURR\"\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "aggregator.fit(bureau_agg_train)\n",
    "bureau_agg_train = aggregator.transform(bureau_agg_train)\n",
    "bureau_agg_test = aggregator.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "# in case \n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_agg_train)\n",
    "bureau_agg_train = imputer.transform(bureau_agg_train)\n",
    "bureau_agg_test = imputer.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.isnull().sum().sum()\", bureau_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_agg_test.isnull().sum().sum()\", bureau_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear cols\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_agg_train)\n",
    "bureau_agg_train = remover.transform(bureau_agg_train)\n",
    "bureau_agg_test = remover.transform(bureau_agg_test)\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "# reset index\n",
    "bureau_agg_train = bureau_agg_train.reset_index()\n",
    "bureau_agg_test = bureau_agg_test.reset_index()\n",
    "print(\"bureau_agg_train.shape\", bureau_agg_train.shape)\n",
    "print(\"bureau_agg_test.shape\", bureau_agg_test.shape)\n",
    "\n",
    "\n",
    "if False:\n",
    "    bureau_agg_train.to_csv(\"data/data_/bureau_agg_train.csv\", index=False)\n",
    "    bureau_agg_test.to_csv(\"data/data_/bureau_agg_test.csv\", index=False)\n",
    "\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `bureau_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_nearest_status(df):\n",
    "    return df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[0]\n",
    "\n",
    "def bb_mode_status_three_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 3]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "def bb_mode_status_six_nearest(df):\n",
    "    statuses = df.sort_values(by=[\"MONTHS_BALANCE\"], ascending=False)[\"STATUS\"].iloc[: 6]\n",
    "    return statuses.mode().values[0]\n",
    "\n",
    "\n",
    "def add_new_agg_cols_bb(df):\n",
    "    results = {}\n",
    "    results[\"NEAREST_STATUS\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_nearest_status)\n",
    "    results[\"MODE_STATUS_THREE_NEAREST\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_mode_status_three_nearest)\n",
    "    results[\"MODE_STATUS_SIX_NEAREST\"] = df.groupby(\"SK_ID_BUREAU\").apply(bb_mode_status_six_nearest)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = load_csv(\"data/download/bureau_balance.csv\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance = bureau_balance.merge(bureau[[\"SK_ID_CURR\", \"SK_ID_BUREAU\"]], how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance = bureau_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "bureau_balance[\"SK_ID_CURR\"] = bureau_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"bureau_balance.shape\", bureau_balance.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_partition(bureau_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "bureau_balance_train = bureau_balance_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "bureau_balance_test = bureau_balance_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"bureau_balance_train.shape:\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape:\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_train.copy()\n",
    "bureau_balance_agg_test = bureau_balance_test.copy()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_bb(bureau_balance_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_bb(bureau_balance_agg_test)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_BUREAU\"\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_BUREAU\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new aggs cols\n",
    "for col in new_agg_cols_train:\n",
    "    bureau_balance_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    bureau_balance_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "bureau_balance_agg_train = load_csv(\"data/data_/bureau_balance_agg_train_tmp.csv\")\n",
    "bureau_balance_agg_test = load_csv(\"data/data_/bureau_balance_agg_test_tmp.csv\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.merge(bureau_train_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.merge(bureau_test_keys, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.drop([\"SK_ID_BUREAU\"], axis=\"columns\")\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = aggregator.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = aggregator.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = imputer.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = imputer.transform(bureau_balance_agg_test)\n",
    "print(\"bureau_balance_agg_train.isnull().sum().sum():\", bureau_balance_agg_train.isnull().sum().sum())\n",
    "print(\"bureau_balance_agg_test.isnull().sum().sum():\", bureau_balance_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(bureau_balance_agg_train)\n",
    "bureau_balance_agg_train = remover.transform(bureau_balance_agg_train)\n",
    "bureau_balance_agg_test = remover.transform(bureau_balance_agg_test)\n",
    "\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "bureau_balance_agg_train = bureau_balance_agg_train.reset_index()\n",
    "bureau_balance_agg_test = bureau_balance_agg_test.reset_index()\n",
    "print(\"bureau_balance_agg_train.shape\", bureau_balance_agg_train.shape)\n",
    "print(\"bureau_balance_agg_test.shape\", bureau_balance_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    bureau_balance_agg_train.to_csv(\"data/data_/bureau_balance_agg_train.csv\", index=False)\n",
    "    bureau_balance_agg_test.to_csv(\"data/data_/bureau_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `previous application` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `previous_application.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevApplImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"RATE_DOWN_PAYMENT\": -1.,\n",
    "                                   \"CNT_PAYMENT\": -10.,\n",
    "                                   \"DAYS_FIRST_DRAWING\": 0., \n",
    "                                   \"DAYS_FIRST_DUE\": 0.,\n",
    "                                   \"DAYS_LAST_DUE_1ST_VERSION\": 0.,\n",
    "                                   \"DAYS_LAST_DUE\": 0.,\n",
    "                                   \"DAYS_TERMINATION\": 0.}\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = {\"NAME_TYPE_SUITE\": \"missing_value\",\n",
    "                                    \"NFLAG_INSURED_ON_APPROVAL\": \"missing_value\"}\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "    \n",
    "\n",
    "def hour_period_bin(hours):\n",
    "    hours = hours.values\n",
    "    hour_bin = np.array([\"evening\"] * len(hours), dtype=\"object\")\n",
    "    morning_mask = (hours > 5) & (hours < 12)\n",
    "    afternoon_mask = (hours >= 12) & (hours < 18)\n",
    "    \n",
    "    hour_bin[morning_mask] = \"morning\"\n",
    "    hour_bin[afternoon_mask] = \"afternoon\"\n",
    "    return hour_bin\n",
    "\n",
    "\n",
    "def add_new_cols_prev_appl(df):\n",
    "    # add to bool cols to identify if values are non-negative\n",
    "    cols_is_nonneg = [\"DAYS_FIRST_DRAWING\", \"DAYS_FIRST_DUE\", \n",
    "                      \"DAYS_LAST_DUE_1ST_VERSION\", \n",
    "                      \"DAYS_LAST_DUE\", \"DAYS_TERMINATION\"]\n",
    "    for col in cols_is_nonneg:\n",
    "        df[col + \"_IS_NONNEG\"] = df[col] >= 0\n",
    "        \n",
    "        df[\"PERIOD_APPR_PROCESS_START\"] = hour_period_bin(df[\"HOUR_APPR_PROCESS_START\"])\n",
    "        df[\"PERIOD_APPR_PROCESS_START\"] = df[\"PERIOD_APPR_PROCESS_START\"].astype(\"category\")\n",
    "    \n",
    "    df[\"AMT_APPLICATION_TO_CREDIT\"] = df[\"AMT_APPLICATION\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    df[\"AMT_DOWN_PAY_TO_CREDIT\"] = df[\"AMT_DOWN_PAYMENT\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    df[\"AMT_GOODS_PRICE_TO_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / (df[\"AMT_CREDIT\"] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def mean_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.mean()\n",
    "\n",
    "def min_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.min()\n",
    "\n",
    "def max_n_nearest(df, sort_by, n, col):\n",
    "    ser = df.sort_values(by=sort_by, ascending=False)[col].iloc[: n]\n",
    "    return ser.max()\n",
    "\n",
    "\n",
    "def add_new_agg_cols_prev_appl(df):\n",
    "    sort_by = [\"DAYS_DECISION\"]\n",
    "    orig_cols = [\"AMT_APPLICATION_TO_CREDIT\", \"AMT_DOWN_PAY_TO_CREDIT\", \"AMT_GOODS_PRICE_TO_CREDIT\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_CURR\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_application = load_csv(\"data/download/previous_application.csv\")\n",
    "print(\"previous_application.shape\", previous_application.shape)\n",
    "\n",
    "prev_appl_train, prev_appl_test = train_test_partition(previous_application, \"SK_ID_CURR\", appl_train_key)\n",
    "\n",
    "print(\"prev_appl_train.shape\", prev_appl_train.shape)\n",
    "print(\"prev_appl_test.shape\", prev_appl_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_train_keys = prev_appl_train[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "prev_appl_test_keys = prev_appl_test[[\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "\n",
    "prev_appl_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "prev_appl_agg_train = prev_appl_train.copy()\n",
    "prev_appl_agg_test = prev_appl_test.copy()\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# drop cols with high percentage of null\n",
    "cols_to_drop = [\"RATE_INTEREST_PRIMARY\", \"RATE_INTEREST_PRIVILEGED\"]\n",
    "prev_appl_agg_train = prev_appl_agg_train.drop(cols_to_drop, axis=\"columns\")\n",
    "prev_appl_agg_test = prev_appl_agg_test.drop(cols_to_drop, axis=\"columns\")\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = PrevApplImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "# add new cols\n",
    "prev_appl_agg_train = add_new_cols_prev_appl(prev_appl_agg_train)\n",
    "prev_appl_agg_test = add_new_cols_prev_appl(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_prev_appl(prev_appl_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_prev_appl(prev_appl_agg_test)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_CURR\"\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = aggregator.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = aggregator.transform(prev_appl_agg_test)\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    prev_appl_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    prev_appl_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = imputer.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = imputer.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.isnull().sum().sum()\", prev_appl_agg_train.isnull().sum().sum())\n",
    "print(\"prev_appl_agg_test.isnull().sum().sum()\", prev_appl_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(prev_appl_agg_train)\n",
    "prev_appl_agg_train = remover.transform(prev_appl_agg_train)\n",
    "prev_appl_agg_test = remover.transform(prev_appl_agg_test)\n",
    "print(\"prev_appl_agg_train.shape\", prev_appl_agg_train.shape)\n",
    "print(\"prev_appl_agg_test.shape\", prev_appl_agg_test.shape)\n",
    "\n",
    "prev_appl_agg_train = prev_appl_agg_train.reset_index()\n",
    "prev_appl_agg_test = prev_appl_agg_test.reset_index()\n",
    "\n",
    "if False:\n",
    "    prev_appl_agg_train.to_csv(\"data/data_/previous_application_agg_train.csv\", index=False)\n",
    "    prev_appl_agg_test.to_csv(\"data/data_/previous_application_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `POS_CASH_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCBalImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"CNT_INSTALMENT\": -1.,\n",
    "                                   \"CNT_INSTALMENT_FUTURE\": -1.,\n",
    "                                   }\n",
    "        self._default_imput_vals_num = \"median\"\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "\n",
    "\n",
    "def add_new_agg_cols_pc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\", \"SK_DPD\", \"SK_DPD_DEF\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance = load_csv(\"data/download/POS_CASH_balance.csv\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "POS_CASH_balance = POS_CASH_balance.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "POS_CASH_balance = POS_CASH_balance.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]], \n",
    "                                          how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "# drop rows that does not have \"SK_ID_PREV\" in previous_application\n",
    "POS_CASH_balance = POS_CASH_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "POS_CASH_balance[\"SK_ID_CURR\"] = POS_CASH_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"POS_CASH_balance.shape\", POS_CASH_balance.shape)\n",
    "\n",
    "PC_bal_train, PC_bal_test = train_test_partition(POS_CASH_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "PC_bal_train = PC_bal_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "PC_bal_test = PC_bal_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"PC_bal_train.shape:\", PC_bal_train.shape)\n",
    "print(\"PC_bal_test.shape:\", PC_bal_test.shape)\n",
    "\n",
    "PC_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "PC_bal_agg_train = PC_bal_train.copy()\n",
    "PC_bal_agg_test = PC_bal_test.copy()\n",
    "\n",
    "# impute missing values\n",
    "imputer = PCBalImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_pc_bal(PC_bal_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_pc_bal(PC_bal_agg_test)\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.drop([\"MONTHS_BALANCE\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_PREV\"\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    PC_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    PC_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "if False:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_train_tmp.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over SK_ID_CURR\n",
    "time_start = time.time()\n",
    "\n",
    "# this turns [0, 1] into bool\n",
    "PC_bal_agg_train = load_csv(\"data/data_/POS_CASH_balance_agg_train_tmp.csv\")\n",
    "PC_bal_agg_test = load_csv(\"data/data_/POS_CASH_balance_agg_test_tmp.csv\")\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "PC_bal_agg_train = PC_bal_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "PC_bal_agg_test = PC_bal_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = aggregator.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = aggregator.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = imputer.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = imputer.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.isnull().sum().sum()\", PC_bal_agg_train.isnull().sum().sum())\n",
    "print(\"PC_bal_agg_test.isnull().sum().sum()\", PC_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(PC_bal_agg_train)\n",
    "PC_bal_agg_train = remover.transform(PC_bal_agg_train)\n",
    "PC_bal_agg_test = remover.transform(PC_bal_agg_test)\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "PC_bal_agg_train = PC_bal_agg_train.reset_index()\n",
    "PC_bal_agg_test = PC_bal_agg_test.reset_index()\n",
    "print(\"PC_bal_agg_train.shape:\", PC_bal_agg_train.shape)\n",
    "print(\"PC_bal_agg_test.shape:\", PC_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "if False:\n",
    "    PC_bal_agg_train.to_csv(\"data/data_/POS_CASH_balance_agg_train.csv\", index=False)\n",
    "    PC_bal_agg_test.to_csv(\"data/data_/POS_CASH_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `installments_payments.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstalPayImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._days_entry_payment_impute = df_train[\"DAYS_ENTRY_PAYMENT\"].min() - 10\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df[\"DAYS_ENTRY_PAYMENT\"] = df[\"DAYS_ENTRY_PAYMENT\"].fillna(self._days_entry_payment_impute)\n",
    "        df[\"AMT_PAYMENT\"] = df[\"AMT_PAYMENT\"].fillna(0.)\n",
    "        return df\n",
    "\n",
    "\n",
    "def add_new_cols_inst_pay(df):\n",
    "    df[\"DAYS_INSTAL_PAY_DIFF\"] = df[\"DAYS_ENTRY_PAYMENT\"] - df[\"DAYS_INSTALMENT\"]\n",
    "    df[\"DAYS_INSTAL_PAY_DIFF_ISPOSITIVE\"] = df[\"DAYS_INSTAL_PAY_DIFF\"] > 0\n",
    "    \n",
    "    df[\"AMT_INSTAL_PAY_DIFF\"] = df[\"AMT_PAYMENT\"] - df[\"AMT_INSTALMENT\"]\n",
    "    df[\"AMT_INSTAL_PAY_DIFF_ISPOSITIVE\"] = df[\"AMT_INSTAL_PAY_DIFF\"] > 0\n",
    "    \n",
    "    df[\"AMT_PAY_INSTAL_RATIO\"] = np.clip((df[\"AMT_PAYMENT\"] + 1) / (df[\"AMT_INSTALMENT\"] + 1), 0., 10.)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_agg_cols_inst_pay(df):\n",
    "    sort_by = [\"DAYS_INSTALMENT\"]\n",
    "    orig_cols = [\"DAYS_INSTAL_PAY_DIFF\", \"AMT_INSTAL_PAY_DIFF\", \"AMT_PAY_INSTAL_RATIO\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments = load_csv(\"data/download/installments_payments.csv\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "installments_payments = installments_payments.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "installments_payments = installments_payments.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]],\n",
    "                                                   how=\"left\", on=\"SK_ID_PREV\")\n",
    "\n",
    "# drop rows that does not have \"SK_ID_PREV\" in previous_application\n",
    "installments_payments = installments_payments.dropna(subset=[\"SK_ID_CURR\"])\n",
    "installments_payments[\"SK_ID_CURR\"] = installments_payments[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"installments_payments.shape\", installments_payments.shape)\n",
    "\n",
    "inst_pay_train, inst_pay_test = train_test_partition(installments_payments, \"SK_ID_CURR\", appl_train_key)\n",
    "inst_pay_train = inst_pay_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "inst_pay_test = inst_pay_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"inst_pay_train.shape:\", inst_pay_train.shape)\n",
    "print(\"inst_pay_test.shape:\", inst_pay_test.shape)\n",
    "\n",
    "inst_pay_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "inst_pay_agg_train = inst_pay_train.copy()\n",
    "inst_pay_agg_test = inst_pay_test.copy()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "imputer = InstalPayImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# add some more cols\n",
    "inst_pay_agg_train = add_new_cols_inst_pay(inst_pay_agg_train)\n",
    "inst_pay_agg_test = add_new_cols_inst_pay(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_inst_pay(inst_pay_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_inst_pay(inst_pay_agg_test)\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.drop([\"DAYS_INSTALMENT\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_PREV\"\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    inst_pay_agg_train[col] = new_agg_cols_train[col]\n",
    "    \n",
    "for col in new_agg_cols_test:\n",
    "    inst_pay_agg_test[col] = new_agg_cols_test[col]\n",
    "\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# just in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_train_tmp.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "inst_pay_agg_train = load_csv(\"data/data_/installments_payments_agg_train_tmp.csv\")\n",
    "inst_pay_agg_test = load_csv(\"data/data_/installments_payments_agg_test_tmp.csv\")\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "inst_pay_agg_train = inst_pay_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "inst_pay_agg_test = inst_pay_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = aggregator.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = aggregator.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum():\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum():\", inst_pay_agg_test.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# just in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = imputer.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = imputer.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.isnull().sum().sum()\", inst_pay_agg_train.isnull().sum().sum())\n",
    "print(\"inst_pay_agg_test.isnull().sum().sum()\", inst_pay_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(inst_pay_agg_train)\n",
    "inst_pay_agg_train = remover.transform(inst_pay_agg_train)\n",
    "inst_pay_agg_test = remover.transform(inst_pay_agg_test)\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "inst_pay_agg_train = inst_pay_agg_train.reset_index()\n",
    "inst_pay_agg_test = inst_pay_agg_test.reset_index()\n",
    "print(\"inst_pay_agg_train.shape:\", inst_pay_agg_train.shape)\n",
    "print(\"inst_pay_agg_test.shape:\", inst_pay_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    inst_pay_agg_train.to_csv(\"data/data_/installments_payments_agg_train.csv\", index=False)\n",
    "    inst_pay_agg_test.to_csv(\"data/data_/installments_payments_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `credit_card_balance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCBalImputer(Imputer):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = None\n",
    "        self._spec_impt_regex_val_num = None\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"AMT_DRAWINGS_ATM_CURRENT\": 0.,\n",
    "                                   \"AMT_DRAWINGS_OTHER_CURRENT\": 0., \n",
    "                                    \"AMT_DRAWINGS_POS_CURRENT\": 0.,\n",
    "                                    \"AMT_INST_MIN_REGULARITY\": 0.,\n",
    "                                    \"AMT_PAYMENT_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_ATM_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_OTHER_CURRENT\": 0.,\n",
    "                                    \"CNT_DRAWINGS_POS_CURRENT\": 0.,\n",
    "                                    \"CNT_INSTALMENT_MATURE_CUM\": 0.,\n",
    "                                    \n",
    "                                   }\n",
    "        self._default_imput_vals_num = 0.\n",
    "        \n",
    "        self._spec_impt_vals_cat = None\n",
    "        self._default_imput_vals_cat = \"missing_value\"\n",
    "        \n",
    "\n",
    "def add_new_cols_cc_bal(df):\n",
    "    df[\"AMT_PAYMENT_TO_BALANCE\"] = df[\"AMT_PAYMENT_TOTAL_CURRENT\"] / df[\"AMT_BALANCE\"].replace(0., 1.)\n",
    "    df[\"AMT_BALANCE_TO_CREDIT_LIMIT\"] = (df[\"AMT_BALANCE\"] + 1) / (df[\"AMT_CREDIT_LIMIT_ACTUAL\"] + 1)\n",
    "    df[\"AMT_DRAWING_TOTAL\"] = (df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_CURRENT\"] + \n",
    "                              df[\"AMT_DRAWINGS_OTHER_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"])\n",
    "    df[\"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\"] = df[\"AMT_DRAWING_TOTAL\"] / (df[\"AMT_CREDIT_LIMIT_ACTUAL\"] + 1)\n",
    "    df[\"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\"] = (df[\"AMT_PAYMENT_TOTAL_CURRENT\"] / \n",
    "                                                df[\"AMT_DRAWING_TOTAL\"].replace(0., 1.))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_new_agg_cols_cc_bal(df):\n",
    "    sort_by = [\"MONTHS_BALANCE\"]\n",
    "    orig_cols = [\"AMT_PAYMENT_TO_BALANCE\", \"AMT_BALANCE_TO_CREDIT_LIMIT\", \n",
    "                 \"AMT_DRAWING_TOTAL_TO_CREDIT_LIMIT\", \"AMT_PAYMENT_TOTAL_TO_DRAWING_TOTAL\", \n",
    "                 \"SK_DPD\"]\n",
    "    number_nn_entries = [3, 6]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for col in orig_cols:\n",
    "        for n in number_nn_entries:\n",
    "            new_col = \"MEAN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest mean for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: mean_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MIN_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest min for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: min_n_nearest(df, sort_by, n, col))\n",
    "            \n",
    "            new_col = \"MAX_\" + col + \"_%d_NEAREST\" % n\n",
    "            print(\"Nearest max for \" + new_col)\n",
    "            results[new_col] = df.groupby(\"SK_ID_PREV\").apply(lambda df: max_n_nearest(df, sort_by, n, col))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance = load_csv(\"data/download/credit_card_balance.csv\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "credit_card_balance = credit_card_balance.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "credit_card_balance = credit_card_balance.merge(previous_application[[\"SK_ID_PREV\", \"SK_ID_CURR\"]],\n",
    "                                               how=\"left\", on=\"SK_ID_PREV\")\n",
    "\n",
    "credit_card_balance = credit_card_balance.dropna(subset=[\"SK_ID_CURR\"])\n",
    "credit_card_balance[\"SK_ID_CURR\"] = credit_card_balance[\"SK_ID_CURR\"].astype(\"int32\")\n",
    "print(\"credit_card_balance.shape\", credit_card_balance.shape)\n",
    "\n",
    "cc_bal_train, cc_bal_test = train_test_partition(credit_card_balance, \"SK_ID_CURR\", appl_train_key)\n",
    "cc_bal_train = cc_bal_train.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "cc_bal_test = cc_bal_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "\n",
    "print(\"cc_bal_train.shape:\", cc_bal_train.shape)\n",
    "print(\"cc_bal_test.shape:\", cc_bal_test.shape)\n",
    "\n",
    "cc_bal_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "cc_bal_agg_train = cc_bal_train.copy()\n",
    "cc_bal_agg_test = cc_bal_test.copy()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "imputer = CCBalImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new cols\n",
    "cc_bal_agg_train = add_new_cols_cc_bal(cc_bal_agg_train)\n",
    "cc_bal_agg_test = add_new_cols_cc_bal(cc_bal_agg_test)\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# new agg cols\n",
    "new_agg_cols_train = add_new_agg_cols_cc_bal(cc_bal_agg_train)\n",
    "new_agg_cols_test = add_new_agg_cols_cc_bal(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# aggregate over \"SK_ID_PREV\"\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_PREV\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# add new agg cols\n",
    "for col in new_agg_cols_train:\n",
    "    cc_bal_agg_train[col] = new_agg_cols_train[col]\n",
    "\n",
    "for col in new_agg_cols_test:\n",
    "    cc_bal_agg_test[col] = new_agg_cols_test[col]\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# just in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_train_tmp.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_test_tmp.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate over \"SK_ID_CURR\"\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "cc_bal_agg_train = load_csv(\"data/data_/credit_card_balance_agg_train_tmp.csv\")\n",
    "cc_bal_agg_test = load_csv(\"data/data_/credit_card_balance_agg_test_tmp.csv\")\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "cc_bal_agg_train = cc_bal_agg_train.merge(prev_appl_train_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "cc_bal_agg_test = cc_bal_agg_test.merge(prev_appl_test_keys, how=\"left\", on=\"SK_ID_PREV\")\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "cc_bal_agg_train = cc_bal_agg_train.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "cc_bal_agg_test = cc_bal_agg_test.drop([\"SK_ID_PREV\"], axis=\"columns\")\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "# aggregate\n",
    "num_stats = [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"var\"]\n",
    "bool_stats = [\"sum\", \"mean\", mode, entropy]\n",
    "cat_stats = [\"count\", \"nunique\", mode, entropy]\n",
    "ohe_cat_stats = [\"sum\", \"mean\", \"var\"]\n",
    "\n",
    "by_list_cols = [\"SK_ID_CURR\"]\n",
    "\n",
    "aggregator = Aggregator(by_list_cols, num_stats, bool_stats, cat_stats,\n",
    "                        ohe_cat_stats=ohe_cat_stats,\n",
    "                        ohe_cat_max_class=10,\n",
    "                        iqr=True, minmax_range=True, mean_median_diff=True)\n",
    "\n",
    "aggregator.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = aggregator.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = aggregator.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# in case\n",
    "imputer = DefaultImputer()\n",
    "imputer.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = imputer.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = imputer.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.isnull().sum().sum():\", cc_bal_agg_train.isnull().sum().sum())\n",
    "print(\"cc_bal_agg_test.isnull().sum().sum():\", cc_bal_agg_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# remove collinear columns\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(cc_bal_agg_train)\n",
    "cc_bal_agg_train = remover.transform(cc_bal_agg_train)\n",
    "cc_bal_agg_test = remover.transform(cc_bal_agg_test)\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "\n",
    "# reset index\n",
    "cc_bal_agg_train = cc_bal_agg_train.reset_index()\n",
    "cc_bal_agg_test = cc_bal_agg_test.reset_index()\n",
    "print(\"cc_bal_agg_train.shape\", cc_bal_agg_train.shape)\n",
    "print(\"cc_bal_agg_test.shape\", cc_bal_agg_test.shape)\n",
    "\n",
    "if False:\n",
    "    cc_bal_agg_train.to_csv(\"data/data_/credit_card_balance_agg_train.csv\", index=False)\n",
    "    cc_bal_agg_test.to_csv(\"data/data_/credit_card_balance_agg_test.csv\", index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_col_align(df_train, df_test, exclude_cols=None):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    cols_train = df_train.columns.to_list()\n",
    "    \n",
    "    for col in exclude_cols:\n",
    "        assert col in cols_train, col + \" is not in df_train\"\n",
    "        \n",
    "    test_cols = [col for col in cols_train if col not in exclude_cols]\n",
    "    return df_train[test_cols + exclude_cols], df_test[test_cols]\n",
    "\n",
    "\n",
    "def add_prefix_to_cols(df, prefix, exclude_cols=None):\n",
    "    df = df.copy()\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "        \n",
    "    cols = df.columns.to_list()\n",
    "    for i, col in enumerate(cols):\n",
    "        if col not in exclude_cols:\n",
    "            cols[i] = prefix + col\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframes(df_left, dfs_right, \n",
    "                     left_prefix, right_prefixes, \n",
    "                     on_col=\"SK_ID_CURR\"):\n",
    "    assert isinstance(dfs_right, list), \"dfs_right must be a list\"\n",
    "    assert isinstance(right_prefixes, list), \"right_prefixes must be a list\"\n",
    "    assert len(dfs_right) == len(right_prefixes), \"dfs_right and right_prefixes must have the same len\"\n",
    "    \n",
    "    result = df_left\n",
    "    result = add_prefix_to_cols(result, left_prefix, exclude_cols=[on_col])\n",
    "    \n",
    "    for df, prefix in zip(dfs_right, right_prefixes):\n",
    "        print(\"Merging with \" + prefix)\n",
    "        df = add_prefix_to_cols(df, prefix, exclude_cols=[on_col])\n",
    "        result = result.merge(df, how=\"left\", on=on_col)\n",
    "        \n",
    "        df_cols = [col for col in result.columns if col.startswith(prefix)]\n",
    "        # add mask column to tell which rows in df are missing\n",
    "        result[prefix + \"MISSING_ROW\"] = result[df_cols[0]].isnull()\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "class MergeMissingImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cols = df_train.columns.to_list()\n",
    "        cols_num = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        cols_cat = [col for col in cols if col not in cols_num]\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in cols_num:\n",
    "            if df_train[col].isnull().sum() > 0:\n",
    "                self._impute_values[col] = df_train[col].median()\n",
    "        \n",
    "        for col in cols_cat:\n",
    "            if df_train[col].isnull().sum() > 0:\n",
    "                self._impute_values[col] = mode(df_train[col])\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            try:\n",
    "                df[col] = df[col].fillna(val)\n",
    "            except AttributeError:\n",
    "                print(\"problem with \" + col)\n",
    "                raise AttributeError()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 311.82 MB\n",
      "Memory usage after changing types 143.31 MB\n",
      "Memory usage before changing types 49.04 MB\n",
      "Memory usage after changing types 22.53 MB\n",
      "application_train.shape: (307511, 139)\n",
      "application_test.shape: (48744, 138)\n",
      "application_train.shape: (307511, 139)\n",
      "application_test.shape: (48744, 138)\n"
     ]
    }
   ],
   "source": [
    "application_train = load_csv(\"data/data_/application_train.csv\")\n",
    "application_test = load_csv(\"data/data_/application_test.csv\")\n",
    "\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)\n",
    "\n",
    "application_train, application_test = train_test_col_align(application_train, application_test, \n",
    "                                                           exclude_cols=[\"TARGET\"])\n",
    "print(\"application_train.shape:\", application_train.shape)\n",
    "print(\"application_test.shape:\", application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OWN_CAR_AGE_ISNULL</th>\n",
       "      <th>EXT_SOURCE_1_ISNULL</th>\n",
       "      <th>EXT_SOURCE_3_ISNULL</th>\n",
       "      <th>TOTALAREA_MODE_ISNULL</th>\n",
       "      <th>APARTMENTS_AVG_ISNULL</th>\n",
       "      <th>BASEMENTAREA_AVG_ISNULL</th>\n",
       "      <th>YEARS_BUILD_AVG_ISNULL</th>\n",
       "      <th>COMMONAREA_AVG_ISNULL</th>\n",
       "      <th>ELEVATORS_AVG_ISNULL</th>\n",
       "      <th>LANDAREA_AVG_ISNULL</th>\n",
       "      <th>...</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>WALLSMATERIAL_MODE</th>\n",
       "      <th>EMERGENCYSTATE_MODE</th>\n",
       "      <th>AMT_INCOME_TOTAL_LOG</th>\n",
       "      <th>DAYS_EMPLOYED_POSITIVE</th>\n",
       "      <th>CREDIT_TO_INCOME</th>\n",
       "      <th>CREDIT_TO_GOODS</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>Stone, brick</td>\n",
       "      <td>No</td>\n",
       "      <td>12.218495</td>\n",
       "      <td>False</td>\n",
       "      <td>2.007889</td>\n",
       "      <td>1.158397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>School</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>Block</td>\n",
       "      <td>No</td>\n",
       "      <td>12.506177</td>\n",
       "      <td>False</td>\n",
       "      <td>4.790750</td>\n",
       "      <td>1.145199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Government</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>11.119883</td>\n",
       "      <td>False</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>11.813030</td>\n",
       "      <td>False</td>\n",
       "      <td>2.316167</td>\n",
       "      <td>1.052803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>11.707669</td>\n",
       "      <td>False</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OWN_CAR_AGE_ISNULL  EXT_SOURCE_1_ISNULL  EXT_SOURCE_3_ISNULL  \\\n",
       "0                True                False                False   \n",
       "1                True                False                 True   \n",
       "2               False                 True                False   \n",
       "3                True                 True                 True   \n",
       "4                True                 True                 True   \n",
       "\n",
       "   TOTALAREA_MODE_ISNULL  APARTMENTS_AVG_ISNULL  BASEMENTAREA_AVG_ISNULL  \\\n",
       "0                  False                  False                    False   \n",
       "1                  False                  False                    False   \n",
       "2                   True                   True                     True   \n",
       "3                   True                   True                     True   \n",
       "4                   True                   True                     True   \n",
       "\n",
       "   YEARS_BUILD_AVG_ISNULL  COMMONAREA_AVG_ISNULL  ELEVATORS_AVG_ISNULL  \\\n",
       "0                   False                  False                 False   \n",
       "1                   False                  False                 False   \n",
       "2                    True                   True                  True   \n",
       "3                    True                   True                  True   \n",
       "4                    True                   True                  True   \n",
       "\n",
       "   LANDAREA_AVG_ISNULL  ...       ORGANIZATION_TYPE  FONDKAPREMONT_MODE  \\\n",
       "0                False  ...  Business Entity Type 3    reg oper account   \n",
       "1                False  ...                  School    reg oper account   \n",
       "2                 True  ...              Government       missing_value   \n",
       "3                 True  ...  Business Entity Type 3       missing_value   \n",
       "4                 True  ...                Religion       missing_value   \n",
       "\n",
       "   HOUSETYPE_MODE  WALLSMATERIAL_MODE  EMERGENCYSTATE_MODE  \\\n",
       "0  block of flats        Stone, brick                   No   \n",
       "1  block of flats               Block                   No   \n",
       "2   missing_value       missing_value        missing_value   \n",
       "3   missing_value       missing_value        missing_value   \n",
       "4   missing_value       missing_value        missing_value   \n",
       "\n",
       "   AMT_INCOME_TOTAL_LOG  DAYS_EMPLOYED_POSITIVE  CREDIT_TO_INCOME  \\\n",
       "0             12.218495                   False          2.007889   \n",
       "1             12.506177                   False          4.790750   \n",
       "2             11.119883                   False          2.000000   \n",
       "3             11.813030                   False          2.316167   \n",
       "4             11.707669                   False          4.222222   \n",
       "\n",
       "   CREDIT_TO_GOODS  TARGET  \n",
       "0         1.158397       1  \n",
       "1         1.145199       0  \n",
       "2         1.000000       0  \n",
       "3         1.052803       0  \n",
       "4         1.000000       0  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OWN_CAR_AGE_ISNULL</th>\n",
       "      <th>EXT_SOURCE_1_ISNULL</th>\n",
       "      <th>EXT_SOURCE_3_ISNULL</th>\n",
       "      <th>TOTALAREA_MODE_ISNULL</th>\n",
       "      <th>APARTMENTS_AVG_ISNULL</th>\n",
       "      <th>BASEMENTAREA_AVG_ISNULL</th>\n",
       "      <th>YEARS_BUILD_AVG_ISNULL</th>\n",
       "      <th>COMMONAREA_AVG_ISNULL</th>\n",
       "      <th>ELEVATORS_AVG_ISNULL</th>\n",
       "      <th>LANDAREA_AVG_ISNULL</th>\n",
       "      <th>...</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>WALLSMATERIAL_MODE</th>\n",
       "      <th>EMERGENCYSTATE_MODE</th>\n",
       "      <th>AMT_INCOME_TOTAL_LOG</th>\n",
       "      <th>DAYS_EMPLOYED_POSITIVE</th>\n",
       "      <th>CREDIT_TO_INCOME</th>\n",
       "      <th>CREDIT_TO_GOODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>Kindergarten</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>Stone, brick</td>\n",
       "      <td>No</td>\n",
       "      <td>11.813030</td>\n",
       "      <td>False</td>\n",
       "      <td>4.213333</td>\n",
       "      <td>1.2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>11.502875</td>\n",
       "      <td>False</td>\n",
       "      <td>2.250182</td>\n",
       "      <td>1.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>Transport: type 3</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>12.218495</td>\n",
       "      <td>False</td>\n",
       "      <td>3.275378</td>\n",
       "      <td>1.0528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>Panel</td>\n",
       "      <td>No</td>\n",
       "      <td>12.660328</td>\n",
       "      <td>False</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>12.100712</td>\n",
       "      <td>False</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OWN_CAR_AGE_ISNULL  EXT_SOURCE_1_ISNULL  EXT_SOURCE_3_ISNULL  \\\n",
       "0                True                False                False   \n",
       "1                True                False                False   \n",
       "2               False                 True                False   \n",
       "3                True                False                False   \n",
       "4               False                False                 True   \n",
       "\n",
       "   TOTALAREA_MODE_ISNULL  APARTMENTS_AVG_ISNULL  BASEMENTAREA_AVG_ISNULL  \\\n",
       "0                  False                  False                    False   \n",
       "1                   True                   True                     True   \n",
       "2                   True                   True                     True   \n",
       "3                  False                  False                    False   \n",
       "4                   True                   True                     True   \n",
       "\n",
       "   YEARS_BUILD_AVG_ISNULL  COMMONAREA_AVG_ISNULL  ELEVATORS_AVG_ISNULL  \\\n",
       "0                    True                   True                  True   \n",
       "1                    True                   True                  True   \n",
       "2                    True                   True                  True   \n",
       "3                   False                  False                 False   \n",
       "4                    True                   True                  True   \n",
       "\n",
       "   LANDAREA_AVG_ISNULL  ...  WEEKDAY_APPR_PROCESS_START  \\\n",
       "0                 True  ...                     TUESDAY   \n",
       "1                 True  ...                      FRIDAY   \n",
       "2                 True  ...                      MONDAY   \n",
       "3                False  ...                   WEDNESDAY   \n",
       "4                 True  ...                      FRIDAY   \n",
       "\n",
       "        ORGANIZATION_TYPE  FONDKAPREMONT_MODE  HOUSETYPE_MODE  \\\n",
       "0            Kindergarten       missing_value  block of flats   \n",
       "1           Self-employed       missing_value   missing_value   \n",
       "2       Transport: type 3       missing_value   missing_value   \n",
       "3  Business Entity Type 3    reg oper account  block of flats   \n",
       "4  Business Entity Type 3       missing_value   missing_value   \n",
       "\n",
       "   WALLSMATERIAL_MODE  EMERGENCYSTATE_MODE  AMT_INCOME_TOTAL_LOG  \\\n",
       "0        Stone, brick                   No             11.813030   \n",
       "1       missing_value        missing_value             11.502875   \n",
       "2       missing_value        missing_value             12.218495   \n",
       "3               Panel                   No             12.660328   \n",
       "4       missing_value        missing_value             12.100712   \n",
       "\n",
       "   DAYS_EMPLOYED_POSITIVE  CREDIT_TO_INCOME  CREDIT_TO_GOODS  \n",
       "0                   False          4.213333           1.2640  \n",
       "1                   False          2.250182           1.2376  \n",
       "2                   False          3.275378           1.0528  \n",
       "3                   False          5.000000           1.0000  \n",
       "4                   False          3.475000           1.0000  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 318.82 MB\n",
      "Memory usage after changing types 155.99 MB\n",
      "Memory usage before changing types 51.21 MB\n",
      "Memory usage after changing types 25.05 MB\n",
      "bureau_train.shape: (263491, 160)\n",
      "bureau_test.shape: (42320, 160)\n",
      "bureau_train.shape: (263491, 160)\n",
      "bureau_test.shape: (42320, 160)\n"
     ]
    }
   ],
   "source": [
    "bureau_train = load_csv(\"data/data_/bureau_agg_train.csv\")\n",
    "bureau_test = load_csv(\"data/data_/bureau_agg_test.csv\")\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)\n",
    "\n",
    "bureau_train, bureau_test = train_test_col_align(bureau_train, bureau_test)\n",
    "print(\"bureau_train.shape:\", bureau_train.shape)\n",
    "print(\"bureau_test.shape:\", bureau_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `bureau_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 216.93 MB\n",
      "Memory usage after changing types 107.36 MB\n",
      "Memory usage before changing types 99.52 MB\n",
      "Memory usage after changing types 49.25 MB\n",
      "bureau_balance_train.shape (92231, 294)\n",
      "bureau_balance_test.shape (42311, 294)\n",
      "bureau_balance_train.shape (92231, 294)\n",
      "bureau_balance_test.shape (42311, 294)\n"
     ]
    }
   ],
   "source": [
    "bureau_balance_train = load_csv(\"data/data_/bureau_balance_agg_train.csv\")\n",
    "bureau_balance_test = load_csv(\"data/data_/bureau_balance_agg_test.csv\")\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)\n",
    "\n",
    "bureau_balance_train, bureau_balance_test = train_test_col_align(bureau_balance_train, bureau_balance_test)\n",
    "print(\"bureau_balance_train.shape\", bureau_balance_train.shape)\n",
    "print(\"bureau_balance_test.shape\", bureau_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `previous_application`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 785.85 MB\n",
      "Memory usage after changing types 379.84 MB\n",
      "Memory usage before changing types 129.06 MB\n",
      "Memory usage after changing types 62.39 MB\n",
      "previous_application_train.shape (291057, 348)\n",
      "previous_application_test.shape (47800, 348)\n",
      "previous_application_train.shape (291057, 348)\n",
      "previous_application_test.shape (47800, 348)\n"
     ]
    }
   ],
   "source": [
    "previous_application_train = load_csv(\"data/data_/previous_application_agg_train.csv\")\n",
    "previous_application_test = load_csv(\"data/data_/previous_application_agg_test.csv\")\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)\n",
    "\n",
    "previous_application_train, previous_application_test = train_test_col_align(previous_application_train, \n",
    "                                                                             previous_application_test)\n",
    "\n",
    "print(\"previous_application_train.shape\", previous_application_train.shape)\n",
    "print(\"previous_application_test.shape\", previous_application_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `POS_CASH_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 1003.81 MB\n",
      "Memory usage after changing types 501.33 MB\n",
      "Memory usage before changing types 165.78 MB\n",
      "Memory usage after changing types 82.79 MB\n",
      "POS_CASH_balance_train.shape (286967, 439)\n",
      "POS_CASH_balance_test.shape (47392, 439)\n",
      "POS_CASH_balance_train.shape (286967, 439)\n",
      "POS_CASH_balance_test.shape (47392, 439)\n"
     ]
    }
   ],
   "source": [
    "POS_CASH_balance_train = load_csv(\"data/data_/POS_CASH_balance_agg_train.csv\")\n",
    "POS_CASH_balance_test = load_csv(\"data/data_/POS_CASH_balance_agg_test.csv\")\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)\n",
    "\n",
    "POS_CASH_balance_train, POS_CASH_balance_test = train_test_col_align(POS_CASH_balance_train, POS_CASH_balance_test)\n",
    "print(\"POS_CASH_balance_train.shape\", POS_CASH_balance_train.shape)\n",
    "print(\"POS_CASH_balance_test.shape\", POS_CASH_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `installments_payments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 1394.36 MB\n",
      "Memory usage after changing types 697.47 MB\n",
      "Memory usage before changing types 228.99 MB\n",
      "Memory usage after changing types 114.55 MB\n",
      "installments_payments_train.shape (289406, 604)\n",
      "installments_payments_test.shape (47529, 604)\n",
      "installments_payments_train.shape (289406, 604)\n",
      "installments_payments_test.shape (47529, 604)\n"
     ]
    }
   ],
   "source": [
    "installments_payments_train = load_csv(\"data/data_/installments_payments_agg_train.csv\")\n",
    "installments_payments_test = load_csv(\"data/data_/installments_payments_agg_test.csv\")\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)\n",
    "\n",
    "installments_payments_train, installments_payments_test = train_test_col_align(installments_payments_train, \n",
    "                                                                               installments_payments_test)\n",
    "print(\"installments_payments_train.shape\", installments_payments_train.shape)\n",
    "print(\"installments_payments_test.shape\", installments_payments_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `credit_card_balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 331.30 MB\n",
      "Memory usage after changing types 165.53 MB\n",
      "Memory usage before changing types 61.69 MB\n",
      "Memory usage after changing types 30.83 MB\n",
      "credit_card_balance_train.shape (77934, 534)\n",
      "credit_card_balance_test.shape (14513, 534)\n",
      "credit_card_balance_train.shape (77934, 534)\n",
      "credit_card_balance_test.shape (14513, 534)\n"
     ]
    }
   ],
   "source": [
    "credit_card_balance_train = load_csv(\"data/data_/credit_card_balance_agg_train.csv\")\n",
    "credit_card_balance_test = load_csv(\"data/data_/credit_card_balance_agg_test.csv\")\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)\n",
    "\n",
    "credit_card_balance_train, credit_card_balance_test = train_test_col_align(credit_card_balance_train, \n",
    "                                                                           credit_card_balance_test)\n",
    "print(\"credit_card_balance_train.shape\", credit_card_balance_train.shape)\n",
    "print(\"credit_card_balance_test.shape\", credit_card_balance_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging with BURE_\n",
      "Merging with BUBA_\n",
      "Merging with PRAP_\n",
      "Merging with POBA_\n",
      "Merging with INPA_\n",
      "Merging with CCBA_\n",
      "Merging with BURE_\n",
      "Merging with BUBA_\n",
      "Merging with PRAP_\n",
      "Merging with POBA_\n",
      "Merging with INPA_\n",
      "Merging with CCBA_\n",
      "merge_train.shape (307511, 2518)\n",
      "merge_test.shape (48744, 2517)\n",
      "merge_train.isnull().sum().sum() 218065886\n",
      "merge_test.isnull().sum().sum() 22803797\n",
      "merge_train.isnull().sum().sum() 0\n",
      "merge_test.isnull().sum().sum() 0\n",
      "merge_train.shape (307511, 2518)\n",
      "merge_test.shape (48744, 2517)\n",
      "Number of columns droped: 69\n",
      "Number of columns droped: 69\n",
      "merge_train.shape (307511, 2449)\n",
      "merge_test.shape (48744, 2448)\n",
      "merge_train.shape (307511, 2449)\n",
      "merge_test.shape (48744, 2448)\n"
     ]
    }
   ],
   "source": [
    "merge_train = merge_dataframes(application_train, \n",
    "                               [bureau_train, bureau_balance_train, \n",
    "                                previous_application_train, POS_CASH_balance_train,\n",
    "                                installments_payments_train, credit_card_balance_train], \n",
    "                               \"APPL_\", [\"BURE_\", \"BUBA_\", \"PRAP_\", \"POBA_\", \"INPA_\", \"CCBA_\"])\n",
    "\n",
    "merge_test = merge_dataframes(application_test, \n",
    "                              [bureau_test, bureau_balance_test, \n",
    "                               previous_application_test, POS_CASH_balance_test,\n",
    "                               installments_payments_test, credit_card_balance_test], \n",
    "                              \"APPL_\", [\"BURE_\", \"BUBA_\", \"PRAP_\", \"POBA_\", \"INPA_\", \"CCBA_\"])\n",
    "\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "print(\"merge_train.isnull().sum().sum()\", merge_train.isnull().sum().sum())\n",
    "print(\"merge_test.isnull().sum().sum()\", merge_test.isnull().sum().sum())\n",
    "\n",
    "imputer = MergeMissingImputer()\n",
    "imputer.fit(merge_train)\n",
    "merge_train = imputer.transform(merge_train)\n",
    "merge_test = imputer.transform(merge_test)\n",
    "\n",
    "print(\"merge_train.isnull().sum().sum()\", merge_train.isnull().sum().sum())\n",
    "print(\"merge_test.isnull().sum().sum()\", merge_test.isnull().sum().sum())\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "\n",
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(merge_train)\n",
    "merge_train = remover.transform(merge_train)\n",
    "merge_test = remover.transform(merge_test)\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)\n",
    "\n",
    "merge_train, merge_test = train_test_col_align(merge_train, merge_test, exclude_cols=[\"APPL_TARGET\"])\n",
    "print(\"merge_train.shape\", merge_train.shape)\n",
    "print(\"merge_test.shape\", merge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train.to_csv(\"data/data_/X_y_train.csv\", index=False)\n",
    "merge_test.to_csv(\"data/data_/X_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
