{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == \"object\") and (df[col].nunique() < df.shape[0]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "        elif set(df[col].unique()) == set([0, 1]):\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        elif df[col].dtype == int:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtypes(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type(val):\n",
    "    if type(val) == str:\n",
    "        return \"string\"\n",
    "    \n",
    "    if np.issubsctype(type(val), np.number):\n",
    "        return \"number\"\n",
    "    \n",
    "    if callable(val):\n",
    "        return \"function\"\n",
    "    \n",
    "    return str(type(val))\n",
    "\n",
    "\n",
    "class NumColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"median\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (float)}, impute values for some specific columns\n",
    "        :param default: str, function or float, value or function used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if self._specified_values is not None:\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) == \"number\", \"Impute value for \" + col + \" is not number.\"\n",
    "        \n",
    "        self._default = default\n",
    "        self._default_type = check_type(self._default)\n",
    "        if self._default_type not in [\"number\", \"string\", \"function\"]:\n",
    "            raise ValueError(\"Unsupported stat type \" + self._default_type)\n",
    "    \n",
    "    def _cal_imput_vals(self, df):\n",
    "        cat_cols = df.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        if len(cat_cols) > 0:\n",
    "            raise ValueError(\"There are non-number columns: \" + \", \".join(cat_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        if self._default_type == \"number\":\n",
    "            impute_values = {col: self._default for col in all_cols}\n",
    "            \n",
    "        elif self._default_type == \"string\":\n",
    "            impute_values = getattr(df, self._default)()\n",
    "        \n",
    "        elif self._default_type == \"function\":\n",
    "            impute_values = df.apply(self._default)\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        impute_values = dict(impute_values)\n",
    "        if self._specified_values is None:\n",
    "            return impute_values\n",
    "        \n",
    "        for col in self._specified_values:\n",
    "            impute_values[col] = self._specified_values[col]\n",
    "            \n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        for k, v in self._impute_values.items():\n",
    "            if np.isnan(v):\n",
    "                raise ValueError(\"One of the impute_values is NaN: \" + k)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df.fillna(self._impute_values)\n",
    "\n",
    "\n",
    "class CatColsImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, specified_values=None, default=\"missing_value\"):\n",
    "        \"\"\"\n",
    "        :param specified_values: dict {colname (str): val (str, float, function)}, \n",
    "                                 impute values for some specific columns\n",
    "        :param default: str, used for the remaining columns\n",
    "        \"\"\"\n",
    "        assert (specified_values is None) or isinstance(specified_values, \n",
    "                                                        dict), \"specified_values must be None or dict\"\n",
    "        \n",
    "        self._specified_values = specified_values\n",
    "        if self._specified_values is not None:\n",
    "            for col, val in self._specified_values.items():\n",
    "                assert check_type(val) in [\"string\", \n",
    "                                           \"function\"], \"Impute value for \" + col + \" is \" + check_type(val)\n",
    "        \n",
    "        self._default = default\n",
    "        assert check_type(self._default) == \"string\", \"default must be string\"\n",
    "        \n",
    "        \n",
    "    def _cal_imput_vals(self, df):\n",
    "        num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "        if len(num_cols) > 0:\n",
    "            raise ValueError(\"There are number columns: \" + \", \".join(num_cols))\n",
    "        \n",
    "        all_cols = df.columns.to_list()\n",
    "        impute_values = {col: self._default for col in all_cols}\n",
    "        if self._specified_values is None:\n",
    "            return impute_values\n",
    "        \n",
    "        for col, val in self._specified_values.items():\n",
    "            dtype = check_type(val)\n",
    "            if dtype == \"string\":\n",
    "                impute_values[col] = val\n",
    "            \n",
    "            elif dtype == \"function\":\n",
    "                impute_values[col] = val(df[col])\n",
    "            \n",
    "            else:\n",
    "                return None\n",
    "        return impute_values\n",
    "    \n",
    "    def fit(self, df):\n",
    "        impute_values = self._cal_imput_vals(df)\n",
    "        \n",
    "        cols_with_na = [col for col in df.columns if df[col].isnull().any()]\n",
    "        self._impute_values = {col: impute_values[col] for col in cols_with_na}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        for col, val in self._impute_values.items():\n",
    "            df_new[col] = df_new[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "            \n",
    "        return df_new\n",
    "\n",
    "    \n",
    "class CollinearColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold, col_regex=None):\n",
    "        \"\"\"\n",
    "        :param threshold: float in [0, 1], if two columns have correlation greater than threshold\n",
    "                          one of them will be removed\n",
    "        :param col_regex: str, regular expression to select columns\n",
    "        \"\"\"\n",
    "        self._threshold = threshold\n",
    "        self._col_regex = col_regex\n",
    "    \n",
    "    def _collinear_columns(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = df_sel[[col_i]].corrwith(df_sel[col_j]).values[0]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    def _collinear_columns_NOTUSED(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "        \n",
    "        corr_matr = df_sel.corr().abs()\n",
    "        upper_matr = corr_matr.where(np.triu(np.ones(corr_matr.shape), k=1).astype(np.bool))\n",
    "        collin_cols = [col for col in upper_matr.columns if (upper_matr[col] > threshold).any()]\n",
    "        return collin_cols\n",
    "    \n",
    "    def fit(self, df):\n",
    "        self._collin_cols = self._collinear_columns(df, self._threshold)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        all_cols = df.columns.to_list()\n",
    "        nonexist_cols = [col for col in self._collin_cols if col not in all_cols]\n",
    "        if len(nonexist_cols) > 0:\n",
    "            print(\"WARNING: These collinear cols to be droped do not exist in df:\", nonexist_cols)\n",
    "            \n",
    "        droped_col = [col for col in self._collin_cols if col in all_cols]\n",
    "        return df.drop(droped_col, axis=\"columns\")\n",
    "\n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, train_df):\n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        self._cat_cols_ohe = pd.get_dummies(df_cat).columns.to_list()\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop redundant classes which my be present in test_df\n",
    "        for col in df_cat.columns:\n",
    "            if col not in self._cat_cols_ohe:\n",
    "                df_cat = df_cat.drop([col], axis=\"columns\")\n",
    "        \n",
    "        # if some some colums are lacking in test but present in train, make them will all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        return pd.concat([df_num, df_cat], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex_cols(columns):\n",
    "    fat_cols = [\"_\".join([str(c) for c in flat_col]) for flat_col in columns.to_flat_index()]\n",
    "    return fat_cols\n",
    "\n",
    "\n",
    "def agg_num_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    cat_cols = df.select_dtypes([\"object\", \"bool\", \"category\"]).columns.to_list()\n",
    "    if len(cat_cols) > 0:\n",
    "        raise ValueError(\"There are non-number cols: \" + \", \".join(cat_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    df_agg = df_agg.reset_index()\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def agg_cat_cols(df, by_sers, stats):\n",
    "    assert type(by_sers) in [list, tuple], \"by_sers must be a list or tuple\"\n",
    "    assert type(stats) in [list, tuple], \"stats must be a list or tuple\"\n",
    "    \n",
    "    for ser in by_sers:\n",
    "        assert isinstance(ser, pd.Series), \"ser in by_sers must be Series\"\n",
    "        \n",
    "    num_cols = df.select_dtypes([\"number\"]).columns.to_list()\n",
    "    if len(num_cols) > 0:\n",
    "        raise ValueError(\"There are number cols: \" + \", \".join(num_cols))\n",
    "    \n",
    "    df_agg = df.groupby(by_sers).agg(stats)\n",
    "    df_agg.columns = flatten_multiindex_cols(df_agg.columns)\n",
    "    df_agg = df_agg.reset_index()\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(ser):\n",
    "    return ser.mode().values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 300.13 MB\n",
      "Memory usage after changing types 104.87 MB\n",
      "Memory usage before changing types 47.18 MB\n",
      "Memory usage after changing types 18.19 MB\n",
      "(307511, 122) (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "application_train = load_csv(\"data/download/application_train.csv\")\n",
    "application_test = load_csv(\"data/download/application_test.csv\")\n",
    "\n",
    "print(application_train.shape, application_test.shape)\n",
    "\n",
    "appl_train_key = application_train[\"SK_ID_CURR\"]\n",
    "appl_test_key = application_test[\"SK_ID_CURR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 233.43 MB\n",
      "Memory usage after changing types 101.27 MB\n",
      "(1716428, 17)\n"
     ]
    }
   ],
   "source": [
    "bureau = load_csv(\"data/download/bureau.csv\")\n",
    "print(bureau.shape)\n",
    "\n",
    "bureau_key = bureau[\"SK_ID_BUREAU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 655.20 MB\n",
      "Memory usage after changing types 245.70 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27299925, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_balance = load_csv(\"data/download/bureau_balance.csv\")\n",
    "bureau_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 494.38 MB\n",
      "Memory usage after changing types 162.02 MB\n",
      "(1670214, 37)\n"
     ]
    }
   ],
   "source": [
    "previous_application = load_csv(\"data/download/previous_application.csv\")\n",
    "print(previous_application.shape)\n",
    "\n",
    "previous_application_key = previous_application[\"SK_ID_PREV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 640.09 MB\n",
      "Memory usage after changing types 290.04 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10001358, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_CASH_balance = load_csv(\"data/download/POS_CASH_balance.csv\")\n",
    "POS_CASH_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 870.75 MB\n",
      "Memory usage after changing types 435.37 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13605401, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "installments_payments = load_csv(\"data/download/installments_payments.csv\")\n",
    "installments_payments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 706.62 MB\n",
      "Memory usage after changing types 341.79 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3840312, 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card_balance = load_csv(\"data/download/credit_card_balance.csv\")\n",
    "credit_card_balance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from `application_[train|test]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colnames_from_regex(df, regex_strings):\n",
    "    cols = []\n",
    "    for regex_str in regex_strings:\n",
    "        cols.extend(df.filter(regex=regex_str).columns.to_list())\n",
    "    return cols\n",
    "\n",
    "\n",
    "class ApplImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._regex_strings = [\"^APARTMENTS_\", \"^BASEMENTAREA_\", \"^YEARS_B\", \"^COMMONAREA_\", \n",
    "                               \"^ELEVATORS_\", \"^ENTRANCES_\", \"^FLOORS\", \"^LANDAREA_\", \"^LIVING\", \n",
    "                               \"^NONLIVING\", \"AMT_REQ_CREDIT_BUREAU_\"]\n",
    "        \n",
    "        self._spec_impt_vals_num = {\"OWN_CAR_AGE\": -1.,\n",
    "                                    \"EXT_SOURCE_1\": 0.,\n",
    "                                    \"EXT_SOURCE_3\": 0.,\n",
    "                                    \"TOTALAREA_MODE\": -1.}\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cols_imput_with_m1 = get_colnames_from_regex(df_train, self._regex_strings)\n",
    "        self._spec_impt_vals_num.update({col: -1. for col in cols_imput_with_m1})\n",
    "        \n",
    "        df_num = df_train.select_dtypes([\"number\"])\n",
    "        self._imputer_num = NumColsImputer(specified_values=self._spec_impt_vals_num, default=\"median\")\n",
    "        self._imputer_num.fit(df_num)\n",
    "        \n",
    "        df_cat = df_train.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        self._imputer_cat = CatColsImputer(specified_values=None, default=\"missing_value\")\n",
    "        self._imputer_cat.fit(df_cat)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        num_df = df.select_dtypes([\"number\"])\n",
    "        \n",
    "        isnull_df = num_df[[\"SK_ID_CURR\"]]\n",
    "        for col in self._spec_impt_vals_num:\n",
    "            isnull_df[col + \"_ISNULL\"] = num_df[col].isnull()\n",
    "            \n",
    "        isnull_df = isnull_df.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "        num_df = self._imputer_num.transform(num_df)\n",
    "        \n",
    "        # cat\n",
    "        cat_df = df.select_dtypes([\"object\", \"category\", \"bool\"])\n",
    "        cat_df = self._imputer_cat.transform(cat_df)\n",
    "        \n",
    "        return pd.concat([num_df, isnull_df, cat_df], axis=\"columns\")\n",
    "        \n",
    "\n",
    "class ApplNewColsAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df_train):\n",
    "        credit_to_income = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_INCOME_TOTAL\"]\n",
    "        self._cti_min = credit_to_income.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._cti_max = credit_to_income.replace(np.inf, np.nan).max() * 10.\n",
    "        \n",
    "        credit_to_goods = df_train[\"AMT_CREDIT\"] / df_train[\"AMT_GOODS_PRICE\"]\n",
    "        self._ctg_min = credit_to_goods.replace(-np.inf, np.nan).min() / 10.\n",
    "        self._ctg_max = credit_to_goods.replace(np.inf, np.nan).max() / 10.\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_new = df.copy()\n",
    "        df_new[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df_new[\"AMT_INCOME_TOTAL\"])\n",
    "        df_new[\"DAYS_EMPLOYED_POSITIVE\"] = df_new[\"DAYS_EMPLOYED\"] > 0\n",
    "        days_emp_max = df_new[\"DAYS_EMPLOYED\"].max()\n",
    "        if days_emp_max > 0:\n",
    "            df_new[\"DAYS_EMPLOYED\"] = df_new[\"DAYS_EMPLOYED\"].replace({days_emp_max: 100})\n",
    "        \n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_INCOME_TOTAL\"]\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(-np.inf, self._cti_min)\n",
    "        df_new[\"CREDIT_TO_INCOME\"] = df_new[\"CREDIT_TO_INCOME\"].replace(np.inf, self._cti_max)\n",
    "        \n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"AMT_CREDIT\"] / df_new[\"AMT_GOODS_PRICE\"]\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(-np.inf, self._ctg_min)\n",
    "        df_new[\"CREDIT_TO_GOODS\"] = df_new[\"CREDIT_TO_GOODS\"].replace(np.inf, self._ctg_max)\n",
    "        \n",
    "        return df_new\n",
    "        \n",
    "\n",
    "def add_cols_application(appl_df):\n",
    "    appl_df[\"DAYS_EMPLOYED_POSITIVE\"] = appl_df[\"DAYS_EMPLOYED\"] > 0\n",
    "    days_emp_max = appl_df[\"DAYS_EMPLOYED\"].max()\n",
    "    appl_df[\"DAYS_EMPLOYED\"] = appl_df[\"DAYS_EMPLOYED\"].replace({days_emp_max: 100})\n",
    "    appl_df[\"AMT_INCOME_TOTAL_LOG\"] = np.log(appl_df[\"AMT_INCOME_TOTAL\"])\n",
    "    \n",
    "    appl_df[\"CREDIT_TO_INCOME\"] = appl_df[\"AMT_CREDIT\"] / appl_df[\"AMT_INCOME_TOTAL\"]\n",
    "    noinf_min = appl_df[\"CREDIT_TO_INCOME\"].replace(-np.inf, np.nan).min() / 10.\n",
    "    noinf_max = appl_df[\"CREDIT_TO_INCOME\"].replace(np.inf, np.nan).max() * 10.\n",
    "    appl_df[\"CREDIT_TO_INCOME\"].replace(-np.inf, noinf_min)\n",
    "    appl_df[\"CREDIT_TO_INCOME\"].replace(np.inf, noinf_max)\n",
    "    \n",
    "    \n",
    "    appl_df[\"CREDIT_TO_GOODS\"] = appl_df[\"AMT_CREDIT\"] / appl_df[\"AMT_GOODS_PRICE\"]\n",
    "    noinf_min = appl_df[\"CREDIT_TO_GOODS\"].replace(-np.inf, np.nan).min() / 10.\n",
    "    noinf_max = appl_df[\"CREDIT_TO_GOODS\"].replace(np.inf, np.nan).max() * 10.\n",
    "    appl_df[\"CREDIT_TO_GOODS\"].replace(-np.inf, noinf_min)\n",
    "    appl_df[\"CREDIT_TO_GOODS\"].replace(np.inf, noinf_max)\n",
    "    return appl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = application_train.copy()\\n\\ndf[\"DAYS_EMPLOYED_POSITIVE\"] = df[\"DAYS_EMPLOYED\"] > 0\\ndf[\"DAYS_EMPLOYED\"] = df[\"DAYS_EMPLOYED\"].replace({days_emp_max: np.nan})\\ndf[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df[\"AMT_INCOME_TOTAL\"])\\n\\ndf[\"CREDIT_TO_INCOME\"] = df[\"AMT_CREDIT\"] / df[\"AMT_INCOME_TOTAL\"]\\ndf[\"CREDIT_TO_GOODS\"] = df[\"AMT_CREDIT\"] / df[\"AMT_GOODS_PRICE\"]\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "df = application_train.copy()\n",
    "\n",
    "df[\"DAYS_EMPLOYED_POSITIVE\"] = df[\"DAYS_EMPLOYED\"] > 0\n",
    "df[\"DAYS_EMPLOYED\"] = df[\"DAYS_EMPLOYED\"].replace({days_emp_max: np.nan})\n",
    "df[\"AMT_INCOME_TOTAL_LOG\"] = np.log(df[\"AMT_INCOME_TOTAL\"])\n",
    "\n",
    "df[\"CREDIT_TO_INCOME\"] = df[\"AMT_CREDIT\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "df[\"CREDIT_TO_GOODS\"] = df[\"AMT_CREDIT\"] / df[\"AMT_GOODS_PRICE\"]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    100002\n",
       "1    100003\n",
       "Name: SK_ID_CURR, dtype: int32"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(application_train.shape)\n",
    "application_train[\"SK_ID_CURR\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48744, 121)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    100001\n",
       "1    100005\n",
       "Name: SK_ID_CURR, dtype: int32"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(application_test.shape)\n",
    "application_test[\"SK_ID_CURR\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_imputer = ApplImputer()\n",
    "appl_imputer.fit(application_train)\n",
    "\n",
    "df_imput_train = appl_imputer.transform(application_train)\n",
    "df_imput_test = appl_imputer.transform(application_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307511, 174), (48744, 173))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imput_train.shape, df_imput_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollinearColumnRemover(threshold=None)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remover = CollinearColumnRemover(0.99, col_regex=\"_ISNULL$\")\n",
    "remover.fit(df_imput)\n",
    "#df_imput_remove_collin = remover.transform(df_imput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_imput_train[[\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"NAME_TYPE_SUITE\"]]\n",
    "df_test = df_imput_test[[\"NAME_CONTRACT_TYPE\", \"CODE_GENDER\", \"NAME_TYPE_SUITE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(df_imput_train)\n",
    "\n",
    "df_train_ohe = ohe.transform(df_imput_train)\n",
    "df_test_ohe = ohe.transform(df_imput_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 174)\n",
      "(307511, 304)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR\n",
       "0      100002\n",
       "1      100003"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_imput_train.shape)\n",
    "print(df_train_ohe.shape)\n",
    "df_train_ohe[[\"SK_ID_CURR\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48744, 173)\n",
      "(48744, 303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR\n",
       "0      100001\n",
       "1      100005"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_imput_test.shape)\n",
    "print(df_test_ohe.shape)\n",
    "df_test_ohe[[\"SK_ID_CURR\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.037001</td>\n",
       "      <td>0.032839</td>\n",
       "      <td>-1.649569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148887</td>\n",
       "      <td>-0.304556</td>\n",
       "      <td>0.527149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.365257</td>\n",
       "      <td>0.239204</td>\n",
       "      <td>0.295006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.424296</td>\n",
       "      <td>-1.536629</td>\n",
       "      <td>0.272708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.484289</td>\n",
       "      <td>0.653949</td>\n",
       "      <td>2.623585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.481497</td>\n",
       "      <td>0.987740</td>\n",
       "      <td>0.765191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.214164</td>\n",
       "      <td>-0.320809</td>\n",
       "      <td>-1.048682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.245094</td>\n",
       "      <td>-0.071874</td>\n",
       "      <td>-0.021049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.584548</td>\n",
       "      <td>-0.068395</td>\n",
       "      <td>-1.105471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001568</td>\n",
       "      <td>-0.440571</td>\n",
       "      <td>0.342614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -1.037001  0.032839 -1.649569\n",
       "1  0.148887 -0.304556  0.527149\n",
       "2  0.365257  0.239204  0.295006\n",
       "3 -0.424296 -1.536629  0.272708\n",
       "4 -1.484289  0.653949  2.623585\n",
       "5 -0.481497  0.987740  0.765191\n",
       "6  1.214164 -0.320809 -1.048682\n",
       "7 -0.245094 -0.071874 -0.021049\n",
       "8 -0.584548 -0.068395 -1.105471\n",
       "9  0.001568 -0.440571  0.342614"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(np.random.randn(10, 3))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.037001</td>\n",
       "      <td>0.032839</td>\n",
       "      <td>-1.649569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148887</td>\n",
       "      <td>-0.304556</td>\n",
       "      <td>0.527149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.365257</td>\n",
       "      <td>0.239204</td>\n",
       "      <td>0.295006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inf</td>\n",
       "      <td>-1.536629</td>\n",
       "      <td>0.272708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.484289</td>\n",
       "      <td>0.653949</td>\n",
       "      <td>2.623585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.481497</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.765191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.214164</td>\n",
       "      <td>-0.320809</td>\n",
       "      <td>-1.048682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.245094</td>\n",
       "      <td>-0.071874</td>\n",
       "      <td>-0.021049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.584548</td>\n",
       "      <td>-0.068395</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001568</td>\n",
       "      <td>-0.440571</td>\n",
       "      <td>0.342614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -1.037001  0.032839 -1.649569\n",
       "1  0.148887 -0.304556  0.527149\n",
       "2  0.365257  0.239204  0.295006\n",
       "3       inf -1.536629  0.272708\n",
       "4 -1.484289  0.653949  2.623585\n",
       "5 -0.481497      -inf  0.765191\n",
       "6  1.214164 -0.320809 -1.048682\n",
       "7 -0.245094 -0.071874 -0.021049\n",
       "8 -0.584548 -0.068395       inf\n",
       "9  0.001568 -0.440571  0.342614"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[3, 0] = np.inf\n",
    "data.loc[5, 1] = -np.inf\n",
    "data.loc[8, 2] = np.inf\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.214164\n",
       "1    0.653949\n",
       "2    2.623585\n",
       "dtype: float64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace(np.inf, np.nan).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.232949\n",
       "1        -inf\n",
       "2   -2.402992\n",
       "dtype: float64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
