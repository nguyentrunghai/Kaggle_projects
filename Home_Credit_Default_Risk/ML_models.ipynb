{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == \"object\") and (df[col].nunique() < df.shape[0]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        elif df[col].dtype == int:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtypes(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._mean = {col: df_train[col].mean() for col in num_cols}\n",
    "        self._std = {col: df_train[col].std() for col in num_cols}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        for col in self._mean:\n",
    "            if self._std[col] > 0:\n",
    "                df[col] = (df[col] - self._mean[col]) / self._std[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "            else:\n",
    "                print(\"WARNING: \" + col + \" has zero std.\")\n",
    "        if self._to_array:\n",
    "            return df.values\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def write_submit_csv(estimator, X_test, id_test, out):\n",
    "    \"\"\"\n",
    "    :param estimator: a sklearn estimator that has predict_proba() method\n",
    "    :param X_test: df or array\n",
    "    :param id_test: dataframe containing column \"SK_ID_CURR\"\n",
    "    :param out: str, csv output file name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    prob_test = estimator.predict_proba(X_test)[:, 1]\n",
    "    submit = id_test\n",
    "    submit[\"TARGET\"] = prob_test\n",
    "    submit.to_csv(out, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_lr(params_tuned, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                num_eval,\n",
    "                params_fixed=None,\n",
    "                rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20}\n",
    "    \n",
    "    def objective(params):\n",
    "        estimator = LogisticRegression(**params_fixed, **params)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        auc = roc_auc(estimator, X_val, y_val)\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    return trials, best_param\n",
    "\n",
    "\n",
    "\n",
    "def hyperopt_rf(params_tuned, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                num_eval, \n",
    "                params_fixed=None,\n",
    "                rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective(params):\n",
    "        estimator = RandomForestClassifier(**params_fixed, **params)\n",
    "        (estimator.get_params())\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        auc = roc_auc(estimator, X_val, y_val)\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "    \n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    return trials, best_param\n",
    "\n",
    "\n",
    "def hyperopt_xgb(params_tuned, \n",
    "                 X_train, y_train, \n",
    "                 X_val, y_val, \n",
    "                 num_eval, \n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective(params):\n",
    "        estimator = XGBClassifier(**params_fixed, **params)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        auc = roc_auc(estimator, X_val, y_val)\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "\n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    return trials, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def averaging_y_hat(submit_csv_files):\n",
    "    y_hats = [pd.read_csv(f) for f in submit_csv_files]\n",
    "    result = y_hats[0][[\"SK_ID_CURR\"]]\n",
    "    result[\"TARGET\"] = 0.\n",
    "    for y in y_hats:\n",
    "        result[\"TARGET\"] = result[\"TARGET\"] + y[\"TARGET\"]\n",
    "    \n",
    "    result[\"TARGET\"] = result[\"TARGET\"] / len(y_hats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = \"data/data1_\"\n",
    "SUB_DIR = \"data/submit_\"\n",
    "MODELS_DIR = \"data/models_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 2691.34 MB\n",
      "Memory usage after changing types 1346.90 MB\n",
      "Memory usage before changing types 426.22 MB\n",
      "Memory usage after changing types 213.30 MB\n",
      "X_train.shape (307511, 1101)\n",
      "X_test.shape (48744, 1100)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 1100)\n",
      "X_test.shape (48744, 1100)\n",
      "Memory usage before changing types 0.39 MB\n",
      "Memory usage after changing types 0.20 MB\n",
      "Elapsed Time 460.78423500061035\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_sel_xgb_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_sel_xgb_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "sk_id_test = load_csv(os.path.join(IN_DIR, \"sk_id_test.csv\"))\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 2691.34 MB\n",
      "Memory usage after changing types 1346.90 MB\n",
      "Memory usage before changing types 426.22 MB\n",
      "Memory usage after changing types 213.30 MB\n",
      "X_train.shape (307511, 1101)\n",
      "X_test.shape (48744, 1100)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 1100)\n",
      "X_test.shape (48744, 1100)\n",
      "Memory usage before changing types 0.39 MB\n",
      "Memory usage after changing types 0.20 MB\n",
      "Elapsed Time 460.2423806190491\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_sel_xgb_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_sel_xgb_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "sk_id_test = load_csv(os.path.join(IN_DIR, \"sk_id_test.csv\"))\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (307511, 1100)\n",
      "X_test.shape (48744, 1100)\n"
     ]
    }
   ],
   "source": [
    "scaler = Standardizer(to_array=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and validation sets for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((246008, 1100), (246008,), (61503, 1100), (61503,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=21083)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (not tuned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=100)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Logistic regression model on the train set: 0.78777\n"
     ]
    }
   ],
   "source": [
    "auc_lr_train = roc_auc(lr, X_train, y_train)\n",
    "print(\"AUC of Logistic regression model on the train set: %0.5f\" % auc_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Logistic regression model on the evaluation set: 0.77466\n"
     ]
    }
   ],
   "source": [
    "auc_lr_val = roc_auc(lr, X_val, y_val)\n",
    "print(\"AUC of Logistic regression model on the evaluation set: %0.5f\" % auc_lr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "write_submit_csv(lr, X_test, sk_id_test, os.path.join(SUB_DIR, \"lr_data1_sel_xgb_baseline.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:00:34<00:00, 726.91s/trial, best loss: -0.7750106132221244]\n",
      "Time elapsed: 3634.56129 s\n"
     ]
    }
   ],
   "source": [
    "params_lr = {\"C\": hp.loguniform('C', np.log(0.0001), np.log(10))}\n",
    "num_eval = 5\n",
    "\n",
    "trials_lr, best_params_lr = hyperopt_lr(params_lr, X_train, y_train, X_val, y_val, num_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr_best = LogisticRegression(**best_params_lr)\n",
    "lr_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"lr_data1_sel_xgb_tuned.csv\")\n",
    "write_submit_csv(lr_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"lr_data1_sel_xgb_tuned.pickle\")\n",
    "pickle.dump(lr_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_leaf=40, n_estimators=1000, n_jobs=16,\n",
       "                       random_state=21083)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, min_samples_leaf=40, n_jobs=16, random_state=21083)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Random Forest model on the train set: 0.93251\n"
     ]
    }
   ],
   "source": [
    "auc_rf_train = roc_auc(rf, X_train, y_train)\n",
    "print(\"AUC of Random Forest model on the train set: %0.5f\" % auc_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Random Forest model on the evaluation set: 0.75218\n"
     ]
    }
   ],
   "source": [
    "auc_rf_val = roc_auc(rf, X_val, y_val)\n",
    "print(\"AUC of Random Forest model on the evaluation set: %0.5f\" % auc_rf_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [2:04:55<00:00, 124.93s/trial, best loss: -0.7583419783402278]  \n",
      "Time elapsed: 7495.69767 s\n"
     ]
    }
   ],
   "source": [
    "params_rf = {\n",
    "    \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 20, 400, 1)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 10, 200, 1)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 10, 200, 1)),\n",
    "}\n",
    "\n",
    "params_fixed_rf = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 100\n",
    "}\n",
    "\n",
    "\n",
    "num_eval = 60\n",
    "\n",
    "trials_rf, best_params_rf = hyperopt_rf(params_rf, \n",
    "                                        X_train, y_train, X_val, y_val, \n",
    "                                        num_eval,\n",
    "                                        params_fixed=params_fixed_rf,\n",
    "                                        rstate=21083)\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 98, 'min_samples_leaf': 91, 'min_samples_split': 174}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_rf = {s: int(best_params_rf[s]) for s in best_params_rf}\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Random Forest model on the train set: 0.87597\n",
      "AUC of Random Forest model on the evaluation set: 0.75972\n"
     ]
    }
   ],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators=500, n_jobs=20, **best_params_rf)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "auc_rf_train = roc_auc(rf_best, X_train, y_train)\n",
    "print(\"AUC of Random Forest model on the train set: %0.5f\" % auc_rf_train)\n",
    "\n",
    "auc_rf_val = roc_auc(rf_best, X_val, y_val)\n",
    "print(\"AUC of Random Forest model on the evaluation set: %0.5f\" % auc_rf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"rf_data1_sel_xgb_tuned.csv\")\n",
    "write_submit_csv(rf_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"rf_data1_sel_xgb_tuned.pickle\")\n",
    "pickle.dump(rf_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBOOST model on the train set: 0.89709\n",
      "AUC of XGBOOST model on the validation set: 0.77105\n",
      "Time elapsed: 245.69354 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBOOST model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of XGBOOST model on the validation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [8:25:49<00:00, 151.75s/trial, best loss: -0.7882966092870476]  \n",
      "Time elapsed: 30349.15448 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.47671365847278224,\n",
       " 'learning_rate': 0.08050806097226641,\n",
       " 'max_depth': 7.0,\n",
       " 'min_child_weight': 5.0,\n",
       " 'reg_alpha': 0.0014314593128521287,\n",
       " 'reg_lambda': 529.4881903912216,\n",
       " 'subsample': 0.7017064695318056}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_xgb = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 8, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 14, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.2, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.2, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.0001), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(10000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(1)),\n",
    "#    \"gamma\": hp.uniform(\"gamma\", 0., 0.4),\n",
    "}\n",
    "\n",
    "params_fixed_xgb = {\n",
    "    \"tree_method\": \"gpu_hist\" ,\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "trials_xgb, best_params_xgb = hyperopt_xgb(params_xgb, \n",
    "                                           X_train, y_train, X_val, y_val, \n",
    "                                           num_eval,\n",
    "                                           params_fixed=params_fixed_xgb,\n",
    "                                           rstate=42)\n",
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'colsample_bytree': 0.47671365847278224,\n",
    " 'learning_rate': 0.08050806097226641,\n",
    " 'max_depth': 7.0,\n",
    " 'min_child_weight': 5.0,\n",
    " 'reg_alpha': 0.0014314593128521287,\n",
    " 'reg_lambda': 529.4881903912216,\n",
    " 'subsample': 0.7017064695318056}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.47671365847278224,\n",
       " 'learning_rate': 0.08050806097226641,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 5,\n",
       " 'reg_alpha': 0.0014314593128521287,\n",
       " 'reg_lambda': 529.4881903912216,\n",
       " 'subsample': 0.7017064695318056}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_xgb[\"max_depth\"] = int(best_params_xgb[\"max_depth\"])\n",
    "best_params_xgb[\"min_child_weight\"] = int(best_params_xgb[\"min_child_weight\"])\n",
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBoost model on the train set: 0.86774\n",
      "AUC of XGBoost model on the evaluation set: 0.78830\n"
     ]
    }
   ],
   "source": [
    "xgb_best = XGBClassifier(n_estimators=500, tree_method=\"gpu_hist\", **best_params_xgb)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb_best, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb_best, X_val, y_val)\n",
    "print(\"AUC of XGBoost model on the evaluation set: %0.5f\" % auc_xgb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned_01.csv\")\n",
    "write_submit_csv(xgb_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data1_sel_xgb_tuned_01.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBoost model on the train set: 0.85349\n",
      "AUC of XGBoost model on the evaluation set: 0.78812\n"
     ]
    }
   ],
   "source": [
    "# try different gamma\n",
    "best_params_xgb = {\"colsample_bytree\": 0.47671365847278224,\n",
    "                   \"learning_rate\": 0.08050806097226641,\n",
    "                   \"max_depth\": 7,\n",
    "                   \"min_child_weight\": 5,\n",
    "                   \"reg_alpha\": 0.0014314593128521287,\n",
    "                   \"reg_lambda\": 529.4881903912216,\n",
    "                   \"subsample\": 0.7017064695318056,\n",
    "                   \"gamma\": 2.}\n",
    "\n",
    "xgb_best = XGBClassifier(n_estimators=500, tree_method=\"gpu_hist\", **best_params_xgb)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb_best, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb_best, X_val, y_val)\n",
    "print(\"AUC of XGBoost model on the evaluation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "\n",
    "xgb_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned_01.csv\")\n",
    "write_submit_csv(xgb_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data1_sel_xgb_tuned_01.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned_02.csv\")\n",
    "write_submit_csv(xgb_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data1_sel_xgb_tuned_02.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [12:38:34<00:00, 227.57s/trial, best loss: -0.7882748627352448]  \n",
      "Time elapsed: 45514.27978 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8617867706122423,\n",
       " 'learning_rate': 0.058376323447823765,\n",
       " 'max_depth': 8.0,\n",
       " 'min_child_weight': 1.0,\n",
       " 'reg_alpha': 0.6654125785836109,\n",
       " 'reg_lambda': 450.87992024647934,\n",
       " 'subsample': 0.6704436939093745}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_xgb = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 12, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 14, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.2, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.2, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.0001), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(10000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.005), np.log(1)),\n",
    "#    \"gamma\": hp.uniform(\"gamma\", 0., 0.4),\n",
    "}\n",
    "\n",
    "params_fixed_xgb = {\n",
    "    \"tree_method\": \"gpu_hist\" ,\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "trials_xgb, best_params_xgb = hyperopt_xgb(params_xgb, \n",
    "                                           X_train, y_train, X_val, y_val, \n",
    "                                           num_eval,\n",
    "                                           params_fixed=params_fixed_xgb,\n",
    "                                           rstate=42)\n",
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'colsample_bytree': 0.8617867706122423,\n",
    " 'learning_rate': 0.058376323447823765,\n",
    " 'max_depth': 8.0,\n",
    " 'min_child_weight': 1.0,\n",
    " 'reg_alpha': 0.6654125785836109,\n",
    " 'reg_lambda': 450.87992024647934,\n",
    " 'subsample': 0.6704436939093745}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBoost model on the train set: 0.87398\n",
      "AUC of XGBoost model on the evaluation set: 0.78794\n"
     ]
    }
   ],
   "source": [
    "best_params_xgb = {\"colsample_bytree\": 0.8617867706122423,\n",
    "                   \"learning_rate\": 0.058376323447823765,\n",
    "                   \"max_depth\": 8,\n",
    "                   \"min_child_weight\": 1,\n",
    "                   \"reg_alpha\": 0.6654125785836109,\n",
    "                   \"reg_lambda\": 450.87992024647934,\n",
    "                   \"subsample\": 0.6704436939093745,}\n",
    "\n",
    "xgb_best = XGBClassifier(n_estimators=500, tree_method=\"gpu_hist\", **best_params_xgb)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb_best, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb_best, X_val, y_val)\n",
    "print(\"AUC of XGBoost model on the evaluation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "\n",
    "xgb_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned_03.csv\")\n",
    "write_submit_csv(xgb_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data1_sel_xgb_tuned_03.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBoost model on the train set: 0.85323\n",
      "AUC of XGBoost model on the evaluation set: 0.78810\n"
     ]
    }
   ],
   "source": [
    "# try different gamma\n",
    "\n",
    "best_params_xgb = {\"colsample_bytree\": 0.8617867706122423,\n",
    "                   \"learning_rate\": 0.058376323447823765,\n",
    "                   \"max_depth\": 8,\n",
    "                   \"min_child_weight\": 1,\n",
    "                   \"reg_alpha\": 0.6654125785836109,\n",
    "                   \"reg_lambda\": 450.87992024647934,\n",
    "                   \"subsample\": 0.6704436939093745,\n",
    "                   \"gamma\": 2.5}\n",
    "\n",
    "xgb_best = XGBClassifier(n_estimators=500, tree_method=\"gpu_hist\", **best_params_xgb)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb_best, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb_best, X_val, y_val)\n",
    "print(\"AUC of XGBoost model on the evaluation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "\n",
    "xgb_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned_04.csv\")\n",
    "write_submit_csv(xgb_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data1_sel_xgb_tuned_04.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_csv_files = glob.glob(os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned*csv\"))\n",
    "\n",
    "averg_pred = averaging_y_hat(sub_csv_files)\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data1_sel_xgb_tuned_ensemble.csv\")\n",
    "averg_pred.to_csv(out_sub, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
