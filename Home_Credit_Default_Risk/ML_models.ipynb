{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser):\n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == \"object\":\n",
    "        if ser.nunique() < ser.shape[0]:\n",
    "            return ser.astype(\"category\")\n",
    "        else:\n",
    "            raise TypeError(ser.name + \": type is object but are all distinct\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "\n",
    "def change_dtype_df(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col])\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._mean = {col: df_train[col].mean() for col in num_cols}\n",
    "        self._std = {col: df_train[col].std() for col in num_cols}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        for col in self._mean:\n",
    "            if self._std[col] > 0:\n",
    "                df[col] = (df[col] - self._mean[col]) / self._std[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "            else:\n",
    "                print(\"WARNING: \" + col + \" has zero std.\")\n",
    "                df[col] = df[col] - self._mean[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "                \n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        \n",
    "        self._cat_col_idx = [i for i, col in enumerate(self._train_cols) if col in cat_cols]\n",
    "        \n",
    "        self._label_maps = {}\n",
    "        self._missing_imputers = {}\n",
    "        for col in cat_cols:\n",
    "            label = df_train[col].unique()\n",
    "            self._label_maps[col] = {c: n for n, c in enumerate(label)}\n",
    "            \n",
    "            mode_label = df_train[col].mode().iloc[0]\n",
    "            self._missing_imputers[col] = self._label_maps[col][mode_label]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"do not have the same set of columns as train\"\n",
    "        \n",
    "        for col, label_map in self._label_maps.items():\n",
    "            df[col] = df[col].map(label_map)\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].astype(np.float32).fillna(self._missing_imputers[col])\n",
    "        \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        \n",
    "        self._features = df.columns.to_list()\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def get_cat_cols(self):\n",
    "        return self._cat_col_idx\n",
    "    \n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self._cols_before = train_df.columns.to_list()\n",
    "        \n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        \n",
    "        num_cols = [col for col in train_df.columns if col not in self._cat_cols]\n",
    "        self._cols_after = num_cols + self._cat_cols_ohe\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        cols_before = df.columns.to_list()\n",
    "        assert set(cols_before) == set(self._cols_before), \"Do not have the same columns as train before transformed\"\n",
    "        \n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df\n",
    "        \n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        cols_to_drop = [col for col in df_cat.columns if col not in self._cat_cols_ohe]\n",
    "        df_cat = df_cat.drop(cols_to_drop, axis=\"columns\")\n",
    "        \n",
    "        # change to float32\n",
    "        for col in df_cat.columns:\n",
    "            df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        cols_after = num_cols + df_cat.columns.to_list()\n",
    "        assert set(cols_after) == set(self._cols_after), \"Do not have the same columns as train after transformed\"\n",
    "        \n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        df = pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "        # align columns\n",
    "        df = df[self._cols_after]\n",
    "        \n",
    "        self._features = df.columns.to_list()\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def write_submit_csv(estimator, X_test, id_test, out):\n",
    "    \"\"\"\n",
    "    :param estimator: a sklearn estimator that has predict_proba() method\n",
    "    :param X_test: df or array\n",
    "    :param id_test: dataframe containing column \"SK_ID_CURR\"\n",
    "    :param out: str, csv output file name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    prob_test = estimator.predict_proba(X_test)[:, 1]\n",
    "    submit = id_test.copy()\n",
    "    submit[\"TARGET\"] = prob_test\n",
    "    submit.to_csv(out, index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "def feature_importance_df(estimator, features):\n",
    "    \"\"\"\n",
    "    :param estimator: an estimator object that has feature_importances_ attribute\n",
    "    :param features: list of str, list of feature names\n",
    "    :return: feature_imp, dataframe\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\"feature\": features, \"importance\": estimator.feature_importances_})\n",
    "    feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "    \n",
    "    feature_imp[\"rank\"] = np.arange(feature_imp.shape[0]) + 1\n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperopt(classifier,\n",
    "                 params_tuned, \n",
    "                 X_train, y_train,\n",
    "                 X_val, y_val,\n",
    "                 num_eval,\n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        auc = roc_auc(classifier, X_val, y_val)\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    best_model = classifier.set_params(**params_fixed, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    \n",
    "    return trials, best_params, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def averaging_y_hat(submit_csv_files):\n",
    "    y_hats = [pd.read_csv(f) for f in submit_csv_files]\n",
    "    result = y_hats[0][[\"SK_ID_CURR\"]]\n",
    "    result[\"TARGET\"] = 0.\n",
    "    for y in y_hats:\n",
    "        result[\"TARGET\"] = result[\"TARGET\"] + y[\"TARGET\"]\n",
    "    \n",
    "    result[\"TARGET\"] = result[\"TARGET\"] / len(y_hats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same_cols(df1, df2):\n",
    "    cols1 = df1.columns.to_list()\n",
    "    cols2 = df2.columns.to_list()\n",
    "    \n",
    "    assert set(cols1) == set(cols2)\n",
    "    for c1, c2 in zip(cols1, cols2):\n",
    "        assert c1 == c2\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = \"data/data_\"\n",
    "SUB_DIR = \"data/submit_\"\n",
    "MODELS_DIR = \"data/models_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "(246008, 2364) (246008,) (61503, 2364) (61503,)\n",
      "Time elapsed: 1753.14431 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "scaler = Standardizer(to_array=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=40293)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=100, n_jobs=20)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.79685\n",
      "AUC of the validation set: 0.78364\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(lr, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(lr, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "lr.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# Submitting this give public AUC = 0.77271 and private AUC = 0.77530\n",
    "write_submit_csv(lr, X_test, sk_id_test, os.path.join(SUB_DIR, \"lr.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "(246008, 2364) (246008,) (61503, 2364) (61503,)\n",
      "Time elapsed: 1872.57600 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(to_array=True)\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=40293)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:50:24<00:00, 66.24s/trial, best loss: -0.7625559894891252]\n",
      "Time elapsed: 6698.88609 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10,\n",
       " 'max_features': 83,\n",
       " 'min_samples_leaf': 110,\n",
       " 'min_samples_split': 20}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 20, 1)),\n",
    "    \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 20, 400, 10)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 20, 200, 10)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 10, 200, 1)),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 100\n",
    "}\n",
    "\n",
    "\n",
    "num_eval = 100\n",
    "rf = RandomForestClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(rf, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'max_depth': 10,\n",
    " 'max_features': 83,\n",
    " 'min_samples_leaf': 110,\n",
    " 'min_samples_split': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.79322\n",
      "AUC of the validation set: 0.76269\n"
     ]
    }
   ],
   "source": [
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"rf.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))\n",
    "\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"rf.csv\")\n",
    "# Submitting this give public AUC = 0.74792 and private AUC = 0.74145\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "(246008, 2364) (246008,) (61503, 2364) (61503,)\n",
      "Time elapsed: 1850.51061 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(to_array=True)\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=40293)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [20:38:48<00:00, 371.64s/trial, best loss: -0.7986753448393237]   \n",
      "Time elapsed: 74747.66016 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5921658797363964,\n",
       " 'gamma': 2.81393985204685,\n",
       " 'learning_rate': 0.037238669538423745,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 8,\n",
       " 'reg_alpha': 13.49762560054903,\n",
       " 'reg_lambda': 3.7989514085640046,\n",
       " 'subsample': 0.828082657230721}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 12, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 20, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(xgb, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5921658797363964,\n",
       " 'gamma': 2.81393985204685,\n",
       " 'learning_rate': 0.037238669538423745,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 8,\n",
       " 'reg_alpha': 13.49762560054903,\n",
       " 'reg_lambda': 3.7989514085640046,\n",
       " 'subsample': 0.828082657230721}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'colsample_bytree': 0.5921658797363964,\n",
    " 'gamma': 2.81393985204685,\n",
    " 'learning_rate': 0.037238669538423745,\n",
    " 'max_depth': 7,\n",
    " 'min_child_weight': 8,\n",
    " 'reg_alpha': 13.49762560054903,\n",
    " 'reg_lambda': 3.7989514085640046,\n",
    " 'subsample': 0.828082657230721}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.87957\n",
      "AUC of the validation set: 0.79802\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_ohe.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))\n",
    "\n",
    "\n",
    "# Submitting this give public AUC = 0.79186 and private AUC = 0.79108\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_ohe.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "(246008, 2044) (246008,) (61503, 2044) (61503,)\n",
      "Time elapsed: 1341.80533 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "X_train = le.fit_transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "cat_col_idx = le.get_cat_cols()\n",
    "features = le._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=52094)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [18:41:32<00:00, 336.46s/trial, best loss: -0.7901599783150772]  \n",
      "Time elapsed: 67704.15255 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6701944459124302,\n",
       " 'learning_rate': 0.09002824038085307,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 8,\n",
       " 'reg_alpha': 1.1615938969390502,\n",
       " 'reg_lambda': 1488.7791790041074,\n",
       " 'subsample': 0.9508741452720249}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 12, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 14, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.001), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(xgb, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6701944459124302,\n",
       " 'learning_rate': 0.09002824038085307,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 8,\n",
       " 'reg_alpha': 1.1615938969390502,\n",
       " 'reg_lambda': 1488.7791790041074,\n",
       " 'subsample': 0.9508741452720249}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'colsample_bytree': 0.6701944459124302,\n",
    " 'learning_rate': 0.09002824038085307,\n",
    " 'max_depth': 8,\n",
    " 'min_child_weight': 8,\n",
    " 'reg_alpha': 1.1615938969390502,\n",
    " 'reg_lambda': 1488.7791790041074,\n",
    " 'subsample': 0.9508741452720249}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.87326\n",
      "AUC of the validation set: 0.78880\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_le.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))\n",
    "\n",
    "# Submitting this give public AUC = 0.79088 and private AUC = 0.79006\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_le.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "(246008, 2044) (246008,) (61503, 2044) (61503,)\n",
      "Time elapsed: 1457.93122 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "X_train = le.fit_transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "cat_col_idx = le.get_cat_cols()\n",
    "features = le._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=16039)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [5:09:48<00:00, 92.94s/trial, best loss: -0.7950186734642588]   \n",
      "Time elapsed: 18653.15159 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.4661559641143767,\n",
       " 'learning_rate': 0.05414702094866219,\n",
       " 'max_depth': 7,\n",
       " 'min_child_samples': 390,\n",
       " 'num_leaves': 75,\n",
       " 'reg_alpha': 0.2621812996397479,\n",
       " 'reg_lambda': 191.60321812828326,\n",
       " 'subsample': 0.6990613159642727}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 10, 200, 5)),\n",
    "    \"min_child_samples\": scope.int(hp.quniform(\"min_child_samples\", 10, 500, 10)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.1), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.005), np.log(1)),\n",
    "}\n",
    "\n",
    "# categorical_feature\n",
    "params_fixed = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"device\": \"gpu\" ,\n",
    "    \"n_estimators\": 500,\n",
    "    \"categorical_feature\": cat_col_idx\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(lgbm, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.87621\n",
      "AUC of the validation set: 0.79502\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"lgb_le.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))\n",
    "\n",
    "# Submitting this give public AUC = 0.79186 and private AUC = 0.79108\n",
    "out_sub = os.path.join(SUB_DIR, \"lgbm_le.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
