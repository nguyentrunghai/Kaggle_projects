{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser):\n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == \"object\":\n",
    "        if ser.nunique() < ser.shape[0]:\n",
    "            return ser.astype(\"category\")\n",
    "        else:\n",
    "            raise TypeError(ser.name + \": type is object but are all distinct\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "\n",
    "def change_dtype_df(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col])\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._mean = {col: df_train[col].mean() for col in num_cols}\n",
    "        self._std = {col: df_train[col].std() for col in num_cols}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        for col in self._mean:\n",
    "            if self._std[col] > 0:\n",
    "                df[col] = (df[col] - self._mean[col]) / self._std[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "            else:\n",
    "                print(\"WARNING: \" + col + \" has zero std.\")\n",
    "                df[col] = df[col] - self._mean[col]\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "                \n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        all_cols = df_train.columns.to_list()\n",
    "        cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        \n",
    "        self._cat_col_idx = [i for i, col in enumerate(all_cols) if col in cat_cols]\n",
    "        \n",
    "        self._label_maps = {}\n",
    "        self._missing_imputers = {}\n",
    "        for col in cat_cols:\n",
    "            label = df_train[col].unique()\n",
    "            self._label_maps[col] = {c: n for n, c in enumerate(label)}\n",
    "            \n",
    "            mode_label = df_train[col].mode().iloc[0]\n",
    "            self._missing_imputers[col] = self._label_maps[col][mode_label]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        for col, label_map in self._label_maps.items():\n",
    "            df[col] = df[col].map(label_map)\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].astype(np.float32).fillna(self._missing_imputers[col])\n",
    "                \n",
    "        self._features = df.columns.to_list()\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def get_cat_cols(self):\n",
    "        return self._cat_col_idx\n",
    "    \n",
    "\n",
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        \n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df\n",
    "        \n",
    "        df_cat = df.select_dtypes([\"object\", \"category\"])\n",
    "        cat_cols = df_cat.columns.to_list()\n",
    "        assert set(cat_cols) == set(self._cat_cols), \"df does not have the same categorical cols as train_df\"\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        cols_to_drop = [col for col in df_cat.columns if col not in self._cat_cols_ohe]\n",
    "        df_cat = df_cat.drop(cols_to_drop, axis=\"columns\")\n",
    "        \n",
    "        # change to float32\n",
    "        for col in df_cat.columns:\n",
    "            df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        df = pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "        self._features = df.columns.to_list()\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def write_submit_csv(estimator, X_test, id_test, out):\n",
    "    \"\"\"\n",
    "    :param estimator: a sklearn estimator that has predict_proba() method\n",
    "    :param X_test: df or array\n",
    "    :param id_test: dataframe containing column \"SK_ID_CURR\"\n",
    "    :param out: str, csv output file name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    prob_test = estimator.predict_proba(X_test)[:, 1]\n",
    "    submit = id_test.copy()\n",
    "    submit[\"TARGET\"] = prob_test\n",
    "    submit.to_csv(out, index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "def feature_importance_df(estimator, features):\n",
    "    \"\"\"\n",
    "    :param estimator: an estimator object that has feature_importances_ attribute\n",
    "    :param features: list of str, list of feature names\n",
    "    :return: feature_imp, dataframe\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\"feature\": features, \"importance\": estimator.feature_importances_})\n",
    "    feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "    \n",
    "    feature_imp[\"rank\"] = np.arange(feature_imp.shape[0]) + 1\n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperopt(classifier,\n",
    "                 params_tuned, \n",
    "                 X_train, y_train,\n",
    "                 X_val, y_val,\n",
    "                 num_eval,\n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        auc = roc_auc(classifier, X_val, y_val)\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    best_model = classifier.set_params(**params_fixed, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    \n",
    "    return trials, best_params, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def averaging_y_hat(submit_csv_files):\n",
    "    y_hats = [pd.read_csv(f) for f in submit_csv_files]\n",
    "    result = y_hats[0][[\"SK_ID_CURR\"]]\n",
    "    result[\"TARGET\"] = 0.\n",
    "    for y in y_hats:\n",
    "        result[\"TARGET\"] = result[\"TARGET\"] + y[\"TARGET\"]\n",
    "    \n",
    "    result[\"TARGET\"] = result[\"TARGET\"] / len(y_hats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = \"data/data5_\"\n",
    "SUB_DIR = \"data/submit_\"\n",
    "MODELS_DIR = \"data/models_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 6478.33 MB\n",
      "Memory usage after changing types 3193.52 MB\n",
      "Memory usage before changing types 1026.50 MB\n",
      "Memory usage after changing types 506.03 MB\n",
      "X_org_train.shape (307511, 2657)\n",
      "X_org_test.shape (48744, 2656)\n",
      "X_org_train.isnull().sum().sum: 0\n",
      "X_org_test.isnull().sum().sum: 0\n",
      "X_org_train.shape (307511, 2655)\n",
      "X_org_test.shape (48744, 2655)\n",
      "Elapsed Time 2182.02224111557\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_org_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_org_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_org_train.shape\", X_org_train.shape)\n",
    "print(\"X_org_test.shape\", X_org_test.shape)\n",
    "print(\"X_org_train.isnull().sum().sum:\", X_org_train.isnull().sum().sum())\n",
    "print(\"X_org_test.isnull().sum().sum:\", X_org_test.isnull().sum().sum())\n",
    "\n",
    "y_org_train = X_org_train[\"APPL_TARGET\"].values\n",
    "X_org_train = X_org_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_org_train.shape\", X_org_train.shape)\n",
    "\n",
    "sk_id_test = X_org_test[[\"SK_ID_CURR\"]]\n",
    "X_org_test = X_org_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_org_test.shape\", X_org_test.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Elapsed Time\", time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_org_train)\n",
    "X_train = ohe.transform(X_org_train)\n",
    "X_test = ohe.transform(X_org_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "scaler = Standardizer(to_array=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_org_train, test_size=0.2, \n",
    "                                                  stratify=y_org_train, random_state=21083)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (not tuned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=100, n_jobs=20)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc(lr, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(lr, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "write_submit_csv(lr, X_test, sk_id_test, os.path.join(SUB_DIR, \"lr_data3_baseline.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"C\": hp.loguniform('C', np.log(0.00001), np.log(100))}\n",
    "num_eval = 10\n",
    "\n",
    "lr = LogisticRegression()\n",
    "trials, best_params, best_model = run_hyperopt(lr, params, X_train, y_train, X_val, y_val, num_eval)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sub = os.path.join(SUB_DIR, \"lr_data3_tuned.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"lr_data3_tuned.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=20)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc(rf, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_rf_train)\n",
    "\n",
    "auc_val = roc_auc(rf, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    #\"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 20, 400, 10)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 20, 200, 10)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 10, 200, 1)),\n",
    "}\n",
    "\n",
    "params_fixed_rf = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 100\n",
    "}\n",
    "\n",
    "\n",
    "num_eval = 60\n",
    "rf = RandomForestClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(rf, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"rf_data3_tuned.csv\")\n",
    "write_submit_csv(rf_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"rf_data3_tuned.pickle\")\n",
    "pickle.dump(rf_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "Time elapsed: 1816.44950 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(to_array=True)\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=30192)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of XGBOOST model on the train set: 0.91061\n",
      "AUC of XGBOOST model on the validation set: 0.76960\n",
      "Time elapsed: 99.20997 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [25:50:23<00:00, 465.12s/trial, best loss: -0.7910676942424485]   \n",
      "Time elapsed: 93494.71568 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8277609897889766,\n",
       " 'learning_rate': 0.23944672331396236,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 12,\n",
       " 'reg_lambda': 5589.365573544125,\n",
       " 'subsample': 0.941505700131026}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 16, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10000)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(100)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(0.5)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 2.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(xgb, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.86169\n",
      "AUC of the validation set: 0.79121\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data5_ohe_tuned_01.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data5_ohe_tuned_01.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "(246008, 2364) (246008,) (61503, 2364) (61503,)\n",
      "Time elapsed: 1911.26998 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(to_array=True)\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=60392)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [24:58:07<00:00, 449.44s/trial, best loss: -0.7957121763270056]  \n",
      "Time elapsed: 90352.94079 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.727760707935723,\n",
       " 'gamma': 1.8480869489155598,\n",
       " 'learning_rate': 0.10043251062182912,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 7,\n",
       " 'reg_alpha': 0.007754632616862692,\n",
       " 'reg_lambda': 1232.6160000051507,\n",
       " 'subsample': 0.886220423354186}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 12, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 20, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(xgb, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.86691\n",
      "AUC of the validation set: 0.79492\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data5_ohe_tuned_02.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data5_ohe_tuned_02.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2364)\n",
      "X_test.shape (48744, 2364)\n",
      "(246008, 2364) (246008,) (61503, 2364) (61503,)\n",
      "Time elapsed: 1808.58785 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(to_array=True)\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "features = ohe._features\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=26802)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [21:17:03<00:00, 383.12s/trial, best loss: -0.7926602920717406]  \n",
      "Time elapsed: 76942.01761 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5191534522752808,\n",
       " 'gamma': 4.623319349053346,\n",
       " 'learning_rate': 0.10403041813785364,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 18,\n",
       " 'reg_alpha': 3.264299697048531,\n",
       " 'reg_lambda': 452.41246910139256,\n",
       " 'subsample': 0.9181638806764917}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 12, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 20, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.001), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(xgb, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.84528\n",
      "AUC of the validation set: 0.79053\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data5_ohe_tuned_04.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data5_ohe_tuned_04.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 4977.37 MB\n",
      "Memory usage after changing types 2449.34 MB\n",
      "Memory usage before changing types 788.58 MB\n",
      "Memory usage after changing types 388.07 MB\n",
      "X_train.shape (307511, 2046)\n",
      "X_test.shape (48744, 2045)\n",
      "X_train.isnull().sum().sum: 0\n",
      "X_test.isnull().sum().sum: 0\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "X_train.shape (307511, 2044)\n",
      "X_test.shape (48744, 2044)\n",
      "(246008, 2044) (246008,) (61503, 2044) (61503,)\n",
      "Time elapsed: 1497.16492 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "X_train = load_csv(os.path.join(IN_DIR, \"X_y_train.csv\"))\n",
    "X_test = load_csv(os.path.join(IN_DIR, \"X_test.csv\"))\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"X_train.isnull().sum().sum:\", X_train.isnull().sum().sum())\n",
    "print(\"X_test.isnull().sum().sum:\", X_test.isnull().sum().sum())\n",
    "\n",
    "y_train = X_train[\"APPL_TARGET\"].values\n",
    "X_train = X_train.drop([\"APPL_TARGET\", \"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "sk_id_test = X_test[[\"SK_ID_CURR\"]]\n",
    "X_test = X_test.drop([\"SK_ID_CURR\"], axis=\"columns\")\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "X_train = le.fit_transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "cat_col_idx = le.get_cat_cols()\n",
    "features = le._features\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=35039)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.91210\n",
      "AUC of the validation set: 0.77463\n",
      "Time elapsed: 68.57388 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [19:48:54<00:00, 356.67s/trial, best loss: -0.7983844283788208]   \n",
      "Time elapsed: 71585.33025 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6054492781459379,\n",
       " 'learning_rate': 0.060075903200569394,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 3,\n",
       " 'reg_alpha': 0.004351104122044269,\n",
       " 'reg_lambda': 231.86577481386294,\n",
       " 'subsample': 0.9034672972914196}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 12, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 14, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.001), np.log(10000)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params, best_model = run_hyperopt(xgb, params, \n",
    "                                               X_train, y_train, X_val, y_val, \n",
    "                                               num_eval,\n",
    "                                               params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_val = roc_auc(best_model, X_val, y_val)\n",
    "print(\"AUC of the validation set: %0.5f\" % auc_val)\n",
    "\n",
    "\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_data5_le_tuned_02.csv\")\n",
    "write_submit_csv(best_model, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_data5_le_tuned_02.pickle\")\n",
    "pickle.dump(best_model, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "lgbm = LGBMClassifier(device=\"gpu\", categorical_feature=cat_col_idx)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "auc_lgbm_train = roc_auc(lgbm, X_train, y_train)\n",
    "print(\"AUC of LightGBM model on the train set: %0.5f\" % auc_lgbm_train)\n",
    "\n",
    "auc_lgbm_val = roc_auc(lgbm, X_val, y_val)\n",
    "print(\"AUC of LightGBM model on the validation set: %0.5f\" % auc_lgbm_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gbtree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbm = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 10, 200, 5)),\n",
    "    \"min_child_samples\": scope.int(hp.quniform(\"min_child_samples\", 10, 500, 10)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.005), np.log(1)),\n",
    "}\n",
    "\n",
    "# categorical_feature\n",
    "params_fixed_lgbm = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"device\": \"gpu\" ,\n",
    "    \"n_estimators\": 500,\n",
    "    \"categorical_feature\": cat_col_idx\n",
    "}\n",
    "\n",
    "num_eval = 100\n",
    "\n",
    "trials_lgbm, best_params_lgbm = hyperopt_lgbm(params_lgbm, \n",
    "                                              X_train, y_train, X_val, y_val, \n",
    "                                              num_eval,\n",
    "                                              params_fixed=params_fixed_lgbm,\n",
    "                                              rstate=31029)\n",
    "best_params_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'colsample_bytree': 0.667434542409391,\n",
    " 'learning_rate': 0.09146817439402177,\n",
    " 'max_depth': 4.0,\n",
    " 'min_child_samples': 380.0,\n",
    " 'num_leaves': 160.0,\n",
    " 'reg_lambda': 197.79685074014847,\n",
    " 'subsample': 0.504377097169451}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgbm = whole_to_int(best_params_lgbm)\n",
    "\n",
    "lgbm_best = LGBMClassifier(**params_fixed_lgbm, **best_params_lgbm)\n",
    "lgbm_best.fit(X_train, y_train)\n",
    "\n",
    "auc_lgbm_train = roc_auc(lgbm_best, X_train, y_train)\n",
    "print(\"AUC of LightGBM model on the train set: %0.5f\" % auc_lgbm_train)\n",
    "\n",
    "auc_lgbm_val = roc_auc(lgbm_best, X_val, y_val)\n",
    "print(\"AUC of LightGBM model on the evaluation set: %0.5f\" % auc_lgbm_val)\n",
    "\n",
    "\n",
    "lgbm_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"lgbm_data5_le_tuned_01.csv\")\n",
    "write_submit_csv(lgbm_best, X_test, sk_id_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"lgbm_data5_le_tuned_01.pickle\")\n",
    "pickle.dump(lgbm_best, open(out_model, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
