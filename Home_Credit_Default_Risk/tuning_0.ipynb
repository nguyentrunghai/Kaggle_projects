{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will tune the **XGBoost** model using the minimal data set `X_train_0.csv` and `X_test_0.csv` which are taken from `application_train.csv` and `application_test.csv` without merging with other tables. We will try some model tuning and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from _preprocessing import change_dtypes\n",
    "from _preprocessing import deal_with_abnormal_days_employed\n",
    "from _preprocessing import onehot_encoding\n",
    "from _preprocessing import GeneralLabelEncoder\n",
    "\n",
    "from _model_tunning import tune_n_estimators_w_early_stopping\n",
    "from _model_tunning import grid_search_stepwise\n",
    "\n",
    "INP_DIR = \"data/data_\"\n",
    "TUNING_DIR = \"data/tuning_\"\n",
    "\n",
    "N_JOBS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `X_train_0.csv` and `X_test_0.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 284.45 MB\n",
      "Memory usage after changing types 128.24 MB\n",
      "Memory usage before changing types 45.09 MB\n",
      "Memory usage after changing types 20.33 MB\n",
      "X_train_0 shape: (307511, 120)\n",
      "y_train shape: (307511,)\n",
      "X_test_0 shape: (48744, 120)\n",
      "id_test shape: (48744, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_0 = pd.read_csv(os.path.join(INP_DIR, \"X_train_0.csv\"))\n",
    "X_train_0 = change_dtypes(X_train_0)\n",
    "\n",
    "y_train = pd.read_csv(os.path.join(INP_DIR, \"y_train.csv\"))\n",
    "y_train = y_train[\"TARGET\"]\n",
    "\n",
    "X_test_0 = pd.read_csv(os.path.join(INP_DIR, \"X_test_0.csv\"))\n",
    "X_test_0 = change_dtypes(X_test_0)\n",
    "\n",
    "id_test = pd.read_csv(os.path.join(INP_DIR, \"id_test.csv\"))\n",
    "\n",
    "print(\"X_train_0 shape:\", X_train_0.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test_0 shape:\", X_test_0.shape)\n",
    "print(\"id_test shape:\", id_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_0_ohe shape (307511, 242)\n",
      "X_test_0_ohe shape (48744, 242)\n"
     ]
    }
   ],
   "source": [
    "# Modify abnormal value in DAYS_EMPLOYED\n",
    "X_train_0_prep = deal_with_abnormal_days_employed(X_train_0)\n",
    "X_test_0_prep = deal_with_abnormal_days_employed(X_test_0)\n",
    "\n",
    "# Onehot enconding\n",
    "X_train_0_ohe, X_test_0_ohe = onehot_encoding(X_train_0_prep, X_test_0_prep)\n",
    "\n",
    "print(\"X_train_0_ohe shape\", X_train_0_ohe.shape)\n",
    "print(\"X_test_0_ohe shape\", X_test_0_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `XGBoost` can deal with missing values, we will not impute them. It is based on trees, so feature scaling is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of default setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=123)\n",
    "cv_scores = cross_val_score(XGBClassifier(n_jobs=N_JOBS), X_train_0_ohe, y_train, scoring=\"roc_auc\", cv=kfold)\n",
    "print(\"CV AUC of XGBoost model: %0.5f +/- %0.5f\" % (cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runing this on a linux machine using 8 CPUs give:\n",
    "\n",
    "`CV AUC of XGBoost model: 0.75119 +/- 0.00278`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning `XGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 250\n",
    "pickle_out = os.path.join(TUNING_DIR, \"tuning_0.pkl\")\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=n_estimators, n_jobs=N_JOBS)\n",
    "\n",
    "step_1 = dict(learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5])\n",
    "step_2 = dict(max_depth = [2, 4, 6, 8, 10])\n",
    "step_3 = dict(min_child_weight = [0, 1, 3, 5, 7, 9])\n",
    "\n",
    "step_4 = dict(subsample=[0.6, 0.8, 1.0])\n",
    "step_5 = dict(colsample_bytree=[0.6, 0.8, 1.0])\n",
    "\n",
    "step_6 = dict(reg_lambda=[0, 1, 10, 100, 1000, 10000])\n",
    "step_7 = dict(reg_alpha=[0, 1, 10, 100, 1000, 10000])\n",
    "\n",
    "\n",
    "params_grid_steps = [step_1, step_2, step_3, step_4, step_5, step_6, step_7]\n",
    "print(\"params_grid_steps:\\n\", params_grid_steps)\n",
    "\n",
    "results = grid_search_stepwise(xgb, X_train_0_ohe, y_train, params_grid_steps, \n",
    "                               scoring=\"roc_auc\", cv=5,\n",
    "                               random_state=123, pkl_out=pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tuning is run on a remote linux machine using 8 CPUs. The pickled best estimator was downloaded to the local laptop. **The best CV AUC is 0.7607**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      " {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.2, 'max_delta_step': 0, 'max_depth': 4, 'min_child_weight': 7, 'missing': nan, 'n_estimators': 250, 'n_jobs': 8, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 10, 'reg_lambda': 100, 'scale_pos_weight': 1, 'seed': None, 'silent': None, 'subsample': 1.0, 'verbosity': 1}\n",
      "Best CV AUC:\n",
      " 0.7607209322734456\n"
     ]
    }
   ],
   "source": [
    "results = pickle.load(open(os.path.join(TUNING_DIR, \"tuning_0.pkl\"), \"rb\"))\n",
    "xgb = results[\"best_estimator\"]\n",
    "print(\"Best params:\\n\", xgb.get_params())\n",
    "print(\"Best CV AUC:\\n\", results[\"best_scores\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding\n",
    "A member of the first-place team said that using label encoding can boost the performance. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_0_lbe shape: (307511, 121)\n",
      "X_test_0_lbe shape: (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "lbe = GeneralLabelEncoder()\n",
    "X_train_0_lbe = lbe.fit(X_train_0_prep)\n",
    "\n",
    "X_train_0_lbe = lbe.transform(X_train_0_prep)\n",
    "X_test_0_lbe = lbe.transform(X_test_0_prep)\n",
    "\n",
    "print(\"X_train_0_lbe shape:\", X_train_0_lbe.shape)\n",
    "print(\"X_test_0_lbe shape:\", X_test_0_lbe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the model tuned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC of XGBoost model: 0.76036 +/- 0.00317\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=123)\n",
    "cv_scores = cross_val_score(xgb, X_train_0_lbe, y_train, scoring=\"roc_auc\", cv=kfold)\n",
    "print(\"CV AUC of XGBoost model: %0.5f +/- %0.5f\" % (cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CV AUC of XGBoost model: 0.76036 +/- 0.00317`. A little bit worse. Maybe we have to re-tune the model with this label-encoded features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = os.path.join(TUNING_DIR, \"tuning_0a.pkl\")\n",
    "\n",
    "step_1 = dict(learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5])\n",
    "step_2 = dict(max_depth = [2, 4, 6, 8, 10])\n",
    "step_3 = dict(min_child_weight = [0, 1, 3, 5, 7, 9])\n",
    "\n",
    "step_4 = dict(subsample=[0.6, 0.8, 1.0])\n",
    "step_5 = dict(colsample_bytree=[0.6, 0.8, 1.0])\n",
    "\n",
    "step_6 = dict(reg_lambda=[0, 1, 10, 100, 1000, 10000])\n",
    "step_7 = dict(reg_alpha=[0, 1, 10, 100, 1000, 10000])\n",
    "\n",
    "\n",
    "params_grid_steps = [step_1, step_2, step_3, step_4, step_5, step_6, step_7]\n",
    "print(\"params_grid_steps:\\n\", params_grid_steps)\n",
    "\n",
    "results = grid_search_stepwise(xgb, X_train_0_lbe, y_train, params_grid_steps, \n",
    "                               scoring=\"roc_auc\", cv=5,\n",
    "                               random_state=123, pkl_out=pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 1,\n",
       " 'verbosity': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier().get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
