{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use data at `2016-05` to predict product purchase for `2016-06`.\n",
    "\n",
    "The best MAP@7 in private leader board is 0.03140. The worst is 0.00448."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import ml_metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser):\n",
    "    \n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == np.object:\n",
    "        return ser.astype(\"category\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "def change_dtype_df(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col])\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"mean\"):\n",
    "        self._method = method\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in num_cols:\n",
    "            self._impute_values[col] = df_train[col].agg(self._method)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"Do not have the same set of cols as train\"\n",
    "        \n",
    "        for col, val in self._impute_values.items():\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].fillna(val)\n",
    "        \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        return df\n",
    "    \n",
    "\n",
    "class CatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, val=\"MISSING\"):\n",
    "        self._val = val\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cat_cols = df_train.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in cat_cols:\n",
    "            self._impute_values[col] = self._val\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"Do not have the same set of cols as train\"\n",
    "        \n",
    "        for col, val in self._impute_values.items():\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "                \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_classes=20, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        self._max_classes = max_classes\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self._cols_before = train_df.columns.to_list()\n",
    "        \n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        self._cat_cols = [col for col in self._cat_cols if train_df[col].nunique() <= self._max_classes]\n",
    "        #print(\"Columns to one-hot encode:\", self._cat_cols)\n",
    "        df_cat = train_df[self._cat_cols]\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        \n",
    "        num_cols = [col for col in train_df.columns if col not in self._cat_cols]\n",
    "        self._cols_after = num_cols + self._cat_cols_ohe\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        cols_before = df.columns.to_list()\n",
    "        assert set(cols_before) == set(self._cols_before), \"Do not have the same columns as train before transformed\"\n",
    "        \n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df[self._cols_after]\n",
    "        \n",
    "        df_cat = df[self._cat_cols]\n",
    "        #print(\"df_cat.columns\", df_cat.columns)\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        cols_to_drop = [col for col in df_cat.columns if col not in self._cat_cols_ohe]\n",
    "        #print(\"cols_to_drop:\", cols_to_drop)\n",
    "        df_cat = df_cat.drop(cols_to_drop, axis=\"columns\")\n",
    "        \n",
    "        # change to float32\n",
    "        for col in df_cat.columns:\n",
    "            df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in self._cat_cols]\n",
    "        cols_after = num_cols + df_cat.columns.to_list()\n",
    "        assert set(cols_after) == set(self._cols_after), \"Do not have the same columns as train after transformed\"\n",
    "        \n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        df = pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "        # align columns\n",
    "        df = df[self._cols_after]\n",
    "        self._features = df.columns.to_list()\n",
    "        \n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "\n",
    "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        \n",
    "        self._cat_col_idx = [i for i, col in enumerate(self._train_cols) if col in cat_cols]\n",
    "        \n",
    "        self._label_maps = {}\n",
    "        self._missing_imputers = {}\n",
    "        for col in cat_cols:\n",
    "            label = df_train[col].unique()\n",
    "            self._label_maps[col] = {c: n for n, c in enumerate(label)}\n",
    "            \n",
    "            mode_label = df_train[col].mode().iloc[0]\n",
    "            self._missing_imputers[col] = self._label_maps[col][mode_label]\n",
    "        \n",
    "        #print(\"Cols to label encode:\", list(self._label_maps.keys()))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"do not have the same set of columns as train\"\n",
    "        \n",
    "        for col, label_map in self._label_maps.items():\n",
    "            df[col] = df[col].map(label_map).astype(np.float32)\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].astype(np.float32).fillna(self._missing_imputers[col])\n",
    "        \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        \n",
    "        self._features = df.columns.to_list()\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def get_cat_cols(self):\n",
    "        return self._cat_col_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same_cols(df1, df2):\n",
    "    cols1 = df1.columns\n",
    "    cols2 = df2.columns\n",
    "    for c1, c2 in zip(cols1, cols2):\n",
    "        if c1 != c2:\n",
    "            print(c1, c2)\n",
    "    return None\n",
    "\n",
    "def col_align(df1, df2, to_array=False):\n",
    "    cols1 = df1.columns.to_list()\n",
    "    cols2 = df2.columns.to_list()\n",
    "    assert set(cols1) == set(cols2), \"df1 and df2 do not have the same set of columns\"\n",
    "    \n",
    "    if to_array:\n",
    "        return df1.values.astype(np.float32), df2[cols1].values.astype(np.float32)\n",
    "    else:\n",
    "        return df1, df2[cols1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean average precision at k\n",
    "def mapk(y, y_prob, k=7):\n",
    "    y = y[:, np.newaxis]\n",
    "    # ascending\n",
    "    y_pred = np.argsort(y_prob, axis=1)\n",
    "    # descending\n",
    "    y_pred = y_pred[:, ::-1]\n",
    "    \n",
    "    return ml_metrics.mapk(y, y_pred, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submit(y_prob, target_labels, ncodpers, filepath, k=7):\n",
    "    # ascending\n",
    "    y_pred = np.argsort(y_prob, axis=1)\n",
    "    # descending\n",
    "    y_pred = y_pred[:, ::-1]\n",
    "    # cut a k\n",
    "    y_pred = y_pred[:, :k]\n",
    "    \n",
    "    added_prods = target_labels[y_pred]\n",
    "    added_prods = [\" \".join(line) for line in added_prods]\n",
    "    \n",
    "    sub_df = pd.DataFrame(ncodpers)\n",
    "    sub_df[\"added_products\"] = added_prods\n",
    "    \n",
    "    sub_df.to_csv(filepath, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def run_hyperopt(classifier,\n",
    "                 params_tuned, \n",
    "                 X_train, y_train,\n",
    "                 X_val, y_val,\n",
    "                 num_eval,\n",
    "                 metric,\n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    assert metric in [\"map7\", \"acc\"]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective_map7(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        y_val_prob = classifier.predict_proba(X_val)\n",
    "        map7 = mapk(y_val, y_val_prob, k=7)\n",
    "        \n",
    "        return {\"loss\": -map7, \"status\": STATUS_OK}\n",
    "    \n",
    "    def objective_acc(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        y_val_pred = classifier.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        return {\"loss\": -acc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if metric == \"map7\":\n",
    "        print(\"Use map7\")\n",
    "        objective = objective_map7\n",
    "    else:\n",
    "        print(\"Use acc\")\n",
    "        objective = objective_acc\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    \n",
    "    return trials, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIR = \"data/data1_\"\n",
    "SUB_DIR = \"data/submit_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (67,71,123,125,126,127,129,179,181,182,183,185,235,237,238,239,241,291,293,294,295,297,347,349,350,351,353) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 92.58 MB\n",
      "Memory usage after changing types 42.72 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33005, 355)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_2016_04 = load_csv(os.path.join(INP_DIR, \"X_y_2016_04.csv\"))\n",
    "\n",
    "X_y_2016_04.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 100.60 MB\n",
      "Memory usage after changing types 46.42 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35865, 355)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_2016_05 = load_csv(os.path.join(INP_DIR, \"X_y_2016_05.csv\"))\n",
    "\n",
    "X_y_2016_05.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (122,124,125,126,128,178,180,181,182,184,234,236,237,238,240,290,292,293,294,296,346,348,349,350,352) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 2587.12 MB\n",
      "Memory usage after changing types 1202.94 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(929615, 354)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2016_06 = load_csv(os.path.join(INP_DIR, \"X_2016_06.csv\"))\n",
    "\n",
    "X_2016_06.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ind_tjcr_fin_ult1', 'ind_recibo_ult1', 'ind_nom_pens_ult1',\n",
       "       'ind_nomina_ult1', 'ind_ctop_fin_ult1', 'ind_cno_fin_ult1',\n",
       "       'ind_ecue_fin_ult1', 'ind_ctpp_fin_ult1', 'ind_cco_fin_ult1',\n",
       "       'ind_valo_fin_ult1', 'ind_fond_fin_ult1', 'ind_reca_fin_ult1',\n",
       "       'ind_plan_fin_ult1', 'ind_ctma_fin_ult1', 'ind_dela_fin_ult1',\n",
       "       'ind_ctju_fin_ult1'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_LABELS = np.array(X_y_2016_05[\"TARGET\"].unique())\n",
    "TARGET_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ind_tjcr_fin_ult1': 0,\n",
       " 'ind_recibo_ult1': 1,\n",
       " 'ind_nom_pens_ult1': 2,\n",
       " 'ind_nomina_ult1': 3,\n",
       " 'ind_ctop_fin_ult1': 4,\n",
       " 'ind_cno_fin_ult1': 5,\n",
       " 'ind_ecue_fin_ult1': 6,\n",
       " 'ind_ctpp_fin_ult1': 7,\n",
       " 'ind_cco_fin_ult1': 8,\n",
       " 'ind_valo_fin_ult1': 9,\n",
       " 'ind_fond_fin_ult1': 10,\n",
       " 'ind_reca_fin_ult1': 11,\n",
       " 'ind_plan_fin_ult1': 12,\n",
       " 'ind_ctma_fin_ult1': 13,\n",
       " 'ind_dela_fin_ult1': 14,\n",
       " 'ind_ctju_fin_ult1': 15}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_MAP = {l: n for n, l in enumerate(TARGET_LABELS)}\n",
    "LABEL_MAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_label_encode(y_labels, label_map):\n",
    "    y_encoded = y_labels.map(label_map)\n",
    "    assert y_encoded.isnull().sum() == 0\n",
    "    return np.array(y_encoded.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (33005, 353)\n",
      "y_train.shape (33005,)\n",
      "X_val.shape (35865, 353)\n",
      "y_val.shape (35865,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33005, 403), (35865, 403))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_04.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_04[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_val = le.transform(X_val)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6695\n",
      "Validation acc: 0.6350\n",
      "Train MAP@7: 0.8154\n",
      "Validation MAP@7: 0.7968\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr.predict(X_train)\n",
    "y_train_prob = lr.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = lr.predict(X_val)\n",
    "y_val_prob = lr.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (35865, 353)\n",
      "y_train.shape (35865,)\n",
      "X_test.shape (929615, 353)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35865, 403), (929615, 403))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_test = X_2016_06.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_2016_06[\"ncodpers\"]\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6527\n",
      "Train MAP@7: 0.8107\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr.predict(X_train)\n",
    "y_train_prob = lr.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "y_test_prob = lr.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submiting this gives MAP@7 = 0.02400 for public and MAP@7 = 0.02438 for private score.\n",
    "write_submit(y_test_prob, TARGET_LABELS, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"lr_d1_2016_05.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer validation MAP@7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (33005, 353)\n",
      "y_train.shape (33005,)\n",
      "X_val.shape (35865, 353)\n",
      "y_val.shape (35865,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33005, 403), (35865, 403))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_04.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_04[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_val = le.transform(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use map7\n",
      "100%|██████████| 100/100 [38:18<00:00, 22.99s/trial, best loss: -0.793821406995811]\n",
      "Time elapsed: 2298.85239 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20,\n",
       " 'max_features': 170,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 80}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 30, 2)),\n",
    "    \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 10, 500, 20)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 5, 200, 10)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 10, 400, 10)),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "\n",
    "num_eval = 100\n",
    "metric = \"map7\"\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "trials, best_params = run_hyperopt(rf, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6661\n",
      "Validation acc: 0.6337\n",
      "Train MAP@7: 0.8134\n",
      "Validation MAP@7: 0.7940\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**params_fixed, **best_params)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_train_prob = rf.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = rf.predict(X_val)\n",
    "y_val_prob = rf.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (35865, 353)\n",
      "y_train.shape (35865,)\n",
      "X_test.shape (929615, 353)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35865, 403), (929615, 403))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_test = X_2016_06.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_2016_06[\"ncodpers\"]\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=20, max_features=170, min_samples_leaf=10,\n",
       "                       min_samples_split=80, n_estimators=500, n_jobs=20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**params_fixed, **best_params)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6490\n",
      "Train MAP@7: 0.8092\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = rf.predict(X_train)\n",
    "y_train_prob = rf.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = rf.predict(X_test)\n",
    "y_test_prob = rf.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submiting this gives MAP@7 = 0.02418 for public and MAP@7 = 0.02447 for private score.\n",
    "write_submit(y_test_prob, TARGET_LABELS, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"rf_d1_2016_05_map7.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (33005, 353)\n",
      "y_train.shape (33005,)\n",
      "X_val.shape (35865, 353)\n",
      "y_val.shape (35865,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33005, 403), (35865, 403))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_04.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_04[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_val = le.transform(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use acc\n",
      "100%|██████████| 100/100 [44:43<00:00, 26.84s/trial, best loss: -0.633096333472745]\n",
      "Time elapsed: 2683.95816 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 24,\n",
       " 'max_features': 300,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 40}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 30, 2)),\n",
    "    \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 10, 500, 20)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 5, 200, 10)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 10, 400, 10)),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "\n",
    "num_eval = 100\n",
    "metric = \"acc\"\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "trials, best_params = run_hyperopt(rf, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6764\n",
      "Validation acc: 0.6326\n",
      "Train MAP@7: 0.8222\n",
      "Validation MAP@7: 0.7927\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**params_fixed, **best_params)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_train_prob = rf.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = rf.predict(X_val)\n",
    "y_val_prob = rf.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (35865, 353)\n",
      "y_train.shape (35865,)\n",
      "X_test.shape (929615, 353)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35865, 403), (929615, 403))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_test = X_2016_06.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_2016_06[\"ncodpers\"]\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6565\n",
      "Train MAP@7: 0.8161\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**params_fixed, **best_params)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_train_prob = rf.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = rf.predict(X_test)\n",
    "y_test_prob = rf.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submiting this gives MAP@7 = 0.02459 for public and MAP@7 = 0.02428 for private score.\n",
    "write_submit(y_test_prob, TARGET_LABELS, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"rf_d1_2016_05_acc.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize validation MAP@7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (33005, 353)\n",
      "y_train.shape (33005,)\n",
      "X_val.shape (35865, 353)\n",
      "y_val.shape (35865,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33005, 403), (35865, 403))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_04.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_04[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_val = le.transform(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use map7\n",
      "100%|██████████| 200/200 [5:41:22<00:00, 102.41s/trial, best loss: -0.8054192640390884]  \n",
      "Time elapsed: 20482.07485 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.4773572245198071,\n",
       " 'learning_rate': 0.03713275404431288,\n",
       " 'max_depth': 4,\n",
       " 'min_child_weight': 13,\n",
       " 'reg_lambda': 0.0005825016743861352,\n",
       " 'subsample': 0.8192814156358021}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 14, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.00001), np.log(100)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "metric = \"map7\"\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params = run_hyperopt(xgb, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'colsample_bytree': 0.4773572245198071,\n",
    " 'learning_rate': 0.03713275404431288,\n",
    " 'max_depth': 4,\n",
    " 'min_child_weight': 13,\n",
    " 'reg_lambda': 0.0005825016743861352,\n",
    " 'subsample': 0.8192814156358021}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6849\n",
      "Validation acc: 0.6495\n",
      "Train MAP@7: 0.8245\n",
      "Validation MAP@7: 0.8054\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**params_fixed, **best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_train_prob = xgb.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = xgb.predict(X_val)\n",
    "y_val_prob = xgb.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (35865, 353)\n",
      "y_train.shape (35865,)\n",
      "X_test.shape (929615, 353)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35865, 403), (929615, 403))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_test = X_2016_06.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_2016_06[\"ncodpers\"]\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6654\n",
      "Train MAP@7: 0.8180\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**params_fixed, **best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_train_prob = xgb.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "y_test_prob = xgb.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submiting this gives MAP@7 = 0.02520 for public and MAP@7 = 0.02556 for private score.\n",
    "write_submit(y_test_prob, TARGET_LABELS, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"xgb_d1_2016_05_map7.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (33005, 353)\n",
      "y_train.shape (33005,)\n",
      "X_val.shape (35865, 353)\n",
      "y_val.shape (35865,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33005, 403), (35865, 403))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_04.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_04[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_val = le.transform(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use acc\n",
      "100%|██████████| 200/200 [8:24:05<00:00, 151.23s/trial, best loss: -0.6502997351178029]  \n",
      "Time elapsed: 30245.36318 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.848734212760469,\n",
       " 'learning_rate': 0.017592291895543413,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 4,\n",
       " 'reg_lambda': 0.01604385893880587,\n",
       " 'subsample': 0.4298776738034783}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 14, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.00001), np.log(100)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "metric = \"acc\"\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params = run_hyperopt(xgb, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.848734212760469,\n",
       " 'learning_rate': 0.017592291895543413,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 4,\n",
       " 'reg_lambda': 0.01604385893880587,\n",
       " 'subsample': 0.4298776738034783}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'colsample_bytree': 0.848734212760469,\n",
    " 'learning_rate': 0.017592291895543413,\n",
    " 'max_depth': 7,\n",
    " 'min_child_weight': 4,\n",
    " 'reg_lambda': 0.01604385893880587,\n",
    " 'subsample': 0.4298776738034783}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6977\n",
      "Validation acc: 0.6503\n",
      "Train MAP@7: 0.8334\n",
      "Validation MAP@7: 0.8051\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**params_fixed, **best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_train_prob = xgb.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = xgb.predict(X_val)\n",
    "y_val_prob = xgb.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (35865, 353)\n",
      "y_train.shape (35865,)\n",
      "X_test.shape (929615, 353)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35865, 403), (929615, 403))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_test = X_2016_06.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_2016_06[\"ncodpers\"]\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6748\n",
      "Train MAP@7: 0.8238\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**params_fixed, **best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_train_prob = xgb.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "y_test_prob = xgb.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submit this gives MAP@7 = 0.02522 for public and MAP@7 = 0.02560 for private score.\n",
    "write_submit(y_test_prob, TARGET_LABELS, ncodpers_test,\n",
    "             os.path.join(SUB_DIR, \"xgb_d1_2016_05_acc.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (33005, 353)\n",
      "y_train.shape (33005,)\n",
      "X_val.shape (35865, 353)\n",
      "y_val.shape (35865,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((33005, 403), (35865, 403))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_04.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_04[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_val = le.transform(X_val)\n",
    "\n",
    "cat_col_idx = le.get_cat_cols()\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use map7\n",
      "100%|██████████| 200/200 [2:35:26<00:00, 46.63s/trial, best loss: -0.8052270750765105]  \n",
      "Time elapsed: 9326.61213 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6007753108549476,\n",
       " 'learning_rate': 0.016213998507390654,\n",
       " 'max_depth': 5,\n",
       " 'min_child_samples': 60,\n",
       " 'num_leaves': 175,\n",
       " 'reg_lambda': 0.06376226621032771,\n",
       " 'subsample': 0.483251255994689}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 10, 300, 5)),\n",
    "    \"min_child_samples\": scope.int(hp.quniform(\"min_child_samples\", 10, 200, 10)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.0001), np.log(1000)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.005), np.log(1)),\n",
    "}\n",
    "\n",
    "# categorical_feature\n",
    "params_fixed = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"device\": \"gpu\" ,\n",
    "    \"n_estimators\": 500,\n",
    "    \"categorical_feature\": cat_col_idx\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "metric = \"map7\"\n",
    "lgbm = LGBMClassifier()\n",
    "trials, best_params = run_hyperopt(lgbm, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6945\n",
      "Validation acc: 0.6488\n",
      "Train MAP@7: 0.8325\n",
      "Validation MAP@7: 0.8051\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(**params_fixed, **best_params)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lgbm.predict(X_train)\n",
    "y_train_prob = lgbm.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = lgbm.predict(X_val)\n",
    "y_val_prob = lgbm.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (35865, 353)\n",
      "y_train.shape (35865,)\n",
      "X_test.shape (929615, 353)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35865, 403), (929615, 403))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_2016_05.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = target_label_encode(X_y_2016_05[\"TARGET\"], LABEL_MAP)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_test = X_2016_06.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_2016_06[\"ncodpers\"]\n",
    "\n",
    "\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_test = ohe.transform(X_test)\n",
    "\n",
    "\n",
    "le = LabelEncoder(to_array=True)\n",
    "le.fit(X_train)\n",
    "X_train = le.transform(X_train)\n",
    "X_test = le.transform(X_test)\n",
    "\n",
    "cat_col_idx = le.get_cat_cols()\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6749\n",
      "Train MAP@7: 0.8256\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(**params_fixed, **best_params)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lgbm.predict(X_train)\n",
    "y_train_prob = lgbm.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = lgbm.predict(X_test)\n",
    "y_test_prob = lgbm.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submit this gives MAP@7 = 0.02520 for public and MAP@7 = 0.02555 for private score.\n",
    "write_submit(y_test_prob, TARGET_LABELS, ncodpers_test,\n",
    "             os.path.join(SUB_DIR, \"lgbm_d1_2016_05_map7.csv\"), k=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
