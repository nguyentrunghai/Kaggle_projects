{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best MAP@7 in private leader board is 0.03140. The worst is 0.00448."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import ml_metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser):\n",
    "    \n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == np.object:\n",
    "        return ser.astype(\"category\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "def change_dtype_df(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col])\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"mean\"):\n",
    "        self._method = method\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        num_cols = df_train.select_dtypes([\"number\"]).columns.to_list()\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in num_cols:\n",
    "            self._impute_values[col] = df_train[col].agg(self._method)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"Do not have the same set of cols as train\"\n",
    "        \n",
    "        for col, val in self._impute_values.items():\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].fillna(val)\n",
    "        \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        return df\n",
    "    \n",
    "\n",
    "class CatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, val=\"MISSING\"):\n",
    "        self._val = val\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        cat_cols = df_train.select_dtypes([\"object\", \"category\", \"bool\"]).columns.to_list()\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        \n",
    "        self._impute_values = {}\n",
    "        for col in cat_cols:\n",
    "            self._impute_values[col] = self._val\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"Do not have the same set of cols as train\"\n",
    "        \n",
    "        for col, val in self._impute_values.items():\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].astype(\"object\").fillna(val).astype(\"category\")\n",
    "                \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_classes=20, to_array=False):\n",
    "        self._to_array = to_array\n",
    "        self._max_classes = max_classes\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        self._cols_before = train_df.columns.to_list()\n",
    "        \n",
    "        df_cat = train_df.select_dtypes([\"object\", \"category\"])\n",
    "        self._cat_cols = df_cat.columns.to_list()\n",
    "        \n",
    "        self._cat_cols = [col for col in self._cat_cols if train_df[col].nunique() <= self._max_classes]\n",
    "        #print(\"Columns to one-hot encode:\", self._cat_cols)\n",
    "        df_cat = train_df[self._cat_cols]\n",
    "        \n",
    "        if len(self._cat_cols) > 0:\n",
    "            self._cat_cols_ohe = pd.get_dummies(df_cat, drop_first=True).columns.to_list()\n",
    "        else:\n",
    "            self._cat_cols_ohe = []\n",
    "        \n",
    "        num_cols = [col for col in train_df.columns if col not in self._cat_cols]\n",
    "        self._cols_after = num_cols + self._cat_cols_ohe\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        cols_before = df.columns.to_list()\n",
    "        assert set(cols_before) == set(self._cols_before), \"Do not have the same columns as train before transformed\"\n",
    "        \n",
    "        if len(self._cat_cols) == 0:\n",
    "            print(\"No cat cols in df_train, so do nothing.\")\n",
    "            return df[self._cols_after]\n",
    "        \n",
    "        df_cat = df[self._cat_cols]\n",
    "        #print(\"df_cat.columns\", df_cat.columns)\n",
    "        \n",
    "        # one-hot encode\n",
    "        df_cat = pd.get_dummies(df_cat)\n",
    "        # drop cols that are present in test_df but absent in train_df\n",
    "        cols_to_drop = [col for col in df_cat.columns if col not in self._cat_cols_ohe]\n",
    "        #print(\"cols_to_drop:\", cols_to_drop)\n",
    "        df_cat = df_cat.drop(cols_to_drop, axis=\"columns\")\n",
    "        \n",
    "        # change to float32\n",
    "        for col in df_cat.columns:\n",
    "            df_cat[col] = df_cat[col].astype(\"float32\")\n",
    "        \n",
    "        # if some some colums are absent in test but present in train, make them all zero \n",
    "        cat_cols_ohe = df_cat.columns.to_list()\n",
    "        for col in self._cat_cols_ohe:\n",
    "            if col not in cat_cols_ohe:\n",
    "                df_cat[col] = 0\n",
    "                df_cat[col] = df_cat[col].astype(np.uint8)\n",
    "        \n",
    "        num_cols = [col for col in df.columns if col not in self._cat_cols]\n",
    "        cols_after = num_cols + df_cat.columns.to_list()\n",
    "        assert set(cols_after) == set(self._cols_after), \"Do not have the same columns as train after transformed\"\n",
    "        \n",
    "        df_num = df[num_cols]\n",
    "        \n",
    "        df = pd.concat([df_num, df_cat], axis=\"columns\")\n",
    "        # align columns\n",
    "        df = df[self._cols_after]\n",
    "        self._features = df.columns.to_list()\n",
    "        \n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "\n",
    "class UDLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, to_array=False):\n",
    "        self._to_array = to_array\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        self._train_cols = df_train.columns.to_list()\n",
    "        cat_cols = df_train.select_dtypes([\"category\", \"object\"]).columns.to_list()\n",
    "        \n",
    "        self._cat_col_idx = [i for i, col in enumerate(self._train_cols) if col in cat_cols]\n",
    "        \n",
    "        self._label_maps = {}\n",
    "        self._missing_imputers = {}\n",
    "        for col in cat_cols:\n",
    "            label = df_train[col].unique()\n",
    "            self._label_maps[col] = {c: n for n, c in enumerate(label)}\n",
    "            \n",
    "            mode_label = df_train[col].mode().iloc[0]\n",
    "            self._missing_imputers[col] = self._label_maps[col][mode_label]\n",
    "        \n",
    "        #print(\"Cols to label encode:\", list(self._label_maps.keys()))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        cols = df.columns.to_list()\n",
    "        assert set(cols) == set(self._train_cols), \"do not have the same set of columns as train\"\n",
    "        \n",
    "        for col, label_map in self._label_maps.items():\n",
    "            df[col] = df[col].map(label_map).astype(np.float32)\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].astype(np.float32).fillna(self._missing_imputers[col])\n",
    "        \n",
    "        # align columns\n",
    "        df = df[self._train_cols]\n",
    "        \n",
    "        self._features = df.columns.to_list()\n",
    "        if self._to_array:\n",
    "            return df.values.astype(np.float32)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def get_cat_cols(self):\n",
    "        return self._cat_col_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same_cols(df1, df2):\n",
    "    cols1 = df1.columns\n",
    "    cols2 = df2.columns\n",
    "    for c1, c2 in zip(cols1, cols2):\n",
    "        if c1 != c2:\n",
    "            print(c1, c2)\n",
    "    return None\n",
    "\n",
    "def col_align(df1, df2, to_array=False):\n",
    "    cols1 = df1.columns.to_list()\n",
    "    cols2 = df2.columns.to_list()\n",
    "    assert set(cols1) == set(cols2), \"df1 and df2 do not have the same set of columns\"\n",
    "    \n",
    "    if to_array:\n",
    "        return df1.values.astype(np.float32), df2[cols1].values.astype(np.float32)\n",
    "    else:\n",
    "        return df1, df2[cols1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean average precision at k\n",
    "def mapk(y, y_prob, k=7):\n",
    "    y = y[:, np.newaxis]\n",
    "    # ascending\n",
    "    y_pred = np.argsort(y_prob, axis=1)\n",
    "    # descending\n",
    "    y_pred = y_pred[:, ::-1]\n",
    "    \n",
    "    return ml_metrics.mapk(y, y_pred, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submit(y_prob, target_labels, ncodpers, filepath, k=7):\n",
    "    # ascending\n",
    "    y_pred = np.argsort(y_prob, axis=1)\n",
    "    # descending\n",
    "    y_pred = y_pred[:, ::-1]\n",
    "    # cut a k\n",
    "    y_pred = y_pred[:, :k]\n",
    "    \n",
    "    added_prods = target_labels[y_pred]\n",
    "    added_prods = [\" \".join(line) for line in added_prods]\n",
    "    \n",
    "    sub_df = pd.DataFrame(ncodpers)\n",
    "    sub_df[\"added_products\"] = added_prods\n",
    "    \n",
    "    sub_df.to_csv(filepath, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def run_hyperopt(classifier,\n",
    "                 params_tuned, \n",
    "                 X_train, y_train,\n",
    "                 X_val, y_val,\n",
    "                 num_eval,\n",
    "                 metric,\n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    assert metric in [\"map7\", \"acc\"]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective_map7(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        y_val_prob = classifier.predict_proba(X_val)\n",
    "        map7 = mapk(y_val, y_val_prob, k=7)\n",
    "        \n",
    "        return {\"loss\": -map7, \"status\": STATUS_OK}\n",
    "    \n",
    "    def objective_acc(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        y_val_pred = classifier.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        return {\"loss\": -acc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if metric == \"map7\":\n",
    "        print(\"Use map7\")\n",
    "        objective = objective_map7\n",
    "    else:\n",
    "        print(\"Use acc\")\n",
    "        objective = objective_acc\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    \n",
    "    return trials, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIR = \"data/data1_\"\n",
    "SUB_DIR = \"data/submit_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 1036.11 MB\n",
      "Memory usage after changing types 497.64 MB\n",
      "Memory usage before changing types 101.92 MB\n",
      "Memory usage after changing types 48.92 MB\n",
      "Memory usage before changing types 2632.67 MB\n",
      "Memory usage after changing types 1267.08 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((364826, 355), (35887, 355), (929615, 354))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_train_df = load_csv(os.path.join(INP_DIR, \"X_y_train.csv\"))\n",
    "X_y_val_df = load_csv(os.path.join(INP_DIR, \"X_y_val.csv\"))\n",
    "X_test_df = load_csv(os.path.join(INP_DIR, \"X_test.csv\"))\n",
    "\n",
    "X_y_train_df.shape, X_y_val_df.shape, X_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with most popular products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_popular_prods(y_train, X_test, out_filepath):\n",
    "    seven_most_popul = y_train.value_counts().index[:7].to_list()\n",
    "    seven_most_popul = \" \".join(seven_most_popul)\n",
    "    \n",
    "    sub_df = X_test[[\"ncodpers\"]].copy()\n",
    "    sub_df[\"added_products\"] = seven_most_popul\n",
    "    \n",
    "    sub_df.to_csv(out_filepath, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filepath = os.path.join(SUB_DIR, \"popula_prods.csv\")\n",
    "\n",
    "# submiting this gives MAP@7 = 0.01580 for public and MAP@7 = 0.01589 for private score.\n",
    "predict_popular_prods(X_y_train_df[\"TARGET\"], X_test_df, out_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (364826, 353)\n",
      "y_train.shape (364826,)\n",
      "X_val.shape (35887, 353)\n",
      "y_val.shape (35887,)\n",
      "X_test.shape (929615, 353)\n",
      "Impute numerical cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "Impute cat cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "One-hot encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n",
      "Label encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((364826, 388), (35887, 388), (929615, 388))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_train_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = X_y_train_df[\"TARGET\"]\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_val_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = X_y_val_df[\"TARGET\"]\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "X_test = X_test_df.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_test_df[\"ncodpers\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "target_classes = le.classes_\n",
    "\n",
    "print(\"Impute numerical cols\")\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute cat cols\")\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "X_test = ohe.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Label encoding\")\n",
    "ud_le = UDLabelEncoder()\n",
    "ud_le.fit(X_train)\n",
    "X_train = ud_le.transform(X_train)\n",
    "X_val = ud_le.transform(X_val)\n",
    "X_test = ud_le.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6251\n",
      "Validation acc: 0.6335\n",
      "Train MAP@7: 0.7746\n",
      "Validation MAP@7: 0.7782\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr.predict(X_train)\n",
    "y_train_prob = lr.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = lr.predict(X_val)\n",
    "y_val_prob = lr.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6248\n",
      "Train MAP@7: 0.7749\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_train_prob = lr.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "y_test_prob = lr.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submiting this gives MAP@7 = 0.02662 for public and MAP@7 = 0.02687 for private score.\n",
    "write_submit(y_test_prob, target_classes, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"lr_d1.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (364826, 353)\n",
      "y_train.shape (364826,)\n",
      "X_val.shape (35887, 353)\n",
      "y_val.shape (35887,)\n",
      "X_test.shape (929615, 353)\n",
      "Impute numerical cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "Impute cat cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "One-hot encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n",
      "Label encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((364826, 388), (35887, 388), (929615, 388))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_train_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = X_y_train_df[\"TARGET\"]\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_val_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = X_y_val_df[\"TARGET\"]\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "X_test = X_test_df.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_test_df[\"ncodpers\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "target_classes = le.classes_\n",
    "\n",
    "print(\"Impute numerical cols\")\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute cat cols\")\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "X_test = ohe.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Label encoding\")\n",
    "ud_le = UDLabelEncoder(to_array=True)\n",
    "ud_le.fit(X_train)\n",
    "X_train = ud_le.transform(X_train)\n",
    "X_val = ud_le.transform(X_val)\n",
    "X_test = ud_le.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs too slowly\n",
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 30, 2)),\n",
    "    \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 10, 500, 20)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 5, 200, 10)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 20, 300, 10)),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 100\n",
    "metric = \"map7\"\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "trials, best_params = run_hyperopt(rf, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**params_fixed, **best_params)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_train_prob = rf.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = rf.predict(X_val)\n",
    "y_val_prob = rf.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fixed = {\n",
    "    \"n_jobs\": 20,\n",
    "    \"n_estimators\": 1000\n",
    "}\n",
    "\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.7953\n",
      "Train MAP@7: 0.8915\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**params_fixed, **best_params)\n",
    "rf.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_train_prob = rf.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = rf.predict(X_test)\n",
    "y_test_prob = rf.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submiting this gives MAP@7 = 0.02758 for public and MAP@7 = 0.02799 for private score.\n",
    "write_submit(y_test_prob, target_classes, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"rf_d1.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (364826, 353)\n",
      "y_train.shape (364826,)\n",
      "X_val.shape (35887, 353)\n",
      "y_val.shape (35887,)\n",
      "X_test.shape (929615, 353)\n",
      "Impute numerical cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "Impute cat cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "One-hot encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n",
      "Label encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((364826, 388), (35887, 388), (929615, 388))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_train_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = X_y_train_df[\"TARGET\"]\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_val_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = X_y_val_df[\"TARGET\"]\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "X_test = X_test_df.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_test_df[\"ncodpers\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "target_classes = le.classes_\n",
    "\n",
    "print(\"Impute numerical cols\")\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute cat cols\")\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "X_test = ohe.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Label encoding\")\n",
    "ud_le = UDLabelEncoder(to_array=True)\n",
    "ud_le.fit(X_train)\n",
    "X_train = ud_le.transform(X_train)\n",
    "X_val = ud_le.transform(X_val)\n",
    "X_test = ud_le.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use map7\n",
      "100%|██████████| 50/50 [17:50:39<00:00, 1284.80s/trial, best loss: -0.7876664450716335]  \n",
      "Time elapsed: 64239.86104 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8714541979195253,\n",
       " 'learning_rate': 0.01839666166429914,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 12,\n",
       " 'reg_lambda': 0.0001365632931605928,\n",
       " 'subsample': 0.913070589798433}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 14, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 20, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.000001), np.log(100)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1.)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 5.),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 50\n",
    "metric = \"map7\"\n",
    "xgb = XGBClassifier()\n",
    "trials, best_params = run_hyperopt(xgb, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'colsample_bytree': 0.8714541979195253,\n",
    " 'learning_rate': 0.01839666166429914,\n",
    " 'max_depth': 11,\n",
    " 'min_child_weight': 12,\n",
    " 'reg_lambda': 0.0001365632931605928,\n",
    " 'subsample': 0.913070589798433}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6606\n",
      "Validation acc: 0.6475\n",
      "Train MAP@7: 0.7980\n",
      "Validation MAP@7: 0.7877\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**params_fixed, **best_params)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_train_prob = xgb.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = xgb.predict(X_val)\n",
    "y_val_prob = xgb.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6596\n",
      "Train MAP@7: 0.7978\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(**params_fixed, **best_params)\n",
    "xgb.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_train_prob = xgb.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "y_test_prob = xgb.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submiting this gives MAP@7 = 0.02870 for public and MAP@7 = 0.02899 for private score.\n",
    "write_submit(y_test_prob, target_classes, ncodpers_test, \n",
    "             os.path.join(SUB_DIR, \"xgb_d1.csv\"), k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (364826, 353)\n",
      "y_train.shape (364826,)\n",
      "X_val.shape (35887, 353)\n",
      "y_val.shape (35887,)\n",
      "X_test.shape (929615, 353)\n",
      "Impute numerical cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "Impute cat cols\n",
      "(364826, 353) (35887, 353) (929615, 353)\n",
      "One-hot encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n",
      "Label encoding\n",
      "(364826, 388) (35887, 388) (929615, 388)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((364826, 388), (35887, 388), (929615, 388))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_y_train_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "y_train = X_y_train_df[\"TARGET\"]\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "\n",
    "X_val = X_y_val_df.drop([\"ncodpers\", \"TARGET\"], axis=1)\n",
    "print(\"X_val.shape\", X_val.shape)\n",
    "y_val = X_y_val_df[\"TARGET\"]\n",
    "print(\"y_val.shape\", y_val.shape)\n",
    "\n",
    "X_test = X_test_df.drop([\"ncodpers\"], axis=1)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "ncodpers_test = X_test_df[\"ncodpers\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "\n",
    "target_classes = le.classes_\n",
    "\n",
    "print(\"Impute numerical cols\")\n",
    "num_imputer = NumImputer()\n",
    "num_imputer.fit(X_train)\n",
    "X_train = num_imputer.transform(X_train)\n",
    "X_val = num_imputer.transform(X_val)\n",
    "X_test = num_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Impute cat cols\")\n",
    "cat_imputer = CatImputer()\n",
    "cat_imputer.fit(X_train)\n",
    "X_train = cat_imputer.transform(X_train)\n",
    "X_val = cat_imputer.transform(X_val)\n",
    "X_test = cat_imputer.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"One-hot encoding\")\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train = ohe.transform(X_train)\n",
    "X_val = ohe.transform(X_val)\n",
    "X_test = ohe.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "print(\"Label encoding\")\n",
    "ud_le = UDLabelEncoder(to_array=True)\n",
    "ud_le.fit(X_train)\n",
    "X_train = ud_le.transform(X_train)\n",
    "X_val = ud_le.transform(X_val)\n",
    "X_test = ud_le.transform(X_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "cat_col_idx = ud_le.get_cat_cols()\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use map7\n",
      " 62%|██████▏   | 62/100 [5:47:07<3:27:45, 328.05s/trial, best loss: -0.787033174236061] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Check failed: (best_split_info.left_count) > (0) at /home/hai/opt/src/LightGBM/src/treelearner/serial_tree_learner.cpp, line 651 .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [5:47:11<3:32:47, 335.99s/trial, best loss: -0.787033174236061]\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: (best_split_info.left_count) > (0) at /home/hai/opt/src/LightGBM/src/treelearner/serial_tree_learner.cpp, line 651 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-205607732b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                    \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                    \u001b[0mnum_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                    params_fixed=params_fixed)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5a8a152af806>\u001b[0m in \u001b[0;36mrun_hyperopt\u001b[0;34m(classifier, params_tuned, X_train, y_train, X_val, y_val, num_eval, metric, params_fixed, rstate)\u001b[0m\n\u001b[1;32m     55\u001b[0m                       \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                       \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                       rstate=rstate)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhole_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5a8a152af806>\u001b[0m in \u001b[0;36mobjective_map7\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobjective_map7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams_fixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0my_val_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    615\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metrics_callable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                               callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2472\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   2473\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2474\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   2475\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Check failed: (best_split_info.left_count) > (0) at /home/hai/opt/src/LightGBM/src/treelearner/serial_tree_learner.cpp, line 651 .\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 14, 1)),\n",
    "    \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 10, 300, 10)),\n",
    "    \"min_child_samples\": scope.int(hp.quniform(\"min_child_samples\", 10, 200, 10)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.0001), np.log(100)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1)),\n",
    "}\n",
    "\n",
    "# categorical_feature\n",
    "params_fixed = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"boosting_type\": \"goss\",\n",
    "    \"device\": \"gpu\" ,\n",
    "    \"n_estimators\": 500,\n",
    "    \"categorical_feature\": cat_col_idx\n",
    "}\n",
    "\n",
    "num_eval = 100\n",
    "\n",
    "metric = \"map7\"\n",
    "lgbm = LGBMClassifier()\n",
    "trials, best_params = run_hyperopt(lgbm, params, \n",
    "                                   X_train, y_train, X_val, y_val, \n",
    "                                   num_eval, metric,\n",
    "                                   params_fixed=params_fixed)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(**params_fixed, **best_params)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lgbm.predict(X_train)\n",
    "y_train_prob = lgbm.predict_proba(X_train)\n",
    "\n",
    "y_val_pred = lgbm.predict(X_val)\n",
    "y_val_prob = lgbm.predict_proba(X_val)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation acc: %0.4f\" %acc_val)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "map7_val = mapk(y_val, y_val_prob, k=7)\n",
    "print(\"Validation MAP@7: %0.4f\" %map7_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for `2016-06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(**params_fixed, **best_params)\n",
    "lgbm.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "y_train_pred = lgbm.predict(X_train)\n",
    "y_train_prob = lgbm.predict_proba(X_train)\n",
    "\n",
    "y_test_pred = lgbm.predict(X_test)\n",
    "y_test_prob = lgbm.predict_proba(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Train acc: %0.4f\" %acc_train)\n",
    "\n",
    "map7_train = mapk(y_train, y_train_prob, k=7)\n",
    "print(\"Train MAP@7: %0.4f\" %map7_train)\n",
    "\n",
    "\n",
    "# submit this gives MAP@7 = 0.02520 for public and MAP@7 = 0.02555 for private score.\n",
    "write_submit(y_test_prob, target_classes, ncodpers_test,\n",
    "             os.path.join(SUB_DIR, \"lgbm_d1.csv\"), k=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
