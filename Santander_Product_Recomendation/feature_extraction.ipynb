{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser):\n",
    "    \n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == np.object:\n",
    "        return ser.astype(\"category\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "def change_dtype_df(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"])\n",
    "    df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"])\n",
    "    \n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col])\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIR = \"data/data_\"\n",
    "OUT_DIR1 = \"data/data1_\"\n",
    "OUT_DIR2 = \"data/data2_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_csv(os.path.join(INP_DIR, \"train_cleaned.csv\"))\n",
    "df_test = load_csv(os.path.join(INP_DIR, \"test_cleaned.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop this column becuase it is too imbalanced\n",
    "df_train = df_train.drop([\"ind_empleado\"], axis=1)\n",
    "df_test = df_test.drop([\"ind_empleado\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days from when the data is recorded \n",
    "df_train[\"fecha_alta\"] = (df_train[\"fecha_alta\"] - df_train[\"fecha_dato\"]).dt.days\n",
    "df_test[\"fecha_alta\"] = (df_test[\"fecha_alta\"] - df_test[\"fecha_dato\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sum(), df_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD_COLS = [col for col in df_train.columns if re.match(r\"^ind_.*_ult1$\", col)]\n",
    "print(PROD_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PURCH_COLS = [col for col in df_train.columns if re.match(r\"^ind_.*_ult1_NEW_PUR$\", col)]\n",
    "print(NEW_PURCH_COLS, \"\\n\")\n",
    "\n",
    "prod_popul = df_train[NEW_PURCH_COLS].sum(axis=0)\n",
    "prod_popul = prod_popul.sort_values(ascending=False)/prod_popul.sum() * 100\n",
    "print(prod_popul, \"\\n\")\n",
    "\n",
    "NEW_PURCH_COLS = prod_popul.sort_values(ascending=False).index.to_list()\n",
    "print(NEW_PURCH_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PURCH_CANCEL_COLS = [col for col in df_train.columns if re.match(r\"^ind_.*_ult1_PUR_OR_CANCEL$\", col)]\n",
    "print(PURCH_CANCEL_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of prod\n",
    "df_train[\"TOTAL_PRODS\"] = df_train[PROD_COLS].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMOG_COLS = [col for col in df_train.columns \n",
    "    if col not in PROD_COLS + NEW_PURCH_COLS + PURCH_CANCEL_COLS + [\"fecha_dato\", \"ncodpers\", \"TOTAL_PRODS\"]]\n",
    "print(DEMOG_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_COLS = [\"fecha_alta\", \"ind_nuevo\", \"antiguedad\", \"indrel\", \"tiprel_1mes\", \n",
    "            \"ind_actividad_cliente\", \"renta\", \"segmento\", \"TOTAL_PRODS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_unique(x):\n",
    "    return len(x) == x.nunique()\n",
    "\n",
    "\n",
    "def _onehot_reduce(df):\n",
    "    df = df.copy()\n",
    "    values = df.values\n",
    "    \n",
    "    new_values = values.copy()\n",
    "    for i in range(values.shape[0]):\n",
    "        row = values[i]\n",
    "        if row.sum() > 1:\n",
    "            new_row = np.zeros([row.shape[0]], dtype=np.int)\n",
    "            \n",
    "            where_1 = np.where(row == 1)[0]\n",
    "            np.random.shuffle(where_1)\n",
    "            idx = where_1[0]\n",
    "            new_row[idx] = 1\n",
    "            \n",
    "            new_values[i] = new_row\n",
    "    \n",
    "    new_df = pd.DataFrame(data=new_values, index=df.index, columns=df.columns)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def extract_subset(df, row_filter, cols):\n",
    "    df = df.copy()\n",
    "    return df.loc[row_filter, cols]\n",
    "\n",
    "\n",
    "\n",
    "def extract_y(df, timestamp, y_cols=NEW_PURCH_COLS):\n",
    "    # only use row when customer buys at least one product\n",
    "    any_new_pur = df[y_cols].sum(axis=1) > 0\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp) & any_new_pur\n",
    "    cols = [\"ncodpers\"] + y_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "    \n",
    "    df_out.columns = [col.replace(\"_NEW_PUR\", \"\") if col.endswith(\"_NEW_PUR\") else col for col in df_out.columns]\n",
    "    \n",
    "    id_col = df_out[\"ncodpers\"]\n",
    "    pur_cols = [col for col in df_out.columns if col != \"ncodpers\"]\n",
    "    df_out = _onehot_reduce(df_out[pur_cols])\n",
    "    df_out[\"ncodpers\"] = id_col\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def extract_x_demog(df, timestamp, customer_ids, demog_cols=DEMOG_COLS):\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp) & df[\"ncodpers\"].isin(customer_ids)\n",
    "    cols = [\"ncodpers\"] + demog_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# This function can give dataframe having less rows than len(customer_ids)\n",
    "# This is becuse number of row at lag_timestamp is not the same as at timestamp\n",
    "def extract_lag_features(df, timestamp_lag, customer_ids, suffix=\"_LAG\", \n",
    "                         sel_cols=PROD_COLS+PURCH_CANCEL_COLS+LAG_COLS):\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp_lag) & df[\"ncodpers\"].isin(customer_ids)\n",
    "    cols = [\"ncodpers\"] + sel_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "    \n",
    "    df_out.columns = [\"ncodpers\"] + [col + suffix for col in df_out.columns if col != \"ncodpers\"]\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def extract_X_y_train(df, timestamp, timestamp_lags):\n",
    "    y_train = extract_y(df, timestamp)\n",
    "    print(\"y_train.shape\", y_train.shape)\n",
    "    \n",
    "    x_demog = extract_x_demog(df, timestamp, customer_ids=y_train[\"ncodpers\"])\n",
    "    print(\"x_demog.shape:\", x_demog.shape)\n",
    "    assert y_train.shape[0] == x_demog.shape[0], \"x_demog must have the same number of rows as y_train\"\n",
    "    \n",
    "    x_lags = []\n",
    "    for t, lag in enumerate(timestamp_lags):\n",
    "        assert pd.to_datetime(lag) < pd.to_datetime(timestamp), lag + \" lag is not before timestamp \" +  timestamp\n",
    "        lag_label = \"_LAG%d\" % (t + 1)\n",
    "        \n",
    "        x_lag = extract_lag_features(df, lag, customer_ids=y_train[\"ncodpers\"], suffix=lag_label)\n",
    "        print(lag, lag_label, x_lag.shape)\n",
    "        x_lags.append(x_lag)\n",
    "    \n",
    "    X_train = y_train[[\"ncodpers\"]].merge(x_demog, how=\"left\", on=\"ncodpers\")\n",
    "    print(\"Nulls after merging y and x_demog:\", X_train.isnull().sum().sum())\n",
    "    \n",
    "    for t, x_lag in enumerate(x_lags):\n",
    "        X_train = X_train.merge(x_lag, how=\"left\", on=\"ncodpers\")\n",
    "        print(\"Nulls at %d:\" %(t + 1), X_train.isnull().sum().sum())\n",
    "    \n",
    "    print(\"X_train.shape:\", X_train.shape)\n",
    "    print(\"y_train.shape:\", y_train.shape)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def extract_X_test(train, test, timestamp, timestamp_lags):\n",
    "    x_demog = extract_x_demog(test, timestamp, customer_ids=test[\"ncodpers\"])\n",
    "    print(\"x_demog.shape:\", x_demog.shape)\n",
    "    print(\"Nulls of x_demog:\", x_demog.isnull().sum().sum())\n",
    "    \n",
    "    x_lags = []\n",
    "    for t, lag in enumerate(timestamp_lags):\n",
    "        assert pd.to_datetime(lag) < pd.to_datetime(timestamp), lag + \" lag is not before timestamp \" +  timestamp\n",
    "        lag_label = \"_LAG%d\" % (t + 1)\n",
    "        \n",
    "        x_lag = extract_lag_features(train, lag, customer_ids=test[\"ncodpers\"], suffix=lag_label)\n",
    "        print(lag, lag_label, x_lag.shape)\n",
    "        x_lags.append(x_lag)\n",
    "        \n",
    "    X_test = x_demog\n",
    "    for t, x_lag in enumerate(x_lags):\n",
    "        X_test = X_test.merge(x_lag, how=\"left\", on=\"ncodpers\")\n",
    "        print(\"Nulls at %d:\" %(t + 1), X_test.isnull().sum().sum())\n",
    "    \n",
    "    print(\"X_test.shape:\", X_test.shape)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x_demog(df, timestamp, customer_ids, demog_cols=DEMOG_COLS):\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp) & df[\"ncodpers\"].isin(customer_ids)\n",
    "    cols = [\"ncodpers\"] + demog_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 6 month lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_04`, `y_2016_04`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = \"2016-04-28\"\n",
    "timestamp_lags = [\"2016-03-28\", \"2016-02-28\", \"2016-01-28\", \n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\", \n",
    "                  \"2015-09-28\", \"2015-08-28\", \"2015-07-28\", \n",
    "                  \"2015-06-28\", \"2015-05-28\", \"2015-04-28\"]\n",
    "\n",
    "X_2016_04, y_2016_04 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_2016_04.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_2016_04.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_04.to_csv(os.path.join(OUT_DIR1, \"X_2016_04.csv\"), index=False)\n",
    "y_2016_04.to_csv(os.path.join(OUT_DIR1, \"y_2016_04.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_05`, `y_2016_05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = \"2016-05-28\"\n",
    "timestamp_lags = [\"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\",\n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\", \n",
    "                  \"2015-09-28\", \"2015-08-28\", \"2015-07-28\", \n",
    "                  \"2015-06-28\", \"2015-05-28\"]\n",
    "\n",
    "X_2016_05, y_2016_05 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_2016_05.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_2016_05.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_05.to_csv(os.path.join(OUT_DIR1, \"X_2016_05.csv\"), index=False)\n",
    "y_2016_05.to_csv(os.path.join(OUT_DIR1, \"y_2016_05.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp = \"2016-06-28\"\n",
    "timestamp_lags = [\"2016-05-28\", \"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\",\n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\", \"2015-09-28\", \n",
    "                  \"2015-08-28\", \"2015-07-28\", \"2015-06-28\"]\n",
    "\n",
    "X_2016_06 = extract_X_test(df_train, df_test, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_2016_06.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_06.to_csv(os.path.join(OUT_DIR1, \"X_2016_06.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
