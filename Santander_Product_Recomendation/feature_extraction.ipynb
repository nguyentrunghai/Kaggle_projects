{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype_ser(ser):\n",
    "    \n",
    "    if ser.dtype == int:\n",
    "        return ser.astype(np.int32)\n",
    "    \n",
    "    if ser.dtype == float:\n",
    "        return ser.astype(np.float32)\n",
    "    \n",
    "    if ser.dtype == np.object:\n",
    "        return ser.astype(\"category\")\n",
    "    \n",
    "    return ser\n",
    "    \n",
    "\n",
    "def change_dtype_df(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"])\n",
    "    df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"])\n",
    "    \n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = change_dtype_ser(df[col])\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtype_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIR = \"data/data_\"\n",
    "OUT_DIR1 = \"data/data1_\"\n",
    "OUT_DIR2 = \"data/data2_\"\n",
    "OUT_DIR3 = \"data/data3_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hai/opt/python_virtual_environments/xgboost/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing types 9566.76 MB\n",
      "Memory usage after changing types 4612.80 MB\n",
      "Memory usage before changing types 116.20 MB\n",
      "Memory usage after changing types 49.28 MB\n"
     ]
    }
   ],
   "source": [
    "df_train = load_csv(os.path.join(INP_DIR, \"train_cleaned.csv\"))\n",
    "df_test = load_csv(os.path.join(INP_DIR, \"test_cleaned.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop this column becuase it is too imbalanced\n",
    "df_train = df_train.drop([\"ind_empleado\"], axis=1)\n",
    "df_test = df_test.drop([\"ind_empleado\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days from when the data is recorded \n",
    "df_train[\"fecha_alta\"] = (df_train[\"fecha_alta\"] - df_train[\"fecha_dato\"]).dt.days\n",
    "df_test[\"fecha_alta\"] = (df_test[\"fecha_alta\"] - df_test[\"fecha_dato\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum().sum(), df_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13647309, 92), (929615, 20))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fecha_dato', 'ncodpers', 'ind_empleado', 'pais_residencia', 'sexo',\n",
       "       'age', 'fecha_alta', 'ind_nuevo', 'antiguedad', 'indrel', 'indrel_1mes',\n",
       "       'tiprel_1mes', 'indresi', 'indext', 'canal_entrada', 'indfall',\n",
       "       'cod_prov', 'ind_actividad_cliente', 'renta', 'segmento',\n",
       "       'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n",
       "       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n",
       "       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n",
       "       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
       "       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n",
       "       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n",
       "       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n",
       "       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',\n",
       "       'ind_ahor_fin_ult1_NEW_PUR', 'ind_aval_fin_ult1_NEW_PUR',\n",
       "       'ind_cco_fin_ult1_NEW_PUR', 'ind_cder_fin_ult1_NEW_PUR',\n",
       "       'ind_cno_fin_ult1_NEW_PUR', 'ind_ctju_fin_ult1_NEW_PUR',\n",
       "       'ind_ctma_fin_ult1_NEW_PUR', 'ind_ctop_fin_ult1_NEW_PUR',\n",
       "       'ind_ctpp_fin_ult1_NEW_PUR', 'ind_deco_fin_ult1_NEW_PUR',\n",
       "       'ind_deme_fin_ult1_NEW_PUR', 'ind_dela_fin_ult1_NEW_PUR',\n",
       "       'ind_ecue_fin_ult1_NEW_PUR', 'ind_fond_fin_ult1_NEW_PUR',\n",
       "       'ind_hip_fin_ult1_NEW_PUR', 'ind_plan_fin_ult1_NEW_PUR',\n",
       "       'ind_pres_fin_ult1_NEW_PUR', 'ind_reca_fin_ult1_NEW_PUR',\n",
       "       'ind_tjcr_fin_ult1_NEW_PUR', 'ind_valo_fin_ult1_NEW_PUR',\n",
       "       'ind_viv_fin_ult1_NEW_PUR', 'ind_nomina_ult1_NEW_PUR',\n",
       "       'ind_nom_pens_ult1_NEW_PUR', 'ind_recibo_ult1_NEW_PUR',\n",
       "       'ind_ahor_fin_ult1_PUR_OR_CANCEL', 'ind_aval_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_cco_fin_ult1_PUR_OR_CANCEL', 'ind_cder_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_cno_fin_ult1_PUR_OR_CANCEL', 'ind_ctju_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_ctma_fin_ult1_PUR_OR_CANCEL', 'ind_ctop_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_ctpp_fin_ult1_PUR_OR_CANCEL', 'ind_deco_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_deme_fin_ult1_PUR_OR_CANCEL', 'ind_dela_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_ecue_fin_ult1_PUR_OR_CANCEL', 'ind_fond_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_hip_fin_ult1_PUR_OR_CANCEL', 'ind_plan_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_pres_fin_ult1_PUR_OR_CANCEL', 'ind_reca_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_tjcr_fin_ult1_PUR_OR_CANCEL', 'ind_valo_fin_ult1_PUR_OR_CANCEL',\n",
       "       'ind_viv_fin_ult1_PUR_OR_CANCEL', 'ind_nomina_ult1_PUR_OR_CANCEL',\n",
       "       'ind_nom_pens_ult1_PUR_OR_CANCEL', 'ind_recibo_ult1_PUR_OR_CANCEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fecha_dato', 'ncodpers', 'ind_empleado', 'pais_residencia', 'sexo',\n",
       "       'age', 'fecha_alta', 'ind_nuevo', 'antiguedad', 'indrel', 'indrel_1mes',\n",
       "       'tiprel_1mes', 'indresi', 'indext', 'canal_entrada', 'indfall',\n",
       "       'cod_prov', 'ind_actividad_cliente', 'renta', 'segmento'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1', 'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1', 'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1', 'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1', 'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n"
     ]
    }
   ],
   "source": [
    "PROD_COLS = [col for col in df_train.columns if re.match(r\"^ind_.*_ult1$\", col)]\n",
    "print(PROD_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ind_ahor_fin_ult1_NEW_PUR', 'ind_aval_fin_ult1_NEW_PUR', 'ind_cco_fin_ult1_NEW_PUR', 'ind_cder_fin_ult1_NEW_PUR', 'ind_cno_fin_ult1_NEW_PUR', 'ind_ctju_fin_ult1_NEW_PUR', 'ind_ctma_fin_ult1_NEW_PUR', 'ind_ctop_fin_ult1_NEW_PUR', 'ind_ctpp_fin_ult1_NEW_PUR', 'ind_deco_fin_ult1_NEW_PUR', 'ind_deme_fin_ult1_NEW_PUR', 'ind_dela_fin_ult1_NEW_PUR', 'ind_ecue_fin_ult1_NEW_PUR', 'ind_fond_fin_ult1_NEW_PUR', 'ind_hip_fin_ult1_NEW_PUR', 'ind_plan_fin_ult1_NEW_PUR', 'ind_pres_fin_ult1_NEW_PUR', 'ind_reca_fin_ult1_NEW_PUR', 'ind_tjcr_fin_ult1_NEW_PUR', 'ind_valo_fin_ult1_NEW_PUR', 'ind_viv_fin_ult1_NEW_PUR', 'ind_nomina_ult1_NEW_PUR', 'ind_nom_pens_ult1_NEW_PUR', 'ind_recibo_ult1_NEW_PUR'] \n",
      "\n",
      "ind_recibo_ult1_NEW_PUR      27.197280\n",
      "ind_nom_pens_ult1_NEW_PUR    15.048197\n",
      "ind_nomina_ult1_NEW_PUR      13.101134\n",
      "ind_cco_fin_ult1_NEW_PUR     12.426018\n",
      "ind_tjcr_fin_ult1_NEW_PUR    12.304237\n",
      "ind_cno_fin_ult1_NEW_PUR      6.601516\n",
      "ind_ecue_fin_ult1_NEW_PUR     4.682679\n",
      "ind_dela_fin_ult1_NEW_PUR     2.255774\n",
      "ind_reca_fin_ult1_NEW_PUR     1.639950\n",
      "ind_ctma_fin_ult1_NEW_PUR     1.243010\n",
      "ind_valo_fin_ult1_NEW_PUR     0.860982\n",
      "ind_ctop_fin_ult1_NEW_PUR     0.689141\n",
      "ind_fond_fin_ult1_NEW_PUR     0.656654\n",
      "ind_deco_fin_ult1_NEW_PUR     0.545703\n",
      "ind_ctpp_fin_ult1_NEW_PUR     0.429604\n",
      "ind_plan_fin_ult1_NEW_PUR     0.109709\n",
      "ind_ctju_fin_ult1_NEW_PUR     0.086986\n",
      "ind_deme_fin_ult1_NEW_PUR     0.044381\n",
      "ind_pres_fin_ult1_NEW_PUR     0.026096\n",
      "ind_cder_fin_ult1_NEW_PUR     0.024143\n",
      "ind_hip_fin_ult1_NEW_PUR      0.013314\n",
      "ind_viv_fin_ult1_NEW_PUR      0.012427\n",
      "ind_aval_fin_ult1_NEW_PUR     0.000710\n",
      "ind_ahor_fin_ult1_NEW_PUR     0.000355\n",
      "dtype: float64 \n",
      "\n",
      "['ind_recibo_ult1_NEW_PUR', 'ind_nom_pens_ult1_NEW_PUR', 'ind_nomina_ult1_NEW_PUR', 'ind_cco_fin_ult1_NEW_PUR', 'ind_tjcr_fin_ult1_NEW_PUR', 'ind_cno_fin_ult1_NEW_PUR', 'ind_ecue_fin_ult1_NEW_PUR', 'ind_dela_fin_ult1_NEW_PUR', 'ind_reca_fin_ult1_NEW_PUR', 'ind_ctma_fin_ult1_NEW_PUR', 'ind_valo_fin_ult1_NEW_PUR', 'ind_ctop_fin_ult1_NEW_PUR', 'ind_fond_fin_ult1_NEW_PUR', 'ind_deco_fin_ult1_NEW_PUR', 'ind_ctpp_fin_ult1_NEW_PUR', 'ind_plan_fin_ult1_NEW_PUR', 'ind_ctju_fin_ult1_NEW_PUR', 'ind_deme_fin_ult1_NEW_PUR', 'ind_pres_fin_ult1_NEW_PUR', 'ind_cder_fin_ult1_NEW_PUR', 'ind_hip_fin_ult1_NEW_PUR', 'ind_viv_fin_ult1_NEW_PUR', 'ind_aval_fin_ult1_NEW_PUR', 'ind_ahor_fin_ult1_NEW_PUR']\n"
     ]
    }
   ],
   "source": [
    "NEW_PURCH_COLS = [col for col in df_train.columns if re.match(r\"^ind_.*_ult1_NEW_PUR$\", col)]\n",
    "print(NEW_PURCH_COLS, \"\\n\")\n",
    "\n",
    "prod_popul = df_train[NEW_PURCH_COLS].sum(axis=0)\n",
    "prod_popul = prod_popul.sort_values(ascending=False)/prod_popul.sum() * 100\n",
    "print(prod_popul, \"\\n\")\n",
    "\n",
    "NEW_PURCH_COLS = prod_popul.sort_values(ascending=False).index.to_list()\n",
    "print(NEW_PURCH_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ind_ahor_fin_ult1_PUR_OR_CANCEL', 'ind_aval_fin_ult1_PUR_OR_CANCEL', 'ind_cco_fin_ult1_PUR_OR_CANCEL', 'ind_cder_fin_ult1_PUR_OR_CANCEL', 'ind_cno_fin_ult1_PUR_OR_CANCEL', 'ind_ctju_fin_ult1_PUR_OR_CANCEL', 'ind_ctma_fin_ult1_PUR_OR_CANCEL', 'ind_ctop_fin_ult1_PUR_OR_CANCEL', 'ind_ctpp_fin_ult1_PUR_OR_CANCEL', 'ind_deco_fin_ult1_PUR_OR_CANCEL', 'ind_deme_fin_ult1_PUR_OR_CANCEL', 'ind_dela_fin_ult1_PUR_OR_CANCEL', 'ind_ecue_fin_ult1_PUR_OR_CANCEL', 'ind_fond_fin_ult1_PUR_OR_CANCEL', 'ind_hip_fin_ult1_PUR_OR_CANCEL', 'ind_plan_fin_ult1_PUR_OR_CANCEL', 'ind_pres_fin_ult1_PUR_OR_CANCEL', 'ind_reca_fin_ult1_PUR_OR_CANCEL', 'ind_tjcr_fin_ult1_PUR_OR_CANCEL', 'ind_valo_fin_ult1_PUR_OR_CANCEL', 'ind_viv_fin_ult1_PUR_OR_CANCEL', 'ind_nomina_ult1_PUR_OR_CANCEL', 'ind_nom_pens_ult1_PUR_OR_CANCEL', 'ind_recibo_ult1_PUR_OR_CANCEL']\n"
     ]
    }
   ],
   "source": [
    "PURCH_CANCEL_COLS = [col for col in df_train.columns if re.match(r\"^ind_.*_ult1_PUR_OR_CANCEL$\", col)]\n",
    "print(PURCH_CANCEL_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of prod\n",
    "df_train[\"TOTAL_PRODS\"] = df_train[PROD_COLS].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ind_empleado', 'pais_residencia', 'sexo', 'age', 'fecha_alta', 'ind_nuevo', 'antiguedad', 'indrel', 'indrel_1mes', 'tiprel_1mes', 'indresi', 'indext', 'canal_entrada', 'indfall', 'cod_prov', 'ind_actividad_cliente', 'renta', 'segmento']\n"
     ]
    }
   ],
   "source": [
    "DEMOG_COLS = [col for col in df_train.columns \n",
    "    if col not in PROD_COLS + NEW_PURCH_COLS + PURCH_CANCEL_COLS + [\"fecha_dato\", \"ncodpers\", \"TOTAL_PRODS\"]]\n",
    "print(DEMOG_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_COLS = [\"fecha_alta\", \"ind_nuevo\", \"antiguedad\", \"indrel\", \"tiprel_1mes\", \n",
    "            \"ind_actividad_cliente\", \"renta\", \"segmento\", \"TOTAL_PRODS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_unique(x):\n",
    "    return len(x) == x.nunique()\n",
    "\n",
    "\n",
    "def _onehot_reduce(df):\n",
    "    df = df.copy()\n",
    "    values = df.values\n",
    "    \n",
    "    new_values = values.copy()\n",
    "    for i in range(values.shape[0]):\n",
    "        row = values[i]\n",
    "        if row.sum() > 1:\n",
    "            new_row = np.zeros([row.shape[0]], dtype=np.int)\n",
    "            \n",
    "            where_1 = np.where(row == 1)[0]\n",
    "            np.random.shuffle(where_1)\n",
    "            idx = where_1[0]\n",
    "            new_row[idx] = 1\n",
    "            \n",
    "            new_values[i] = new_row\n",
    "    \n",
    "    new_df = pd.DataFrame(data=new_values, index=df.index, columns=df.columns)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def extract_subset(df, row_filter, cols):\n",
    "    df = df.copy()\n",
    "    return df.loc[row_filter, cols]\n",
    "\n",
    "\n",
    "\n",
    "def extract_y(df, timestamp, y_cols=NEW_PURCH_COLS):\n",
    "    # only use row when customer buys at least one product\n",
    "    any_new_pur = df[y_cols].sum(axis=1) > 0\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp) & any_new_pur\n",
    "    cols = [\"ncodpers\"] + y_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "    \n",
    "    df_out.columns = [col.replace(\"_NEW_PUR\", \"\") if col.endswith(\"_NEW_PUR\") else col for col in df_out.columns]\n",
    "    \n",
    "    id_col = df_out[\"ncodpers\"]\n",
    "    pur_cols = [col for col in df_out.columns if col != \"ncodpers\"]\n",
    "    df_out = _onehot_reduce(df_out[pur_cols])\n",
    "    df_out[\"ncodpers\"] = id_col\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def extract_x_demog(df, timestamp, customer_ids, demog_cols=DEMOG_COLS):\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp) & df[\"ncodpers\"].isin(customer_ids)\n",
    "    cols = [\"ncodpers\"] + demog_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# This function can give dataframe having less rows than len(customer_ids)\n",
    "# This is becuse number of row at lag_timestamp is not the same as at timestamp\n",
    "def extract_lag_features(df, timestamp_lag, customer_ids, suffix=\"_LAG\", \n",
    "                         sel_cols=PROD_COLS+PURCH_CANCEL_COLS+LAG_COLS):\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp_lag) & df[\"ncodpers\"].isin(customer_ids)\n",
    "    cols = [\"ncodpers\"] + sel_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "    \n",
    "    df_out.columns = [\"ncodpers\"] + [col + suffix for col in df_out.columns if col != \"ncodpers\"]\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def extract_X_y_train(df, timestamp, timestamp_lags):\n",
    "    y_train = extract_y(df, timestamp)\n",
    "    print(\"y_train.shape\", y_train.shape)\n",
    "    \n",
    "    x_demog = extract_x_demog(df, timestamp, customer_ids=y_train[\"ncodpers\"])\n",
    "    print(\"x_demog.shape:\", x_demog.shape)\n",
    "    assert y_train.shape[0] == x_demog.shape[0], \"x_demog must have the same number of rows as y_train\"\n",
    "    \n",
    "    x_lags = []\n",
    "    for t, lag in enumerate(timestamp_lags):\n",
    "        assert pd.to_datetime(lag) < pd.to_datetime(timestamp), lag + \" lag is not before timestamp \" +  timestamp\n",
    "        lag_label = \"_LAG%d\" % (t + 1)\n",
    "        \n",
    "        x_lag = extract_lag_features(df, lag, customer_ids=y_train[\"ncodpers\"], suffix=lag_label)\n",
    "        print(lag, lag_label, x_lag.shape)\n",
    "        x_lags.append(x_lag)\n",
    "    \n",
    "    X_train = y_train[[\"ncodpers\"]].merge(x_demog, how=\"left\", on=\"ncodpers\")\n",
    "    print(\"Nulls after merging y and x_demog:\", X_train.isnull().sum().sum())\n",
    "    \n",
    "    for t, x_lag in enumerate(x_lags):\n",
    "        X_train = X_train.merge(x_lag, how=\"left\", on=\"ncodpers\")\n",
    "        print(\"Nulls at %d:\" %(t + 1), X_train.isnull().sum().sum())\n",
    "    \n",
    "    print(\"X_train.shape:\", X_train.shape)\n",
    "    print(\"y_train.shape:\", y_train.shape)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def extract_X_test(train, test, timestamp, timestamp_lags):\n",
    "    x_demog = extract_x_demog(test, timestamp, customer_ids=test[\"ncodpers\"])\n",
    "    print(\"x_demog.shape:\", x_demog.shape)\n",
    "    print(\"Nulls of x_demog:\", x_demog.isnull().sum().sum())\n",
    "    \n",
    "    x_lags = []\n",
    "    for t, lag in enumerate(timestamp_lags):\n",
    "        assert pd.to_datetime(lag) < pd.to_datetime(timestamp), lag + \" lag is not before timestamp \" +  timestamp\n",
    "        lag_label = \"_LAG%d\" % (t + 1)\n",
    "        \n",
    "        x_lag = extract_lag_features(train, lag, customer_ids=test[\"ncodpers\"], suffix=lag_label)\n",
    "        print(lag, lag_label, x_lag.shape)\n",
    "        x_lags.append(x_lag)\n",
    "        \n",
    "    X_test = x_demog\n",
    "    for t, x_lag in enumerate(x_lags):\n",
    "        X_test = X_test.merge(x_lag, how=\"left\", on=\"ncodpers\")\n",
    "        print(\"Nulls at %d:\" %(t + 1), X_test.isnull().sum().sum())\n",
    "    \n",
    "    print(\"X_test.shape:\", X_test.shape)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x_demog(df, timestamp, customer_ids, demog_cols=DEMOG_COLS):\n",
    "    row_filter = (df[\"fecha_dato\"] == timestamp) & df[\"ncodpers\"].isin(customer_ids)\n",
    "    cols = [\"ncodpers\"] + demog_cols\n",
    "    \n",
    "    df_out = extract_subset(df, row_filter, cols)\n",
    "    assert _is_unique(df_out[\"ncodpers\"]), \"ncodpers must be unique\"\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-28T00:00:00.000000000', '2015-02-28T00:00:00.000000000',\n",
       "       '2015-03-28T00:00:00.000000000', '2015-04-28T00:00:00.000000000',\n",
       "       '2015-05-28T00:00:00.000000000', '2015-06-28T00:00:00.000000000',\n",
       "       '2015-07-28T00:00:00.000000000', '2015-08-28T00:00:00.000000000',\n",
       "       '2015-09-28T00:00:00.000000000', '2015-10-28T00:00:00.000000000',\n",
       "       '2015-11-28T00:00:00.000000000', '2015-12-28T00:00:00.000000000',\n",
       "       '2016-01-28T00:00:00.000000000', '2016-02-28T00:00:00.000000000',\n",
       "       '2016-03-28T00:00:00.000000000', '2016-04-28T00:00:00.000000000',\n",
       "       '2016-05-28T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `2016-05`  with 6-month lags to predict `2016-06`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_04`, `y_2016_04`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (26791, 25)\n",
      "x_demog.shape: (26791, 18)\n",
      "2016-03-28 _LAG1 (26736, 58)\n",
      "2016-02-28 _LAG2 (25051, 58)\n",
      "2016-01-28 _LAG3 (24255, 58)\n",
      "2015-12-28 _LAG4 (23556, 58)\n",
      "2015-11-28 _LAG5 (22805, 58)\n",
      "2015-10-28 _LAG6 (22308, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 3135\n",
      "Nulls at 2: 102315\n",
      "Nulls at 3: 246867\n",
      "Nulls at 4: 431262\n",
      "Nulls at 5: 658464\n",
      "Nulls at 6: 913995\n",
      "X_train.shape: (26791, 360)\n",
      "y_train.shape: (26791, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-04-28\"\n",
    "timestamp_lags = [\"2016-03-28\", \"2016-02-28\", \"2016-01-28\", \n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\"]\n",
    "\n",
    "X_2016_04, y_2016_04 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_04.to_csv(os.path.join(OUT_DIR1, \"X_2016_04.csv\"), index=False)\n",
    "y_2016_04.to_csv(os.path.join(OUT_DIR1, \"y_2016_04.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_05`, `y_2016_05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (27916, 25)\n",
      "x_demog.shape: (27916, 18)\n",
      "2016-04-28 _LAG1 (27875, 58)\n",
      "2016-03-28 _LAG2 (26372, 58)\n",
      "2016-02-28 _LAG3 (25692, 58)\n",
      "2016-01-28 _LAG4 (24936, 58)\n",
      "2015-12-28 _LAG5 (23968, 58)\n",
      "2015-11-28 _LAG6 (23553, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 2337\n",
      "Nulls at 2: 90345\n",
      "Nulls at 3: 217113\n",
      "Nulls at 4: 386973\n",
      "Nulls at 5: 612009\n",
      "Nulls at 6: 860700\n",
      "X_train.shape: (27916, 360)\n",
      "y_train.shape: (27916, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-05-28\"\n",
    "timestamp_lags = [\"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\",\n",
    "                  \"2015-12-28\", \"2015-11-28\"]\n",
    "\n",
    "X_2016_05, y_2016_05 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_05.to_csv(os.path.join(OUT_DIR1, \"X_2016_05.csv\"), index=False)\n",
    "y_2016_05.to_csv(os.path.join(OUT_DIR1, \"y_2016_05.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_demog.shape: (929615, 18)\n",
      "Nulls of x_demog: 0\n",
      "2016-05-28 _LAG1 (929615, 58)\n",
      "2016-04-28 _LAG2 (925252, 58)\n",
      "2016-03-28 _LAG3 (920975, 58)\n",
      "2016-02-28 _LAG4 (915679, 58)\n",
      "2016-01-28 _LAG5 (909885, 58)\n",
      "2015-12-28 _LAG6 (903429, 58)\n",
      "Nulls at 1: 0\n",
      "Nulls at 2: 248691\n",
      "Nulls at 3: 741171\n",
      "Nulls at 4: 1535523\n",
      "Nulls at 5: 2660133\n",
      "Nulls at 6: 4152735\n",
      "X_test.shape: (929615, 360)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-06-28\"\n",
    "timestamp_lags = [\"2016-05-28\", \"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\",\n",
    "                  \"2015-12-28\"]\n",
    "\n",
    "X_2016_06 = extract_X_test(df_train, df_test, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_06.to_csv(os.path.join(OUT_DIR1, \"X_2016_06.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `2016-05`  with 12-month lags to predict `2016-06`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_04`, `y_2016_04`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (26791, 25)\n",
      "x_demog.shape: (26791, 18)\n",
      "2016-03-28 _LAG1 (26736, 58)\n",
      "2016-02-28 _LAG2 (25051, 58)\n",
      "2016-01-28 _LAG3 (24255, 58)\n",
      "2015-12-28 _LAG4 (23556, 58)\n",
      "2015-11-28 _LAG5 (22805, 58)\n",
      "2015-10-28 _LAG6 (22308, 58)\n",
      "2015-09-28 _LAG7 (21819, 58)\n",
      "2015-08-28 _LAG8 (21415, 58)\n",
      "2015-07-28 _LAG9 (21143, 58)\n",
      "2015-06-28 _LAG10 (20223, 58)\n",
      "2015-05-28 _LAG11 (20064, 58)\n",
      "2015-04-28 _LAG12 (19885, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 3135\n",
      "Nulls at 2: 102315\n",
      "Nulls at 3: 246867\n",
      "Nulls at 4: 431262\n",
      "Nulls at 5: 658464\n",
      "Nulls at 6: 913995\n",
      "Nulls at 7: 1197399\n",
      "Nulls at 8: 1503831\n",
      "Nulls at 9: 1825767\n",
      "Nulls at 10: 2200143\n",
      "Nulls at 11: 2583582\n",
      "Nulls at 12: 2977224\n",
      "X_train.shape: (26791, 702)\n",
      "y_train.shape: (26791, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-04-28\"\n",
    "timestamp_lags = [\"2016-03-28\", \"2016-02-28\", \"2016-01-28\", \n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\",\n",
    "                  \"2015-09-28\", \"2015-08-28\", \"2015-07-28\", \n",
    "                  \"2015-06-28\", \"2015-05-28\", \"2015-04-28\"]\n",
    "\n",
    "X_2016_04, y_2016_04 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_04.to_csv(os.path.join(OUT_DIR2, \"X_2016_04.csv\"), index=False)\n",
    "y_2016_04.to_csv(os.path.join(OUT_DIR2, \"y_2016_04.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_05`, `y_2016_05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (27916, 25)\n",
      "x_demog.shape: (27916, 18)\n",
      "2016-04-28 _LAG1 (27875, 58)\n",
      "2016-03-28 _LAG2 (26372, 58)\n",
      "2016-02-28 _LAG3 (25692, 58)\n",
      "2016-01-28 _LAG4 (24936, 58)\n",
      "2015-12-28 _LAG5 (23968, 58)\n",
      "2015-11-28 _LAG6 (23553, 58)\n",
      "2015-10-28 _LAG7 (23179, 58)\n",
      "2015-09-28 _LAG8 (22725, 58)\n",
      "2015-08-28 _LAG9 (22315, 58)\n",
      "2015-07-28 _LAG10 (22053, 58)\n",
      "2015-06-28 _LAG11 (21110, 58)\n",
      "2015-05-28 _LAG12 (20963, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 2337\n",
      "Nulls at 2: 90345\n",
      "Nulls at 3: 217113\n",
      "Nulls at 4: 386973\n",
      "Nulls at 5: 612009\n",
      "Nulls at 6: 860700\n",
      "Nulls at 7: 1130709\n",
      "Nulls at 8: 1426596\n",
      "Nulls at 9: 1745853\n",
      "Nulls at 10: 2080044\n",
      "Nulls at 11: 2467986\n",
      "Nulls at 12: 2864307\n",
      "X_train.shape: (27916, 702)\n",
      "y_train.shape: (27916, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-05-28\"\n",
    "timestamp_lags = [\"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\",\n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\", \"2015-09-28\",\n",
    "                 \"2015-08-28\", \"2015-07-28\", \"2015-06-28\", \"2015-05-28\"]\n",
    "\n",
    "X_2016_05, y_2016_05 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_05.to_csv(os.path.join(OUT_DIR2, \"X_2016_05.csv\"), index=False)\n",
    "y_2016_05.to_csv(os.path.join(OUT_DIR2, \"y_2016_05.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_demog.shape: (929615, 18)\n",
      "Nulls of x_demog: 0\n",
      "2016-05-28 _LAG1 (929615, 58)\n",
      "2016-04-28 _LAG2 (925252, 58)\n",
      "2016-03-28 _LAG3 (920975, 58)\n",
      "2016-02-28 _LAG4 (915679, 58)\n",
      "2016-01-28 _LAG5 (909885, 58)\n",
      "2015-12-28 _LAG6 (903429, 58)\n",
      "2015-11-28 _LAG7 (896458, 58)\n",
      "2015-10-28 _LAG8 (881573, 58)\n",
      "2015-09-28 _LAG9 (854574, 58)\n",
      "2015-08-28 _LAG10 (832230, 58)\n",
      "2015-07-28 _LAG11 (818424, 58)\n",
      "2015-06-28 _LAG12 (622404, 58)\n",
      "Nulls at 1: 0\n",
      "Nulls at 2: 248691\n",
      "Nulls at 3: 741171\n",
      "Nulls at 4: 1535523\n",
      "Nulls at 5: 2660133\n",
      "Nulls at 6: 4152735\n",
      "Nulls at 7: 6042684\n",
      "Nulls at 8: 8781078\n",
      "Nulls at 9: 13058415\n",
      "Nulls at 10: 18609360\n",
      "Nulls at 11: 24947247\n",
      "Nulls at 12: 42458274\n",
      "X_test.shape: (929615, 702)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-06-28\"\n",
    "timestamp_lags = [\"2016-05-28\", \"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\",\n",
    "                  \"2015-12-28\", \"2015-11-28\", \"2015-10-28\", \"2015-09-28\", \"2015-08-28\",\n",
    "                 \"2015-07-28\", \"2015-06-28\"]\n",
    "\n",
    "X_2016_06 = extract_X_test(df_train, df_test, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_06.to_csv(os.path.join(OUT_DIR2, \"X_2016_06.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `2015-06`  with 4-month lags to predict `2016-06`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2015_05`, `y_2015_05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (21422, 25)\n",
      "x_demog.shape: (21422, 19)\n",
      "2015-04-28 _LAG1 (21203, 58)\n",
      "2015-03-28 _LAG2 (20484, 58)\n",
      "2015-02-28 _LAG3 (20057, 58)\n",
      "2015-01-28 _LAG4 (19526, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 12483\n",
      "Nulls at 2: 65949\n",
      "Nulls at 3: 143754\n",
      "Nulls at 4: 251826\n",
      "X_train.shape: (21422, 247)\n",
      "y_train.shape: (21422, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2015-05-28\"\n",
    "timestamp_lags = [\"2015-04-28\", \"2015-03-28\", \"2015-02-28\", \"2015-01-28\"]\n",
    "\n",
    "X_2015_05, y_2015_05 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2015_05.to_csv(os.path.join(OUT_DIR3, \"X_2015_05.csv\"), index=False)\n",
    "y_2015_05.to_csv(os.path.join(OUT_DIR3, \"y_2015_05.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2015_06`, `y_2015_06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (33519, 25)\n",
      "x_demog.shape: (33519, 19)\n",
      "2015-05-28 _LAG1 (33318, 58)\n",
      "2015-04-28 _LAG2 (32453, 58)\n",
      "2015-03-28 _LAG3 (32052, 58)\n",
      "2015-02-28 _LAG4 (31503, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 11457\n",
      "Nulls at 2: 72219\n",
      "Nulls at 3: 155838\n",
      "Nulls at 4: 270750\n",
      "X_train.shape: (33519, 247)\n",
      "y_train.shape: (33519, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2015-06-28\"\n",
    "timestamp_lags = [\"2015-05-28\", \"2015-04-28\", \"2015-03-28\", \"2015-02-28\"]\n",
    "\n",
    "X_2015_06, y_2015_06 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2015_06.to_csv(os.path.join(OUT_DIR3, \"X_2015_06.csv\"), index=False)\n",
    "y_2015_06.to_csv(os.path.join(OUT_DIR3, \"y_2015_06.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_05`, `y_2016_05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (27916, 25)\n",
      "x_demog.shape: (27916, 19)\n",
      "2016-04-28 _LAG1 (27875, 58)\n",
      "2016-03-28 _LAG2 (26372, 58)\n",
      "2016-02-28 _LAG3 (25692, 58)\n",
      "2016-01-28 _LAG4 (24936, 58)\n",
      "Nulls after merging y and x_demog: 0\n",
      "Nulls at 1: 2337\n",
      "Nulls at 2: 90345\n",
      "Nulls at 3: 217113\n",
      "Nulls at 4: 386973\n",
      "X_train.shape: (27916, 247)\n",
      "y_train.shape: (27916, 25)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-05-28\"\n",
    "timestamp_lags = [\"2016-04-28\", \"2016-03-28\", \"2016-02-28\", \"2016-01-28\"]\n",
    "\n",
    "X_2016_05, y_2016_05 = extract_X_y_train(df_train, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_05.to_csv(os.path.join(OUT_DIR3, \"X_2016_05.csv\"), index=False)\n",
    "y_2016_05.to_csv(os.path.join(OUT_DIR3, \"y_2016_05.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `X_2016_06`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_demog.shape: (929615, 19)\n",
      "Nulls of x_demog: 0\n",
      "2016-05-28 _LAG1 (929615, 58)\n",
      "2016-04-28 _LAG2 (925252, 58)\n",
      "2016-03-28 _LAG3 (920975, 58)\n",
      "2016-02-28 _LAG4 (915679, 58)\n",
      "Nulls at 1: 0\n",
      "Nulls at 2: 248691\n",
      "Nulls at 3: 741171\n",
      "Nulls at 4: 1535523\n",
      "X_test.shape: (929615, 247)\n"
     ]
    }
   ],
   "source": [
    "timestamp = \"2016-06-28\"\n",
    "timestamp_lags = [\"2016-05-28\", \"2016-04-28\", \"2016-03-28\", \"2016-02-28\"]\n",
    "\n",
    "X_2016_06 = extract_X_test(df_train, df_test, timestamp, timestamp_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2016_06.to_csv(os.path.join(OUT_DIR3, \"X_2016_06.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
