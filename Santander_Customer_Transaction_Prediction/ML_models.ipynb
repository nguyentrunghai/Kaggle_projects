{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df):\n",
    "    \"\"\"\n",
    "    change types of columns to reduce memory size\n",
    "    :param df: dataframe\n",
    "    :return df: dataframe\n",
    "    \"\"\"\n",
    "    memory = df.memory_usage().sum() / 10**6\n",
    "    print(\"Memory usage before changing types %0.2f MB\" % memory)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == \"object\") and (df[col].nunique() < df.shape[0]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        elif df[col].dtype == int:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "    memory = df.memory_usage().sum() / 10 ** 6\n",
    "    print(\"Memory usage after changing types %0.2f MB\" % memory)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = change_dtypes(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(dfs):\n",
    "    \"\"\"assume that indices match\"\"\"\n",
    "    print(\"Shape of dfs\")\n",
    "    for df in dfs:\n",
    "        print(df.shape)\n",
    "        \n",
    "    df_concat = pd.concat(dfs, axis=\"columns\")\n",
    "    print(\"shape of concatenated df\", df_concat.shape)\n",
    "    print(\"Number of nulls:\", df_concat.isnull().sum().sum())\n",
    "    \n",
    "    features = df_concat.columns.to_list()\n",
    "    return features, df_concat.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def write_submit_csv(estimator, X_test, id_test, out):\n",
    "    \"\"\"\n",
    "    :param estimator: a sklearn estimator that has predict_proba() method\n",
    "    :param X_test: df or array\n",
    "    :param id_test: dataframe containing column \"SK_ID_CURR\"\n",
    "    :param out: str, csv output file name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    prob_test = estimator.predict_proba(X_test)[:, 1]\n",
    "    submit = id_test\n",
    "    submit[\"TARGET\"] = prob_test\n",
    "    submit.to_csv(out, index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "def feature_importance_df(estimator, features):\n",
    "    \"\"\"\n",
    "    :param estimator: an estimator object that has feature_importances_ attribute\n",
    "    :param features: list of str, list of feature names\n",
    "    :return: feature_imp, dataframe\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame({\"feature\": features, \"importance\": estimator.feature_importances_})\n",
    "    feature_imp = feature_imp.sort_values(by=[\"importance\"], ascending=False)\n",
    "    \n",
    "    feature_imp[\"rank\"] = np.arange(feature_imp.shape[0]) + 1\n",
    "    return feature_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def run_hyperopt(classifier,\n",
    "                 params_tuned, \n",
    "                 X_train, y_train,\n",
    "                 X_val, y_val,\n",
    "                 num_eval,\n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {\"n_jobs\": 20, \"n_estimators\": 100}\n",
    "    \n",
    "    def objective(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        auc = roc_auc(classifier, X_val, y_val)\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    best_model = classifier.set_params(**params_fixed, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    \n",
    "    return trials, best_params, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_y_hat(submit_csv_files):\n",
    "    y_hats = [pd.read_csv(f) for f in submit_csv_files]\n",
    "    result = y_hats[0][[\"SK_ID_CURR\"]]\n",
    "    result[\"TARGET\"] = 0.\n",
    "    for y in y_hats:\n",
    "        result[\"TARGET\"] = result[\"TARGET\"] + y[\"TARGET\"]\n",
    "    \n",
    "    result[\"TARGET\"] = result[\"TARGET\"] / len(y_hats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP_DIR = \"data/data_\"\n",
    "SUB_DIR = \"data/submit_\"\n",
    "MODELS_DIR = \"data/models_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `X_org`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org_train = load_csv(os.path.join(INP_DIR, \"X_org_train.csv\"))\n",
    "X_org_test = load_csv(os.path.join(INP_DIR, \"X_org_test.csv\"))\n",
    "\n",
    "X_org_train.shape, X_org_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `X_q10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q10_train = load_csv(os.path.join(INP_DIR, \"X_q10_train.csv\"))\n",
    "X_q10_test = load_csv(os.path.join(INP_DIR, \"X_q10_test.csv\"))\n",
    "\n",
    "X_q10_train.shape, X_q10_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q10_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q10_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `X_valcount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valcount_train = load_csv(os.path.join(INP_DIR, \"X_valcount_train.csv\"))\n",
    "X_valcount_test = load_csv(os.path.join(INP_DIR, \"X_valcount_test.csv\"))\n",
    "\n",
    "X_valcount_train.shape, X_valcount_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valcount_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valcount_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `X_target_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target_mean_train = load_csv(os.path.join(INP_DIR, \"X_target_mean_train.csv\"))\n",
    "X_target_mean_test = load_csv(os.path.join(INP_DIR, \"X_target_mean_test.csv\"))\n",
    "\n",
    "X_target_mean_train.shape, X_target_mean_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target_mean_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target_mean_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `X_woe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_woe_train = load_csv(os.path.join(INP_DIR, \"X_woe_train.csv\"))\n",
    "X_woe_test = load_csv(os.path.join(INP_DIR, \"X_woe_test.csv\"))\n",
    "\n",
    "X_woe_train.shape, X_woe_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_woe_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_woe_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `id_code_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_code_test = load_csv(os.path.join(INP_DIR, \"id_code_test.csv\"))\n",
    "id_code_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `X_org`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merge train\")\n",
    "\n",
    "dfs_train = [X_org_train]\n",
    "features, X_train = merge_dfs(dfs_train)\n",
    "y_train = load_csv(os.path.join(INP_DIR, \"y_train.csv\"))\n",
    "y_train = y_train[\"target\"].values\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=21083)\n",
    "\n",
    "print(\"after train-validatin split\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Merge test\")\n",
    "dfs_test = [X_org_test]\n",
    "_, X_test = merge_dfs(dfs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=2)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBOOST model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of XGBOOST model on the validation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning using `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 20, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 20, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10000)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(100)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.005), np.log(1.)),\n",
    "    #\"gamma\": hp.uniform(\"gamma\", 0., 2.),\n",
    "}\n",
    "\n",
    "params_fixed_xgb = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"n_jobs\": 2,\n",
    "    \"n_estimators\": 200\n",
    "}\n",
    "\n",
    "num_eval = 10\n",
    "\n",
    "trials_xgb, best_params_xgb = hyperopt_xgb(params_xgb, \n",
    "                                           X_train, y_train, X_val, y_val, \n",
    "                                           num_eval,\n",
    "                                           params_fixed=params_fixed_xgb,\n",
    "                                           rstate=30918)\n",
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = whole_to_int(best_params_xgb)\n",
    "\n",
    "xgb_best = XGBClassifier(**params_fixed_xgb, **best_params_xgb)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb_best, X_train, y_train)\n",
    "print(\"AUC of XGBoost model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb_best, X_val, y_val)\n",
    "print(\"AUC of XGBoost model on the evaluation set: %0.5f\" % auc_xgb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "out_sub = os.path.join(SUB_DIR, \"xgb_org_tuned_01.csv\")\n",
    "write_submit_csv(xgb_best, X_test, id_code_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_org_tuned_01.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sub = os.path.join(SUB_DIR, \"xgb_org_tuned_01.csv\")\n",
    "write_submit_csv(xgb_best, X_test, id_code_test, out_sub)\n",
    "\n",
    "out_model = os.path.join(MODELS_DIR, \"xgb_org_tuned_01.pickle\")\n",
    "pickle.dump(xgb_best, open(out_model, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `X_org` and `X_q10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merge train\")\n",
    "\n",
    "dfs_train = [X_org_train, X_q10_train]\n",
    "\n",
    "features, X_train = merge_dfs(dfs_train)\n",
    "y_train = load_csv(os.path.join(INP_DIR, \"y_train.csv\"))\n",
    "y_train = y_train[\"target\"].values\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=21083)\n",
    "\n",
    "print(\"after train-validatin split\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Merge test\")\n",
    "dfs_test = [X_org_test, X_q10_test]\n",
    "_, X_test = merge_dfs(dfs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=2)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBOOST model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of XGBOOST model on the validation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `X_org` and `X_valcount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merge train\")\n",
    "\n",
    "dfs_train = [X_org_train, X_valcount_train]\n",
    "\n",
    "features, X_train = merge_dfs(dfs_train)\n",
    "y_train = load_csv(os.path.join(INP_DIR, \"y_train.csv\"))\n",
    "y_train = y_train[\"target\"].values\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=21083)\n",
    "\n",
    "print(\"after train-validatin split\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Merge test\")\n",
    "dfs_test = [X_org_test, X_valcount_test]\n",
    "_, X_test = merge_dfs(dfs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=2)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBOOST model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of XGBOOST model on the validation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `X_org`, `X_q10` and `X_valcount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Merge train\")\n",
    "\n",
    "dfs_train = [X_org_train, X_q10_train, X_valcount_train]\n",
    "\n",
    "features, X_train = merge_dfs(dfs_train)\n",
    "y_train = load_csv(os.path.join(INP_DIR, \"y_train.csv\"))\n",
    "y_train = y_train[\"target\"].values\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                  stratify=y_train, random_state=21083)\n",
    "\n",
    "print(\"after train-validatin split\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Merge test\")\n",
    "dfs_test = [X_org_test, X_q10_test, X_valcount_test]\n",
    "_, X_test = merge_dfs(dfs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=2)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "auc_xgb_train = roc_auc(xgb, X_train, y_train)\n",
    "print(\"AUC of XGBOOST model on the train set: %0.5f\" % auc_xgb_train)\n",
    "\n",
    "auc_xgb_val = roc_auc(xgb, X_val, y_val)\n",
    "print(\"AUC of XGBOOST model on the validation set: %0.5f\" % auc_xgb_val)\n",
    "\n",
    "time_end = time.time()\n",
    "time_elapse = time_end - time_start\n",
    "print(\"Time elapsed: %0.5f s\" % time_elapse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
